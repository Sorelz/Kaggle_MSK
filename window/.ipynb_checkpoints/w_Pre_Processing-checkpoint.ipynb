{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('..//bases/training_variants')\n",
    "test = pd.read_csv('..//bases/test_variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts = pd.read_csv('..//bases/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")\n",
    "test_texts = pd.read_csv('..//bases/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_texts, how='left', on='ID')\n",
    "test = pd.merge(test, test_texts, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform Gene Letter to their abbreviation in order to find them in the text\n",
    "One_to_Three_AA = {'C': 'Cys', 'D': 'Asp', 'S': 'Ser', 'Q': 'Gln', 'K': 'Lys',\n",
    "         'I': 'Ile', 'P': 'Pro', 'T': 'Thr', 'F': 'Phe', 'N': 'Asn', \n",
    "         'G': 'Gly', 'H': 'His', 'L': 'Leu', 'R': 'Arg', 'W': 'Trp', \n",
    "         'A': 'Ala', 'V': 'Val', 'E': 'Glu', 'Y': 'Tyr', 'M': 'Met'}\n",
    "pattern = re.compile('|'.join(One_to_Three_AA.keys()))\n",
    "##### Get variation types by using regex\n",
    "def variation_regex(data, pattern): # if you want to not ignore cases, add extra argument to function\n",
    "    Boolean = [not bool(re.search(pattern, i, re.IGNORECASE)) for i in data.Variation]\n",
    "    data_no_regex = data[Boolean]  # 182 Fusions => 495 over \n",
    "    not_Boolean = [not i for i in Boolean]  \n",
    "    data_regex = data[not_Boolean]\n",
    "    \n",
    "    return (data_regex, data_no_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### process the train and test set together\n",
    "data_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "data_all_backup = data_all[:] ##### We keep backup because we want dummy variables of Gene & Text \n",
    "# TODO maybe also use Variation function of Gene from a database, and other suggestions. Also can use Count_sub as feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sub(data):\n",
    "\n",
    "    ##### The normal case is around 2080 out of the 2644\n",
    "    \n",
    "    \n",
    "    Boolean = [data.Variation[i] in data.Text[i] or #normal case\n",
    "               data.Variation[i][:-1] in data.Text[i] or #case 1.\n",
    "               pattern.sub(lambda x: One_to_Three_AA[x.group()], data.Variation[i][:-1]) # case2\n",
    "               in data.Text[i]  for i in data.index] ## because new indexing we use \n",
    "    \n",
    "    #TODO could also match insensitive as a next step for more info.\n",
    "    #Shorter Boolean below = the normal version\n",
    "    \n",
    "    #Boolean = [trainSub.Variation[i] in trainSub.Text[i] #normal case\n",
    "    #           for i in trainSub.ID] ## because new indexing we use ID\n",
    "    #           \n",
    "            \n",
    "    sub_in_text = data[Boolean]\n",
    "    not_Boolean = [not i for i in Boolean]  \n",
    "\n",
    "    sub_not_in_text = data[not_Boolean]\n",
    "#    sub_in_text['Count'] = [sub_in_text.Text[i].count(sub_in_text.Variation[i][:-1])\n",
    "#                    +sub_in_text.Text[i].count(pattern.sub(lambda x: One_to_Three_AA[x.group()], sub_in_text.Variation[i][:-1]))\n",
    "#                    for i in sub_in_text.index]\n",
    "    \n",
    "    return sub_in_text, sub_not_in_text\n",
    "##### For subs that are not find in text: use regex to account for a different number\n",
    "##### TODO: things you can further try - with AA name replacement, searching for the number only etc.\n",
    "def find_sub_noText(data):\n",
    "    Booleans = []\n",
    "    for i in data.index:\n",
    "        split_variation = re.split('(\\d+)', data.Variation[i]) # split based on a number\n",
    "        first_Amino = re.escape(split_variation[0]) #re.escpae uses variable as regex\n",
    "        last_Amino = re.escape(split_variation[-1])\n",
    "        #first_number = re.escape(split_variation[1][0])\n",
    "        #new_regex = r\"[^a-zA-Z0-9]\" + first_Amino + first_number\n",
    "        new_regex  = first_Amino + r\"\\d+\" + last_Amino\n",
    "        Boolean = bool(re.search(new_regex, data.Text[i]))\n",
    "        Booleans.append(Boolean)\n",
    "    \n",
    "    sub_number_in_text = data[Booleans]\n",
    "    not_Boolean = [not i for i in Booleans]  \n",
    "\n",
    "    sub_again_no_text = data[not_Boolean]\n",
    "    return sub_again_no_text, sub_number_in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Next we use a window to extract sentences\n",
    "def get_sentences_sub(data, splitted_sentences, window_left, window_right):\n",
    "    #position_sentences = [[] for _ in range(len(data))]  #### currently not used\n",
    "    data.index = range(len(data))\n",
    "    sentences_with_sub = [[] for _ in range(len(data))]\n",
    "    \n",
    "    for i in range(len(splitted_sentences)):\n",
    "        sentences = splitted_sentences[i]\n",
    "        one_to_three_variation = pattern.sub(lambda x: One_to_Three_AA[x.group()], data.Variation[i][:-1])\n",
    "        Variation = data.Variation[i][:-1]        \n",
    "        for j in range(len(sentences)):                              \n",
    "            if (Variation in sentences[j]) or (one_to_three_variation in sentences[j]):\n",
    "                new_regex = re.escape(Variation) + r\"[\\S]*\" ### Means no white space 0 or more\n",
    "                sentences[j] = re.sub(new_regex, ' placeholderMutation', sentences[j]) #case 1\n",
    "                new_regex = re.escape(one_to_three_variation) + r\"[\\S]*\"\n",
    "                sentences[j] = re.sub(new_regex, ' placeholderMutation', sentences[j]) #case 2\n",
    "                sentences_with_sub[i].extend(sentences[max(j-window_left,0) : min(j+1+window_right, len(sentences)-1)])\n",
    "                \n",
    "                ### We add space to ' placeholderMutation' because sometimes there are letters in front of it\n",
    "                # position_sentences[i].append(j) # not used for the moment\n",
    "\n",
    "    return sentences_with_sub   ### This might take a while because it's looping through all sentences\n",
    "def get_sentences_sub_noText(data, splitted_sentences, window_left, window_right):\n",
    "    #position_sentences = [[] for _ in range(len(data))]  #### currently not used\n",
    "    data.index = range(len(data))\n",
    "    sentences_with_sub = [[] for _ in range(len(data))]\n",
    "    \n",
    "    for i in range(len(splitted_sentences)):\n",
    "        sentences = splitted_sentences[i] \n",
    "        for j in range(len(sentences)):\n",
    "            split_variation = re.split('(\\d+)', data.Variation[i]) # split based on a number\n",
    "            first_Amino = re.escape(split_variation[0]) #re.escpae uses variable as regex\n",
    "            last_Amino = re.escape(split_variation[-1])\n",
    "            new_regex  = first_Amino + r\"\\d+\" + last_Amino\n",
    "            \n",
    "            #counter=len(re.findall(new_regex, sentences[j]))\n",
    "            \n",
    "            Boolean = bool(re.search(new_regex, sentences[j]))\n",
    "            if Boolean:\n",
    "                sentences[j] = re.sub(new_regex, ' placeholderMutation', sentences[j]) # Might help catch sy\n",
    "                sentences_with_sub[i].extend(sentences[max(j-window_left,0) : min(j+1+window_right,len(sentences)-1)])\n",
    "                # position_sentences[i].append(j) # not used for the moment\n",
    "\n",
    "    return sentences_with_sub   ### This might take a while because it's looping through all sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Converts list of sentences into one string of sentences for each document => to use for tfidf etc.\n",
    "def sentences_to_string(sentences_list):\n",
    "    sentence_strings = []\n",
    "    for sentences in sentences_list:\n",
    "        sentence_string =  ' '.join(str(sentence) for sentence in sentences)\n",
    "        sentence_strings.append(sentence_string)\n",
    "    \n",
    "    return sentence_strings ### This doesn't take such a long time to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitutions (subs) processing of data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### First find those that have the format of being a substitution in data\n",
    "data_all['Substitutions_var'] = data_all.Variation.apply(lambda x: bool(re.search('^[A-Z]\\\\d+[A-Z*]$', x))*1) #multiplying by 1 converts True to 1, False to 0 => Maybe modify this later?\n",
    "data_all['Stop_codon_var'] = data_all.Variation.apply(lambda x: bool(re.search('[*]', x))*1) #multiplying by 1 converts True to 1, False to 0\n",
    "data_sub = data_all[data_all['Substitutions_var']==1] ### Now we know the index of where a substitution occurs - the data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_in_text, sub_not_in_text = find_sub(data_sub)\n",
    "sub_in_text_backup = sub_in_text[:] ## index gets changed by text_processing if we don't make a copy\n",
    "##### INVESTIGATION: Why do some subs don't appear in Text?: Try to automize this and find out\n",
    "### Substitutions can appear as SAME_PREFIX - Other number - SAME_SUFFIX\n",
    "\n",
    "sub_again_no_Text, sub_noText = find_sub_noText(sub_not_in_text) # 108 such cases out of 411 = nice improvement\n",
    "sub_noText_backup = sub_noText[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on variations who have the 2 letters right but not same numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NLTK_sub_noText = [sent_tokenize(sub_noText.Text[i]) for i in sub_noText.index]\n",
    "sub_noText_sentences = get_sentences_sub_noText(sub_noText, NLTK_sub_noText, window_left = 2, window_right = 2) # Retrieves sentences where subsitution mutation is included\n",
    "sub_noText_sentences = [sorted(set(sentences), key = sentences.index) for sentences in sub_noText_sentences] # only use unique sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_sub = [sent_tokenize(sub_in_text.Text[i]) for i in sub_in_text.index] # takes a long time to run tokenizer => use pickle to save\n",
    "sub_sentences = get_sentences_sub(sub_in_text, NLTK_sub, window_left = 2, window_right = 2) \n",
    "# Retrieves sentences where subsitution mutation is included.\n",
    "# window_left and window_right specify which sentences to keep at the left side or right side of the sub sentences.\n",
    "# IMPORTANT: I used also placeholderMutation to replace the original sub mutations here\n",
    "sub_sentences = [sorted(set(sentences), key = sentences.index) for sentences in sub_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_sentences_string = sentences_to_string(sub_sentences)\n",
    "sub_noText_string = sentences_to_string(sub_noText_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all.Text[sub_in_text_backup.index] = sub_sentences_string\n",
    "data_all.Text[sub_noText_backup.index] = sub_noText_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "############## Non-subs preprocessing of data set #######################\n",
    "\n",
    "#def find_mutation_type(row, pattern):  ##### TODO: make clearer by using a function instead of lambda\n",
    "#    return bool(re.search('^fusion', row, re.IGNORECASE)) *1. Also for subs\n",
    "\n",
    "####### Fusions : 'Fusions' ############\n",
    "data_all['Fusion_var'] = data_all.Variation.apply(lambda x: bool(re.search('^fusion', x, re.IGNORECASE))*1) #multiplying by 1 converts True to 1, False to 0\n",
    "new_fusion, new_data_all = variation_regex(data_all, '^fusion') \n",
    "\n",
    "###### Fusions: 'Gene-Gene fusion' ########\n",
    "data_all['gene_fusion_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('fusion', x, re.IGNORECASE))*1) \n",
    "_ , new_data_all = variation_regex(new_data_all, 'fusion') \n",
    "###### Notice that NaN introduced for places where splicing occured => replace after NaN with 0's when complete\n",
    "\n",
    "####### Deletions: 'Deletions' ############\n",
    "data_all['Deletion_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('^del', x, re.IGNORECASE))*1) \n",
    "new_del, new_data_all = variation_regex(new_data_all, '^del') \n",
    "\n",
    "####### Deletions & Insertions wheteher together or seperately (doesn't make a big difference IMO)\n",
    "data_all['del_or_ins_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('del|ins', x, re.IGNORECASE))*1) \n",
    "# we could also later divide it into del, ins if we want to\n",
    "\n",
    "###### Amplifications #########\n",
    "data_all['Amplification_var'] = data_all.Variation.apply(lambda x: bool(re.search('ampl', x, re.IGNORECASE))*1) \n",
    "\n",
    "###### Truncations ########### Don't forget there are 'Truncating mutations' = 95 and '_trunc' = 4\n",
    "data_all['Truncation_var'] = data_all.Variation.apply(lambda x: bool(re.search('trunc', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Exons #########\n",
    "data_all['exon_var'] = data_all.Variation.apply(lambda x: bool(re.search('exon', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Frameshift mutations ########\n",
    "data_all['frameshift_var'] = data_all.Variation.apply(lambda x: bool(re.search('fs', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Duplications ##############\n",
    "data_all['dup_var'] = data_all.Variation.apply(lambda x: bool(re.search('dup', x, re.IGNORECASE))*1) \n",
    "\n",
    "data_all.fillna(0, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO : #Sentence Tokenizer for non-subs that are not in text, AND\n",
    "#subs that are not in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not used\n",
    "################ The dummy variables for Gene and Text ##################\n",
    "## TODO: also use dummy for Text? There are 135 shared Genes and 142 shared Text between train and Leaks!  len(set(train.Text) & set(Leaks.Text))\n",
    "#data_all_dummy = data_all_backup[['Gene', 'Text']] # drop those columns we don't need as dummy.\n",
    "#X_dummy = pd.get_dummies(data_all_dummy) # converts categorical variables into dummy variable. From len set => 269 genes + 2090 texts\n",
    "#X_dummy_train = X_dummy[:train.shape[0]]\n",
    "#X_dummy_test = X_dummy[train.shape[0]:]\n",
    "#dummy_names = X_dummy.columns.values #### To remember names if you want to check again what Gene or Text used\n",
    "#X_dummy = X_dummy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Use the variation types \n",
    "#variation_types = data_all.drop(['ID', 'Gene', 'Class', 'Text', 'Variation'], axis =1)\n",
    "#X_variation_train = variation_types[:train.shape[0]]\n",
    "#X_variation_test = variation_types[train.shape[0]:]\n",
    "#variation_names = variation_types.columns.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}~0123456789') \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc,lemmatiz=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 = [re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    if lemmatiz:\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free_lem.split())\n",
    "        return normalized\n",
    "    else:\n",
    "        punc_free = \"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        return punc_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No lemmatization for the moment, be careful not to lemmatize then w2vec\n",
    "data_all.Text = [clean(doc) for doc in data_all.Text]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some more features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some counters on the gene and variation, here not using the 3letters abreviations\n",
    "#that means if the text contains only the gene or variation but with the \n",
    "#3letters format, the variable will capture it\n",
    "data_all[\"Gene_Share\"] = data_all.apply(lambda r: sum([1 for w in r[\"Gene\"].split(\" \") if w in r[\"Text\"].split(\" \")]), axis=1)\n",
    "data_all[\"Variation_Share\"] = data_all.apply(lambda r: sum([1 for w in r[\"Variation\"].split(\" \") if w in r[\"Text\"].split(\" \")]), axis=1)\n",
    "\n",
    "data_all[\"Text_words\"] = data_all[\"Text\"].map(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure_counter=[\"fig\",\"figure\"]\n",
    "for fig in figure_counter: \n",
    "        data_all[\"Figure_counter\"]=(data_all[\"Text\"].map(lambda x : str(x).count(fig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_all[\"Text\"]=[re.sub(\"fig|figure\",\" \",doc) for doc in data_all[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"checkpoints_databases/w_working_train.csv\",index=False,encoding=\"utf8\")\n",
    "test.to_csv(\"checkpoints_databases/w_working_test.csv\",index=False,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
