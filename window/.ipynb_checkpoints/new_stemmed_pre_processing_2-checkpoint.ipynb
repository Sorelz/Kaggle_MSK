{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim import utils\n",
    "import os\n",
    "import nltk\n",
    "import scipy.sparse as ssp\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"checkpoints_databases/new_working_train.csv\",encoding=\"utf8\")\n",
    "test=pd.read_csv(\"checkpoints_databases/new_working_test.csv\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock as input features for meta model\n",
    "train_cl=train.drop([\"Variation\",\"Text\",\"Class\",\"Gene\",\"Text_words\"],axis=1)\n",
    "test_cl=test.drop([\"Text\",\"Class\",\"Variation\",\"Gene\",\"Text_words\"],axis=1)\n",
    "train_cl.to_csv(\"w_meta_features/meta_train_l1l2.csv\",index=False)\n",
    "test_cl.to_csv(\"w_meta_features/meta_test_l1l2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all=pd.concat((train,test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=['a','about','above','after','again','ain','am', 'an','and','any','are','aren','as','at','be','because','been','before','being',\n",
    " 'below','between','both','but','by','d','down','during','for','from','further','had','hadn','has','hasn','have','haven','having',\n",
    "'he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','isn','it','its','itself','just','ll','m',\n",
    "'ma','me','more','most','my','myself','needn','no','nor','not','now','o','of','off','on','once','only','or','other','our','ours',\n",
    " 'ourselves','out','over','own','re','s','same','shan','she','so','some','such','t','than','that','the','their','theirs','them',\n",
    " 'themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','very','was','wasn','we',\n",
    " 'were','weren','what','when','where','which','while','who','whom','why','will','with','y','you','your','yours','yourself','yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set('.,!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}')\n",
    "ps=PorterStemmer()\n",
    "def clean(doc,lemmatiz=False,stemming=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 =[re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    punc_free_1= re.sub(r'\\s\\d+\\s',' ',\"\".join(punc_free_0))\n",
    "    if lemmatiz==True:\n",
    "        lem=[]\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        for word,tag in pos_tag(word_tokenize(punc_free_lem)):\n",
    "            wntag=tag[0].lower()\n",
    "            wntag=wntag if wntag in [\"a\",\"r\",\"n\",\"v\"] else None\n",
    "            if not wntag:\n",
    "                lem.append(word)\n",
    "            else:\n",
    "                lem.append(lemma.lemmatize(word,wntag))\n",
    "        normalized=\" \".join(word for word in lem)\n",
    "        return normalized\n",
    "    if stemming==True:\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free_0 =[re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "        punc_free_1= re.sub(r'\\s\\d+\\s',' ',\"\".join(punc_free_0))\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_1 if ch not in exclude)\n",
    "        normalized=\" \".join(ps.stem(word) for word in word_tokenize(punc_free_lem))\n",
    "        return normalized\n",
    "    else:\n",
    "        return (\"Choose a cleaning man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all[\"Window_Text\"] = [clean(doc,stemming=True) for doc in data_all[\"Window_Text\"]]\n",
    "data_all[\"Full_Text\"] = [clean(doc,lemmatiz=True) for doc in data_all[\"Full_Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841\n",
      "1078\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_all)):\n",
    "    if (len(data_all.Text[i])<100):\n",
    "        print (i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will add features from word2vec retrained then get the mean for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location,size):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=size, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#It's important to remove duplicated spaces for word2vec learning !\n",
    "train[\"Full_Text\"]=[\" \".join(doc.split()) for doc in train[\"Full_Text\"].values]\n",
    "test[\"Full_Text\"]=[\" \".join(doc.split()) for doc in test[\"Full_Text\"].values]\n",
    "train[\"Window_Text\"]=[\" \".join(doc.split()) for doc in train[\"Window_Text\"].values]\n",
    "test[\"Window_Text\"]=[\" \".join(doc.split()) for doc in test[\"Window_Text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_stem_w2v_features100 not found. training model\n",
      "Model done training. Saving to disk\n",
      "new_stem_w2v_features200 not found. training model\n",
      "Model done training. Saving to disk\n",
      "new_stem_w2v_features300 not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "number_w2v=[300] # we know it's 300 from previous runs, no time to gridsearch again and fit weights for lowers \n",
    "w2v={}\n",
    "for size in number_w2v:\n",
    "    w2v[\"w2v_\"+str(size)] = get_word2vec(\n",
    "        MySentences(\n",
    "            train[\"Window_Text\"].values),\"new_stem_w2v_features\"+str(size),size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer={}\n",
    "mean_embedded_train={}\n",
    "mean_embedded_test={}\n",
    "for name in w2v:\n",
    "    mean_embedding_vectorizer[name] = MeanEmbeddingVectorizer(w2v[name])\n",
    "    mean_embedded_train[name] = mean_embedding_vectorizer[name].fit_transform(train['Window_Text'])\n",
    "    mean_embedded_test[name] = mean_embedding_vectorizer[name].fit_transform(test['Window_Text'])\n",
    "df_embed_tr={}\n",
    "df_embed_te={}\n",
    "for name in w2v:\n",
    "    df_embed_tr[name]=pd.DataFrame(mean_embedded_train[name])\n",
    "    df_embed_te[name]=pd.DataFrame(mean_embedded_test[name])\n",
    "train_w2v={}\n",
    "test_w2v={}\n",
    "for name in w2v:\n",
    "    train_w2v[name]=df_embed_tr[name]\n",
    "    test_w2v[name]=df_embed_te[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in w2v:\n",
    "    train_w2v[name].to_csv(\"checkpoints_databases/new_stem_working_train_\"+name+\".csv\",index=False)\n",
    "    test_w2v[name].to_csv(\"checkpoints_databases/new_stem_working_test_\"+name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now TFIDF +300tsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_w = TfidfVectorizer(\n",
    "        min_df=3, max_features=8000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(train[\"Window_Text\"])\n",
    "tfidf_f = TfidfVectorizer(\n",
    "        min_df=10, max_features=10000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(train[\"Full_Text\"])\n",
    "\n",
    "X_train_text_w = tfidf_w.transform(train[\"Window_Text\"])\n",
    "X_test_text_w = tfidf_w.transform(test[\"Window_Text\"])\n",
    "X_train_text_f = tfidf_f.transform(train[\"Full_Text\"])\n",
    "X_test_text_f = tfidf_f.transform(test[\"Full_Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'acid',\n",
       " 'activate',\n",
       " 'activation',\n",
       " 'activity',\n",
       " 'al',\n",
       " 'alk',\n",
       " 'also',\n",
       " 'amplification',\n",
       " 'analysis',\n",
       " 'associate',\n",
       " 'b',\n",
       " 'bind',\n",
       " 'braf',\n",
       " 'brca',\n",
       " 'c',\n",
       " 'cancer',\n",
       " 'case',\n",
       " 'cell',\n",
       " 'cell line',\n",
       " 'clinical',\n",
       " 'compare',\n",
       " 'd',\n",
       " 'data',\n",
       " 'deletion',\n",
       " 'detect',\n",
       " 'different',\n",
       " 'dna',\n",
       " 'domain',\n",
       " 'e',\n",
       " 'effect',\n",
       " 'egfr',\n",
       " 'egfr mutation',\n",
       " 'et',\n",
       " 'et al',\n",
       " 'exon',\n",
       " 'express',\n",
       " 'expression',\n",
       " 'f',\n",
       " 'fgfr',\n",
       " 'fig',\n",
       " 'figure',\n",
       " 'find',\n",
       " 'flt',\n",
       " 'function',\n",
       " 'fusion',\n",
       " 'g',\n",
       " 'gefitinib',\n",
       " 'gene',\n",
       " 'human',\n",
       " 'identify',\n",
       " 'include',\n",
       " 'increase',\n",
       " 'inhibitor',\n",
       " 'insertion',\n",
       " 'kinase',\n",
       " 'kit',\n",
       " 'level',\n",
       " 'line',\n",
       " 'lung',\n",
       " 'may',\n",
       " 'missense',\n",
       " 'mutant',\n",
       " 'mutation',\n",
       " 'n',\n",
       " 'number',\n",
       " 'observe',\n",
       " 'one',\n",
       " 'p',\n",
       " 'patient',\n",
       " 'placeholdermutation',\n",
       " 'previously',\n",
       " 'protein',\n",
       " 'receptor',\n",
       " 'region',\n",
       " 'report',\n",
       " 'residue',\n",
       " 'resistance',\n",
       " 'response',\n",
       " 'result',\n",
       " 's',\n",
       " 'sample',\n",
       " 'sequence',\n",
       " 'show',\n",
       " 'site',\n",
       " 'study',\n",
       " 'substitution',\n",
       " 'suggest',\n",
       " 't',\n",
       " 'table',\n",
       " 'three',\n",
       " 'tumor',\n",
       " 'two',\n",
       " 'type',\n",
       " 'use',\n",
       " 'variant',\n",
       " 'wild',\n",
       " 'wild type',\n",
       " 'within',\n",
       " 'wt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf_names =tfidf.get_feature_names()\n",
    "#tfidf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#same did thousands of time gridsearchs, perfect is 100 for our cases\n",
    "dic_svd=TruncatedSVD(n_components=100,n_iter=25,random_state=26)\n",
    "\n",
    "tsvd_train_w=dic_svd.fit_transform(X_train_text_w)\n",
    "tsvd_test_w=dic_svd.transform(X_test_text_w)\n",
    "tsvd_train_f=dic_svd.fit_transform(X_train_text_f)\n",
    "tsvd_test_f=dic_svd.transform(X_test_text_f)\n",
    "X_train=pd.DataFrame()\n",
    "X_test=pd.DataFrame()\n",
    "for i in range(int(100)):\n",
    "    X_train['window_' +\"tfidf_\"+str(i)] = tsvd_train_w[:, i]\n",
    "    X_test['window_' +\"tfidf_\"+str(i)] = tsvd_test_w[:, i]\n",
    "    X_train['full_' +\"tfidf_\"+str(i)] = tsvd_train_f[:, i]\n",
    "    X_test['full_' +\"tfidf_\"+str(i)] = tsvd_test_f[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wind=X_train.iloc[:,:100]\n",
    "X_train_full=X_train.iloc[:,100:200]\n",
    "X_test_wind=X_test.iloc[:,:100]\n",
    "X_test_full=X_test.iloc[:,100:200]\n",
    "dic_train={}\n",
    "dic_test={}\n",
    "dic_train[\"wind_tfidf_100\"]=X_train_wind\n",
    "dic_test[\"wind_tfidf_100\"]=X_test_wind\n",
    "dic_train[\"full_tfidf_100\"]=X_train_full\n",
    "dic_test[\"full_tfidf_100\"]=X_test_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dic_train:\n",
    "    dic_train[name].to_csv(\"checkpoints_databases/new_stem_working_train_\"+name+\".csv\",index=False)\n",
    "    dic_test[name].to_csv(\"checkpoints_databases/new_stem_working_test_\"+name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add w2v bio for lemma on FULL text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_bio = KeyedVectors.load_word2vec_format(\"../bases/PMC-w2v.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_vec={}\n",
    "me_train={}\n",
    "me_test={}\n",
    "me_vec = MeanEmbeddingVectorizer(w2v_bio)\n",
    "me_train = me_vec.fit_transform(train['Full_Text'])\n",
    "me_test = me_vec.fit_transform(test['Full_Text'])\n",
    "df_bio_tr=pd.DataFrame(me_train)\n",
    "df_bio_te=pd.DataFrame(me_test)\n",
    "\n",
    "train_w2v_bio=df_bio_tr\n",
    "test_w2v_bio=df_bio_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_bio.to_csv(\"checkpoints_databases/new_working_train_bio.csv\",index=False)\n",
    "test_w2v_bio.to_csv(\"checkpoints_databases/new_working_test_bio.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
