{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim import utils\n",
    "import os\n",
    "import nltk\n",
    "import scipy.sparse as ssp\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"checkpoints_databases/new_working_train.csv\",encoding=\"utf8\")\n",
    "test=pd.read_csv(\"checkpoints_databases/new_working_test.csv\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock as input features for meta model\n",
    "train_cl=train.drop([\"Variation\",\"Full_Text\",\"Window_Text\",\"Class\",\"Gene\",\"Text_words\"],axis=1)\n",
    "test_cl=test.drop([\"Text\",\"Class\",\"Full_Text\",\"Window_Text\",\"Variation\",\"Gene\",\"Text_words\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl.to_csv(\"w_meta_features/meta_train_l2.csv\",index=False)\n",
    "test_cl.to_csv(\"w_meta_features/meta_test_l2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all=pd.concat((train,test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('.,!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}')\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc,lemmatiz=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 =[re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    punc_free_1= re.sub(r'\\s\\d+\\s',' ',\"\".join(punc_free_0))\n",
    "    if lemmatiz==True:\n",
    "        lem=[]\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        for word,tag in pos_tag(word_tokenize(punc_free_lem)):\n",
    "            wntag=tag[0].lower()\n",
    "            wntag=wntag if wntag in [\"a\",\"r\",\"n\",\"v\"] else None\n",
    "            if not wntag:\n",
    "                lem.append(word)\n",
    "            else:\n",
    "                lem.append(lemma.lemmatize(word,wntag))\n",
    "        normalized=\" \".join(word for word in lem)\n",
    "        return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.Text = [clean(doc, lemmatiz=True) for doc in data_all.Text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "233\n",
      "235\n",
      "450\n",
      "461\n",
      "841\n",
      "1078\n",
      "1255\n",
      "2166\n",
      "2220\n",
      "2484\n",
      "2979\n",
      "3107\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_all)):\n",
    "    if (len(data_all.Text[i])<300):\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will add features from word2vec retrained then get the mean for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location,size):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=size, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#It's important to remove duplicated spaces for word2vec learning !\n",
    "train[\"Text\"]=[\" \".join(doc.split()) for doc in train[\"Text\"].values]\n",
    "test[\"Text\"]=[\" \".join(doc.split()) for doc in test[\"Text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_w2v_features100 not found. training model\n",
      "Model done training. Saving to disk\n",
      "new_w2v_features200 not found. training model\n",
      "Model done training. Saving to disk\n",
      "new_w2v_features300 not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "number_w2v=[100,200,300]\n",
    "w2v={}\n",
    "for size in number_w2v:\n",
    "    w2v[\"w2v_\"+str(size)] = get_word2vec(\n",
    "        MySentences(\n",
    "            train[\"Text\"].values),\"new_w2v_features\"+str(size),size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer={}\n",
    "mean_embedded_train={}\n",
    "mean_embedded_test={}\n",
    "for name in w2v:\n",
    "    mean_embedding_vectorizer[name] = MeanEmbeddingVectorizer(w2v[name])\n",
    "    mean_embedded_train[name] = mean_embedding_vectorizer[name].fit_transform(train['Text'])\n",
    "    mean_embedded_test[name] = mean_embedding_vectorizer[name].fit_transform(test['Text'])\n",
    "df_embed_tr={}\n",
    "df_embed_te={}\n",
    "for name in w2v:\n",
    "    df_embed_tr[name]=pd.DataFrame(mean_embedded_train[name])\n",
    "    df_embed_te[name]=pd.DataFrame(mean_embedded_test[name])\n",
    "train_w2v={}\n",
    "test_w2v={}\n",
    "for name in w2v:\n",
    "    train_w2v[name]=df_embed_tr[name]\n",
    "    test_w2v[name]=df_embed_te[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure if ID is on the column, if not just suppr the drop\n",
    "for name in w2v:\n",
    "    train_w2v[name].drop(\"ID\",axis=1).to_csv(\"checkpoints_databases/new_working_train_\"+name+\".csv\",index=False)\n",
    "    test_w2v[name].drop(\"ID\",axis=1).to_csv(\"checkpoints_databases/new_working_test_\"+name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now TFIDF +300tsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=3, max_features=8000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(train[\"Text\"])\n",
    "\n",
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'acid',\n",
       " 'activate',\n",
       " 'activation',\n",
       " 'activity',\n",
       " 'al',\n",
       " 'alk',\n",
       " 'also',\n",
       " 'amplification',\n",
       " 'analysis',\n",
       " 'associate',\n",
       " 'b',\n",
       " 'bind',\n",
       " 'braf',\n",
       " 'brca',\n",
       " 'c',\n",
       " 'cancer',\n",
       " 'case',\n",
       " 'cell',\n",
       " 'cell line',\n",
       " 'clinical',\n",
       " 'compare',\n",
       " 'd',\n",
       " 'data',\n",
       " 'deletion',\n",
       " 'detect',\n",
       " 'different',\n",
       " 'dna',\n",
       " 'domain',\n",
       " 'e',\n",
       " 'effect',\n",
       " 'egfr',\n",
       " 'egfr mutation',\n",
       " 'et',\n",
       " 'et al',\n",
       " 'exon',\n",
       " 'express',\n",
       " 'expression',\n",
       " 'f',\n",
       " 'fgfr',\n",
       " 'fig',\n",
       " 'figure',\n",
       " 'find',\n",
       " 'flt',\n",
       " 'function',\n",
       " 'fusion',\n",
       " 'g',\n",
       " 'gefitinib',\n",
       " 'gene',\n",
       " 'human',\n",
       " 'identify',\n",
       " 'include',\n",
       " 'increase',\n",
       " 'inhibitor',\n",
       " 'insertion',\n",
       " 'kinase',\n",
       " 'kit',\n",
       " 'level',\n",
       " 'line',\n",
       " 'lung',\n",
       " 'may',\n",
       " 'missense',\n",
       " 'mutant',\n",
       " 'mutation',\n",
       " 'n',\n",
       " 'number',\n",
       " 'observe',\n",
       " 'one',\n",
       " 'p',\n",
       " 'patient',\n",
       " 'placeholdermutation',\n",
       " 'previously',\n",
       " 'protein',\n",
       " 'receptor',\n",
       " 'region',\n",
       " 'report',\n",
       " 'residue',\n",
       " 'resistance',\n",
       " 'response',\n",
       " 'result',\n",
       " 's',\n",
       " 'sample',\n",
       " 'sequence',\n",
       " 'show',\n",
       " 'site',\n",
       " 'study',\n",
       " 'substitution',\n",
       " 'suggest',\n",
       " 't',\n",
       " 'table',\n",
       " 'three',\n",
       " 'tumor',\n",
       " 'two',\n",
       " 'type',\n",
       " 'use',\n",
       " 'variant',\n",
       " 'wild',\n",
       " 'wild type',\n",
       " 'within',\n",
       " 'wt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_names =tfidf.get_feature_names()\n",
    "tfidf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsvd_train= {}\n",
    "tsvd_test={}\n",
    "list_comp=[100,200,300]\n",
    "dic_svd={}\n",
    "for comp in list_comp:\n",
    "    dic_svd[str(comp)]=TruncatedSVD(n_components=comp,n_iter=25,random_state=26)\n",
    "for svd in dic_svd:\n",
    "    tsvd_train[svd]=dic_svd[svd].fit_transform(X_train_text)\n",
    "    tsvd_test[svd]=dic_svd[svd].transform(X_test_text)\n",
    "X_train=pd.DataFrame()\n",
    "X_test=pd.DataFrame()\n",
    "for n in dic_svd:\n",
    "    for i in range(int(n)):\n",
    "        X_train['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_train[n][:, i]\n",
    "        X_test['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_test[n][:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100=X_train.iloc[:,:100]\n",
    "X_train_200=X_train.iloc[:,100:300]\n",
    "X_train_300=X_train.iloc[:,300:600]\n",
    "X_test_100=X_test.iloc[:,:100]\n",
    "X_test_200=X_test.iloc[:,100:300]\n",
    "X_test_300=X_test.iloc[:,300:600]\n",
    "dic_train={}\n",
    "dic_test={}\n",
    "dic_train[\"tsvd_100\"]=X_train_100\n",
    "dic_test[\"tsvd_100\"]=X_test_100\n",
    "dic_train[\"tsvd_200\"]=X_train_200\n",
    "dic_test[\"tsvd_200\"]=X_test_200\n",
    "dic_train[\"tsvd_300\"]=X_train_300\n",
    "dic_test[\"tsvd_300\"]=X_test_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dic_train:\n",
    "    dic_train[name].to_csv(\"checkpoints_databases/new_working_train_tfidf_\"+name+\".csv\",index=False)\n",
    "    dic_test[name].to_csv(\"checkpoints_databases/new_working_test_tfidf_\"+name+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now w2v bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_bio = KeyedVectors.load_word2vec_format(\"../bases/PMC-w2v.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_vec={}\n",
    "me_train={}\n",
    "me_test={}\n",
    "me_vec = MeanEmbeddingVectorizer(w2v_bio)\n",
    "me_train = me_vec.fit_transform(train['Text'])\n",
    "me_test = me_vec.fit_transform(test['Text'])\n",
    "df_bio_tr={}\n",
    "df_bio_te={}\n",
    "df_bio_tr=pd.DataFrame(me_train)\n",
    "df_bio_te=pd.DataFrame(me_test)\n",
    "\n",
    "df_tr=df_bio_tr.reset_index()\n",
    "df_te=df_bio_te.reset_index()\n",
    "df_tr=df_tr.rename(columns={\"index\":\"ID\"})\n",
    "df_te=df_te.rename(columns={\"index\":\"ID\"})\n",
    "\n",
    "train_w2v_bio=df_tr\n",
    "test_w2v_bio=df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_bio.drop(\"ID\",axis=1).to_csv(\"checkpoints_databases/new_working_train_bio.csv\",index=False)\n",
    "test_w2v_bio.drop(\"ID\",axis=1).to_csv(\"checkpoints_databases/new_working_test_bio.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
