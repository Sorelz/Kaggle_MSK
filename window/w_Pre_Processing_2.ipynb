{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim import utils\n",
    "import os\n",
    "import nltk\n",
    "import scipy.sparse as ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"checkpoints_databases/w_working_train.csv\",encoding=\"utf8\")\n",
    "test=pd.read_csv(\"checkpoints_databases/w_working_test.csv\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will add features from word2vec retrained then get the mean for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-35d4ff493bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#It's important to remove duplicated spaces for word2vec learning !\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-35d4ff493bf2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#It's important to remove duplicated spaces for word2vec learning !\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#It's important to remove duplicated spaces for word2vec learning !\n",
    "train[\"Text\"]=[\" \".join(doc.split()) for doc in train[\"Text\"].values]\n",
    "test[\"Text\"]=[\" \".join(doc.split()) for doc in test[\"Text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Substitutions_var</th>\n",
       "      <th>Stop_codon_var</th>\n",
       "      <th>Background</th>\n",
       "      <th>Fusion_var</th>\n",
       "      <th>gene_fusion_var</th>\n",
       "      <th>Deletion_var</th>\n",
       "      <th>...</th>\n",
       "      <th>Gene_ZFP57</th>\n",
       "      <th>Gene_ZFPM2</th>\n",
       "      <th>Gene_ZFYVE27</th>\n",
       "      <th>Gene_ZIC3</th>\n",
       "      <th>Gene_ZMPSTE24</th>\n",
       "      <th>Gene_ZNF365</th>\n",
       "      <th>Gene_ZNF41</th>\n",
       "      <th>Gene_ZNF513</th>\n",
       "      <th>Gene_ZNF592</th>\n",
       "      <th>Gene_ZNF81</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1628T</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>genetic abnormality frequently identified glio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class    ID Text Variation  Substitutions_var  Stop_codon_var  \\\n",
       "3889    NaN  3889  NaN    M1628T                  1               0   \n",
       "\n",
       "                                             Background  Fusion_var  \\\n",
       "3889  genetic abnormality frequently identified glio...           0   \n",
       "\n",
       "      gene_fusion_var  Deletion_var     ...      Gene_ZFP57  Gene_ZFPM2  \\\n",
       "3889              0.0           0.0     ...               0           0   \n",
       "\n",
       "      Gene_ZFYVE27  Gene_ZIC3  Gene_ZMPSTE24  Gene_ZNF365  Gene_ZNF41  \\\n",
       "3889             0          0              0            0           0   \n",
       "\n",
       "      Gene_ZNF513  Gene_ZNF592  Gene_ZNF81  \n",
       "3889            0            0           0  \n",
       "\n",
       "[1 rows x 1517 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test[\"Text\"].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        train[\"Text\"].values,test[\"Text\"].values),\"w2v_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded_train = mean_embedding_vectorizer.fit_transform(train['Text'])\n",
    "mean_embedded_test = mean_embedding_vectorizer.fit_transform(test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_embed_tr=pd.DataFrame(mean_embedded_train)\n",
    "df_embed_te=pd.DataFrame(mean_embedded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_embedding_tr=df_embed_tr.reset_index()\n",
    "df_embedding_te=df_embed_te.reset_index()\n",
    "df_embedding_tr=df_embedding_tr.rename(columns={\"index\":\"ID\"})\n",
    "df_embedding_te=df_embedding_te.rename(columns={\"index\":\"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cl=train.drop([\"Variation\",\"Text\",\"Class\"],axis=1)\n",
    "test_cl=test.drop([\"Text\",\"Class\",\"Variation\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_stack_train.to_csv(\"w_meta_features/meta_train_l1l2.csv\")\n",
    "feat_stack_test.to_csv(\"w_meta_features/meta_test_l1l2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_w2v=pd.merge(train_cl,df_embedding_tr,on=\"ID\")\n",
    "test_w2v=pd.merge(test_cl,df_embedding_te,on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_w2v_train=np.array(train_w2v.drop(\"ID\",axis=1))\n",
    "np_w2v_test=np.array(test_w2v.drop(\"ID\",axis=1))\n",
    "ssp_w2v_train=ssp.csc_matrix(np_w2v_train)\n",
    "ssp_w2v_test=ssp.csc_matrix(np_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssp.save_npz(\"checkpoints_databases/w_working_train_w2v.npz\",ssp_w2v_train)\n",
    "ssp.save_npz(\"checkpoints_databases/w_working_test_w2v.npz\",ssp_w2v_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "        min_df=10, max_features=10000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(train[\"Text\"])\n",
    "\n",
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 10015) (5668, 10015)\n"
     ]
    }
   ],
   "source": [
    "X_train=ssp.hstack((train_cl.drop(\"ID\",axis=1), X_train_text),format=\"csc\")\n",
    "X_test=ssp.hstack((test_cl.drop(\"ID\",axis=1),X_test_text),format=\"csc\")\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssp.save_npz(\"checkpoints_databases/w_working_train_tfidf.npz\",X_train)\n",
    "ssp.save_npz(\"checkpoints_databases/w_working_test_tfidf.npz\",X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "train_sentences = constructLabeledSentences(train['Text'])\n",
    "test_sentences = constructLabeledSentences(test['Text'])\n",
    "\n",
    "Text_INPUT_DIM=350\n",
    "\n",
    "d2v_train = Doc2Vec(min_count=1, window=10, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=-1, iter=5,seed=26)\n",
    "d2v_train.build_vocab(train_sentences)\n",
    "d2v_train.train(train_sentences, total_examples=d2v_train.corpus_count, epochs=d2v_train.iter)\n",
    "\n",
    "d2v_test = Doc2Vec(min_count=1, window=10, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=-1, iter=5,seed=26)\n",
    "d2v_test.build_vocab(test_sentences)\n",
    "d2v_test.train(test_sentences, total_examples=d2v_test.corpus_count, epochs=d2v_test.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 365) (5668, 365)\n"
     ]
    }
   ],
   "source": [
    "d2v_train_arrays = np.zeros((len(train), Text_INPUT_DIM))\n",
    "d2v_test_arrays = np.zeros((len(test), Text_INPUT_DIM))\n",
    "\n",
    "for i in range(len(train)):\n",
    "    d2v_train_arrays[i] = d2v_train.docvecs['Text_'+str(i)]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    d2v_test_arrays[i] = d2v_test.docvecs['Text_'+str(i)]\n",
    "\n",
    "X_train=ssp.hstack((train_cl.drop(\"ID\",axis=1), d2v_train_arrays),format=\"csc\")\n",
    "X_test=ssp.hstack((test_cl.drop(\"ID\",axis=1), d2v_test_arrays),format=\"csc\")\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "ssp.save_npz(\"checkpoints_databases/w_working_train_d2v.npz\",X_train)\n",
    "ssp.save_npz(\"checkpoints_databases/w_working_test_d2v.npz\",X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
