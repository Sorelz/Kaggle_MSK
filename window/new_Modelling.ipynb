{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import os \n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use cross_val_score on XGBoost to select 100,200 or 300 for each preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_w2v = {} \n",
    "work_test_w2v = {}\n",
    "pre_process=[\"w2v_100.csv\",\"w2v_200.csv\",\"w2v_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_w2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_train_\"+f)\n",
    "    work_test_w2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_bio = {} \n",
    "work_test_bio = {}\n",
    "pre_process=[\"bio.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_bio[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_train_\"+f)\n",
    "    work_test_bio[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_tfidf = {} \n",
    "work_test_tfidf = {}\n",
    "pre_process=[\"tfidf_tsvd_100.csv\",\"tfidf_tsvd_200.csv\",\"tfidf_tsvd_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_tfidf[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_train_\"+f)\n",
    "    work_test_tfidf[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train=pd.read_csv(\"checkpoints_databases/new_working_train.csv\")\n",
    "new_test=pd.read_csv(\"checkpoints_databases/new_working_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=np.array(new_train.Class)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train=new_train.ID\n",
    "ID_test=new_test.ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5, random_state=26, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanw2v_100 -1.07331330963 std:w2v_100 0.0364550094376\n",
      "meanw2v_200 -1.08126323561 std:w2v_200 0.0447141389549\n",
      "meanw2v_300 -1.06656954989 std:w2v_300 0.0304045282825\n"
     ]
    }
   ],
   "source": [
    "clf_xgb=XGBClassifier(max_depth=5, objective=\"multi:softprob\",seed=26)\n",
    "for name in work_train_w2v:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_w2v[name]),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))\n",
    "#300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanbio -1.09764345161 std:bio 0.034944053108\n"
     ]
    }
   ],
   "source": [
    "for name in work_train_bio:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_bio[name]),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))\n",
    "#k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meantfidf_tsvd_100 -1.00962632533 std:tfidf_tsvd_100 0.0343409770402\n",
      "meantfidf_tsvd_200 -1.01784942814 std:tfidf_tsvd_200 0.0316089971765\n",
      "meantfidf_tsvd_300 -1.01995911039 std:tfidf_tsvd_300 0.0456225357959\n"
     ]
    }
   ],
   "source": [
    "for name in work_train_tfidf:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_tfidf[name]),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))\n",
    "#100 tsvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH PHASE ALL ALGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train= {} \n",
    "work_test = {}\n",
    "#To complete\n",
    "pre_process=[\"bio.csv\",\"tfidf_tsvd_100.csv\",\"w2v_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_train_\"+f)\n",
    "    work_test[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"new_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train=pd.read_csv(\"../window/w_meta_features/meta_train_l1l2.csv\")\n",
    "feat_test=pd.read_csv(\"../window/w_meta_features/meta_test_l1l2.csv\")\n",
    "feat_train=feat_train.drop(\"ID\",axis=1)\n",
    "feat_test=feat_test.drop(\"ID\",axis=1)\n",
    "feat_ext_train=pd.read_csv(\"w_meta_features/new_working_train_ext.csv\")\n",
    "feat_ext_test=pd.read_csv(\"w_meta_features/new_working_test_ext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a \"external model\" xgb alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_bio=pd.read_csv(\"../l2_meta_features/svd25_biological.csv\")\n",
    "ext_cell=pd.read_csv(\"../l2_meta_features/svd25_cellular.csv\")\n",
    "ext_mol=pd.read_csv(\"../l2_meta_features/svd25_molecular.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_bio=ext_bio.drop([\"Class\",\"Gene\",\"ID\",\"Variation\"],axis=1)\n",
    "ext_cell=ext_cell.drop([\"Class\",\"Gene\",\"ID\",\"Variation\"],axis=1)\n",
    "ext_mol=ext_mol.drop([\"Class\",\"Gene\",\"ID\",\"Variation\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext=pd.concat((ext_bio,ext_cell,ext_mol),axis=1)\n",
    "model_ext_train=model_ext[:len(y)]\n",
    "model_ext_test=model_ext[len(y):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 21.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bio\n",
      "[mean: -1.16119, std: 0.03745, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16675, std: 0.04054, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.18082, std: 0.04592, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.17657, std: 0.04654, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.16237, std: 0.03865, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16553, std: 0.04232, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.17751, std: 0.04598, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.17329, std: 0.04792, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.17571, std: 0.04425, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16764, std: 0.04546, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.22929, std: 0.05756, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.21748, std: 0.05806, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.17143, std: 0.04338, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16980, std: 0.04578, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.21624, std: 0.05479, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.20462, std: 0.05435, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.19712, std: 0.04761, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.18592, std: 0.04799, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.26280, std: 0.06211, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.23596, std: 0.05948, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.18875, std: 0.04506, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.18116, std: 0.04503, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.24414, std: 0.05895, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.21839, std: 0.05396, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.16209, std: 0.03817, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16611, std: 0.04276, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.18344, std: 0.04707, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.17802, std: 0.04954, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.16072, std: 0.03762, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.16545, std: 0.04344, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.17811, std: 0.04706, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.17434, std: 0.04783, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.17879, std: 0.04390, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.17217, std: 0.04619, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.23382, std: 0.05851, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.22413, std: 0.05818, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.17332, std: 0.04139, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.17129, std: 0.04613, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.21869, std: 0.05463, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.20848, std: 0.05538, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.19895, std: 0.04860, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.18881, std: 0.04851, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.26712, std: 0.06432, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.24019, std: 0.05908, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.18927, std: 0.04628, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.18408, std: 0.04703, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.24517, std: 0.05974, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.22039, std: 0.05630, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}]\n",
      "{'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "-1.160723399\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_xgb=XGBClassifier(max_depth=3, objective=\"multi:softprob\",seed=26)\n",
    "param_test= {\n",
    "    \"max_depth\" : [3,5,7],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"n_estimators\" : [100,200],\n",
    "    \"subsample\":[0.8,1],\n",
    "    \"colsample_bytree\":[0.8,1]\n",
    "}\n",
    "\n",
    "gsearch=GridSearchCV(estimator=clf_xgb, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "gsearch.fit(np.array(model_ext_train),y)\n",
    "print(name)\n",
    "print(gsearch.grid_scores_)\n",
    "print(gsearch.best_params_)\n",
    "print(gsearch.best_score_)\n",
    "# best grid : {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now add the others models that worked on the window pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gene features + manual features on variations + text embeddings\n",
    "work_train_final={}\n",
    "work_test_final={}\n",
    "work_train_ext={}\n",
    "work_test_ext={}\n",
    "for name in work_train:\n",
    "    work_train_final[name]=pd.concat((work_train[name],feat_train),axis=1)\n",
    "    work_test_final[name]=pd.concat((work_test[name],feat_test),axis=1)\n",
    "    work_train_ext[name]=pd.concat((work_train[name],feat_ext_train),axis=1)\n",
    "    work_test_ext[name]=pd.concat((work_test[name],feat_ext_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_franck=pd.concat((work_train[\"tfidf_tsvd_100\"],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 98.6min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1af8588f0a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwork_train_final\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgsearch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf_xgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neg_log_loss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_train_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \"\"\"\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m--> 564\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[1;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'timeout'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_xgb=XGBClassifier(max_depth=3, objective=\"multi:softprob\",seed=26)\n",
    "param_test= {\n",
    "    \"max_depth\" : [3,5,7],\n",
    "    \"min_child_weight\" : [1,3,5,7],\n",
    "    \"n_estimators\" : [100,200],\n",
    "    \"subsample\":[0.8,0.9,1],\n",
    "    \"colsample_bytree\":[0.8,0.9,1]\n",
    "}\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_xgb, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_final[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#bio : {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}\n",
    "#tfidf: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}\n",
    "#w2v: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lgbm=LGBMClassifier(seed=26)\n",
    "param_test= {\n",
    "    'n_estimators': [8,,16,24,36,48],\n",
    "    'num_leaves': [6,12,16,22],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'colsample_bytree' : [0.7,0.8,0.9],\n",
    "    'subsample' : [0.7,0.8,0.9]\n",
    "    }\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_lgbm, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_final[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
    "#tfidf : {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
    "#w2v : {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bio\n",
      "[mean: -2.16489, std: 0.00029, params: {'C': 0.001, 'penalty': 'l1'}, mean: -2.10134, std: 0.00074, params: {'C': 0.001, 'penalty': 'l2'}, mean: -1.91341, std: 0.00133, params: {'C': 0.01, 'penalty': 'l1'}, mean: -1.87990, std: 0.00458, params: {'C': 0.01, 'penalty': 'l2'}, mean: -1.74917, std: 0.01223, params: {'C': 0.1, 'penalty': 'l1'}, mean: -1.60313, std: 0.01831, params: {'C': 0.1, 'penalty': 'l2'}, mean: -1.36559, std: 0.02814, params: {'C': 1, 'penalty': 'l1'}, mean: -1.36289, std: 0.02980, params: {'C': 1, 'penalty': 'l2'}, mean: -1.16996, std: 0.04263, params: {'C': 10, 'penalty': 'l1'}, mean: -1.20232, std: 0.03268, params: {'C': 10, 'penalty': 'l2'}, mean: -1.38548, std: 0.06647, params: {'C': 100, 'penalty': 'l1'}, mean: -1.16012, std: 0.04500, params: {'C': 100, 'penalty': 'l2'}, mean: -1.72748, std: 0.12829, params: {'C': 1000, 'penalty': 'l1'}, mean: -1.30786, std: 0.05802, params: {'C': 1000, 'penalty': 'l2'}]\n",
      "{'C': 100, 'penalty': 'l2'}\n",
      "-1.16011995766\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   40.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -2.16489, std: 0.00029, params: {'C': 0.001, 'penalty': 'l1'}, mean: -2.12527, std: 0.00046, params: {'C': 0.001, 'penalty': 'l2'}, mean: -1.91341, std: 0.00133, params: {'C': 0.01, 'penalty': 'l1'}, mean: -1.89633, std: 0.00310, params: {'C': 0.01, 'penalty': 'l2'}, mean: -1.58242, std: 0.01423, params: {'C': 0.1, 'penalty': 'l1'}, mean: -1.52553, std: 0.01136, params: {'C': 0.1, 'penalty': 'l2'}, mean: -1.16043, std: 0.01439, params: {'C': 1, 'penalty': 'l1'}, mean: -1.19221, std: 0.01642, params: {'C': 1, 'penalty': 'l2'}, mean: -1.08386, std: 0.02675, params: {'C': 10, 'penalty': 'l1'}, mean: -1.07288, std: 0.02133, params: {'C': 10, 'penalty': 'l2'}, mean: -1.20319, std: 0.04193, params: {'C': 100, 'penalty': 'l1'}, mean: -1.10435, std: 0.03337, params: {'C': 100, 'penalty': 'l2'}, mean: -1.30641, std: 0.06715, params: {'C': 1000, 'penalty': 'l1'}, mean: -1.19175, std: 0.04642, params: {'C': 1000, 'penalty': 'l2'}]\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "-1.07288442852\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.1s\n"
     ]
    }
   ],
   "source": [
    "clf_log=LogisticRegression()\n",
    "param_test= {\n",
    "    \"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"penalty\" : [\"l1\",\"l2\"]\n",
    "}\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_log, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_final[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#bio : {'C': 10, 'penalty': 'l1'}\n",
    "#tfidf : {'C': 10, 'penalty': 'l1'}\n",
    "#w2v : {'C': 1, 'penalty': 'l1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log=LogisticRegression()\n",
    "param_test= {\n",
    "    \"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"penalty\" : [\"l1\",\"l2\"]\n",
    "}\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_log, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_ext[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada=AdaBoostClassifier(n_estimators=100, learning_rate=0.3, algorithm=\"SAMME.R\", random_state=26)\n",
    "param_test={\n",
    "    \"n_estimators\":[50,70,100,200],\n",
    "    \"learning_rate\":[0.01,0.05,0.1,0.2]\n",
    "}\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_ada, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_final[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'learning_rate': 0.01, 'n_estimators': 50}\n",
    "#tfidf : {'learning_rate': 0.01, 'n_estimators': 70}\n",
    "#w2v :{'learning_rate': 0.01, 'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt=RandomForestClassifier()\n",
    "param_test={\n",
    "    \"max_depth\":[10,15,20,25,30,35],\n",
    "    \"n_estimators\":[200,300,400]\n",
    "}\n",
    "for name in work_train_final:\n",
    "    gsearch=GridSearchCV(estimator=clf_dt, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(np.array(work_train_final[name]),y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'max_depth': 15, 'n_estimators': 300}\n",
    "#tfidf :{'max_depth': 20, 'n_estimators': 400}\n",
    "#w2v : {'max_depth': 15, 'n_estimators': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING PHASE ALL ALGOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gen(X,X_test,y,classifier,file,five_fold_predict=True):\n",
    "    #if not os.path.exists(\"scores/\"+file):\n",
    "    #   os.makedirs(\"scores/\"+file)\n",
    "    if five_fold_predict:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "            fold += 1\n",
    "\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            p_test = clf.predict_proba(X_test)\n",
    "            y_test += p_test/5\n",
    "\n",
    "    classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"new_scores/new_stack_test/new_{}.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train \") #peut etre que to array est exclusivement pour les xgb\n",
    "\n",
    "    y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict_proba\")\n",
    "    subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "    subm1['ID'] = ID_train\n",
    "    subm1.to_csv(\"new_scores/new_stack_train/new_{}.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost here\n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 243) (743, 243)\n",
      "Fold 2 (2949, 243) (740, 243)\n",
      "Fold 3 (2952, 243) (737, 243)\n",
      "Fold 4 (2954, 243) (735, 243)\n",
      "Fold 5 (2955, 243) (734, 243)\n",
      "cross_val sur train \n",
      "lgbm here\n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 243) (743, 243)\n",
      "Fold 2 (2949, 243) (740, 243)\n",
      "Fold 3 (2952, 243) (737, 243)\n",
      "Fold 4 (2954, 243) (735, 243)\n",
      "Fold 5 (2955, 243) (734, 243)\n",
      "cross_val sur train \n",
      "logreg here\n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 243) (743, 243)\n",
      "Fold 2 (2949, 243) (740, 243)\n",
      "Fold 3 (2952, 243) (737, 243)\n",
      "Fold 4 (2954, 243) (735, 243)\n",
      "Fold 5 (2955, 243) (734, 243)\n",
      "cross_val sur train \n",
      "adaboost here\n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 243) (743, 243)\n",
      "Fold 2 (2949, 243) (740, 243)\n",
      "Fold 3 (2952, 243) (737, 243)\n",
      "Fold 4 (2954, 243) (735, 243)\n",
      "Fold 5 (2955, 243) (734, 243)\n",
      "cross_val sur train \n",
      "random forest here\n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 142) (743, 142)\n",
      "Fold 2 (2949, 142) (740, 142)\n",
      "Fold 3 (2952, 142) (737, 142)\n",
      "Fold 4 (2954, 142) (735, 142)\n",
      "Fold 5 (2955, 142) (734, 142)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 243) (743, 243)\n",
      "Fold 2 (2949, 243) (740, 243)\n",
      "Fold 3 (2952, 243) (737, 243)\n",
      "Fold 4 (2954, 243) (735, 243)\n",
      "Fold 5 (2955, 243) (734, 243)\n",
      "cross_val sur train \n"
     ]
    }
   ],
   "source": [
    "dic_xgb={\"xgb_bio\":XGBClassifier(colsample_bytree=0.8,max_depth=7,min_child_weight=3,n_estimators=100,subsample=0.8,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_tfidf\":XGBClassifier(colsample_bytree=0.8,max_depth=5,min_child_weight=3,n_estimators=100,subsample=1,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_w2v\":XGBClassifier(colsample_bytree=0.8,max_depth=5,min_child_weight=3,n_estimators=100,subsample=0.8,objective=\"multi:softprob\",seed=26)}\n",
    "\n",
    "dic_lgbm={\"lgbm_bio\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.8,n_estimators=48,num_leaves=22,subsample=0.8,seed=26),\n",
    "        \"lgbm_tfidf\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.8,n_estimators=48,num_leaves=22,subsample=0.8,seed=26),\n",
    "        \"lgbm_w2v\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.8,n_estimators=48,num_leaves=22,subsample=0.8,seed=26)}\n",
    "dic_lr={\"lr_bio\":LogisticRegression(C=10,penalty=\"l1\"),\n",
    "        \"lr_tfidf\":LogisticRegression(C=10,penalty=\"l1\"),\n",
    "        \"lr_w2v\":LogisticRegression(C=1,penalty=\"l1\")}\n",
    "dic_ada={\"ada_bio\":AdaBoostClassifier(n_estimators=50, learning_rate=0.01, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_tfidf\":AdaBoostClassifier(n_estimators=70, learning_rate=0.01, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_w2v\":AdaBoostClassifier(n_estimators=50, learning_rate=0.01, algorithm=\"SAMME.R\", random_state=26)}\n",
    "dic_rf={\"rf_bio\":RandomForestClassifier(n_estimators=400,max_depth=25,random_state=26),\n",
    "        \"rf_tfidf\":RandomForestClassifier(n_estimators=400,max_depth=20,random_state=26),\n",
    "        \"rf_w2v\":RandomForestClassifier(n_estimators=300,max_depth=15,random_state=26)}\n",
    "\n",
    "print(\"xgboost here\")\n",
    "for clf,name in zip(dic_xgb.keys(),work_train_final.keys()):\n",
    "    model_gen(X=np.array(work_train_final[name]),X_test=np.array(work_test_final[name].drop(\"ID\",axis=1)),y=y,classifier=dic_xgb[clf],file=clf)\n",
    "print(\"lgbm here\")\n",
    "for clf,name in zip(dic_lgbm.keys(),work_train_final.keys()):\n",
    "    model_gen(X=np.array(work_train_final[name]),X_test=np.array(work_test_final[name].drop(\"ID\",axis=1)),y=y,classifier=dic_lgbm[clf],file=clf)\n",
    "print(\"logreg here\")\n",
    "for clf,name in zip(dic_lr.keys(),work_train_final.keys()):\n",
    "    model_gen(X=np.array(work_train_final[name]),X_test=np.array(work_test_final[name].drop(\"ID\",axis=1)),y=y,classifier=dic_lr[clf],file=clf)\n",
    "print(\"adaboost here\")\n",
    "for clf,name in zip(dic_ada.keys(),work_train_final.keys()):\n",
    "    model_gen(X=np.array(work_train_final[name]),X_test=np.array(work_test_final[name].drop(\"ID\",axis=1)),y=y,classifier=dic_ada[clf],file=clf)\n",
    "print(\"random forest here\")\n",
    "for clf,name in zip(dic_rf.keys(),work_train_final.keys()):\n",
    "    model_gen(X=np.array(work_train_final[name]),X_test=np.array(work_test_final[name].drop(\"ID\",axis=1)),y=y,classifier=dic_rf[clf],file=clf)\n",
    "\n",
    "ext_xgb=XGBClassifier(tocomplete)\n",
    "model_gen(X=np.array(model_ext_train),X_test=np.array(model_ext_test),y=y,classifier=ext_xgb,file=\"xgb_external\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
