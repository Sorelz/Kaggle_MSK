{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('bases/training_variants')\n",
    "test = pd.read_csv('bases/test_variants')\n",
    "train_texts = pd.read_csv('bases/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")\n",
    "test_texts = pd.read_csv('bases/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_texts, how='left', on='ID')\n",
    "test = pd.merge(test, test_texts, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### process the train and test set together\n",
    "data_all = pd.concat((train, test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}~0123456789') \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc,lemmatiz=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 = [re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    if lemmatiz:\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free_lem.split())\n",
    "        return normalized\n",
    "    else:\n",
    "        punc_free = \"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        return punc_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No lemmatization for the moment, be careful not to lemmatize then w2vec\n",
    "data_all.Text = [clean(doc,lemmatiz=True) for doc in data_all.Text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID_train=train.ID\n",
    "ID_test=test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=train[\"Class\"]-1\n",
    "X_train=train.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "X_test=test.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "txt_no_dup=train[\"Text\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=10, max_features=15000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(txt_no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_comp=[10,20,30,40,50,60,80,100,120,150,170,200]\n",
    "dic_svd={}\n",
    "for comp in list_comp:\n",
    "    dic_svd[comp]=TruncatedSVD(n_components=comp,n_iter=10,random_state=26)\n",
    "tsvd_train,tsvd_test = {},{}\n",
    "for sv in dic_svd:\n",
    "    tsvd_train[sv]=dic_svd[sv].fit_transform(X_train_text)\n",
    "    tsvd_test[sv]=dic_svd[sv].transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in dic_svd:\n",
    "    for i in range(n):\n",
    "        X_train['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_train[n][:, i]\n",
    "        X_test['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_test[sv][:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_0=np.array(X_train.drop(\"Text\",axis=1))\n",
    "X_test_0=np.array(X_test.drop(\"Text\",axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "def model_ann():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim=1030,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dense(9,activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=model_ann, epochs=30, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_classifier={\"XGB_medium\":XGBClassifier(n_estimators=200,max_depth=5, objective=\"multi:softprob\",subsample=0.7,seed=26),\n",
    "    \"XGB_small\":XGBClassifier(max_depth=2,objective=\"multi:softprob\",subsample=0.5,seed=26),\n",
    "                   \"XGB_tall\":XGBClassifier(n_estimators=300,max_depth=7,subsample=0.9,objective=\"multi:softprob\",seed=26)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gen_nowdw(X,X_test,y,classifier,file,neural_net=False):\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=26, shuffle=True)\n",
    "    if neural_net==False:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            fold += 1\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            y_test = clf.predict(X_test)\n",
    "    else:\n",
    "        print(\"One Fold predict for NN\")\n",
    "        clf=classifier\n",
    "        clf.fit(X,y)\n",
    "        y_test=clf.predict(X_test)\n",
    "    classes = \"class_predicted\"+file\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"scores/stack_test/nowdw_{}.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train\") #peut etre que to array est exclusivement pour les xgb\n",
    "    \n",
    "    if os.path.isfile(\"scores/stack_train/nowdw_{}.csv\".format(file)):\n",
    "        print(\"not necessary, already done\")\n",
    "    else:\n",
    "        if neural_net==False:\n",
    "            y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict\")\n",
    "        else:\n",
    "            y_pred=clf.predict(X)\n",
    "        subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "        subm1['ID'] = ID_train\n",
    "        subm1.to_csv(\"scores/stack_train/nowdw_{}.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (2653, 1030) (668, 1030)\n",
      "Fold 2 (2654, 1030) (667, 1030)\n",
      "Fold 3 (2657, 1030) (664, 1030)\n",
      "Fold 4 (2659, 1030) (662, 1030)\n"
     ]
    }
   ],
   "source": [
    "for clf in dic_classifier:\n",
    "    model_gen_nowdw(X_train_0,X_test_0,y,dic_classifier[clf],file=clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen_nowdw(X_train_0,X_test_0,to_categorical(y,9),model,\"neural_net\",neural_net=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
