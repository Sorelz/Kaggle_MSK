{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LF/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('bases/training_variants')\n",
    "test = pd.read_csv('bases/test_variants')\n",
    "train_texts = pd.read_csv('bases/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")\n",
    "test_texts = pd.read_csv('bases/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_texts, how='left', on='ID')\n",
    "test = pd.merge(test, test_texts, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### process the train and test set together\n",
    "data_all = pd.concat((train, test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}~0123456789') \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc,lemmatiz=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 = [re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    if lemmatiz:\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free_lem.split())\n",
    "        return normalized\n",
    "    else:\n",
    "        punc_free = \"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        return punc_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No lemmatization for the moment, be careful not to lemmatize then w2vec\n",
    "data_all.Text = [clean(doc,lemmatiz=True) for doc in data_all.Text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID_train=train.ID\n",
    "ID_test=test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=train[\"Class\"]-1\n",
    "X_train=train.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "X_test=test.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "txt_no_dup=train[\"Text\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=10, max_features=15000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(txt_no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_comp=[10,20,30,40,50,60,80,100,120,150,170,200]\n",
    "dic_svd={}\n",
    "for comp in list_comp:\n",
    "    dic_svd[comp]=TruncatedSVD(n_components=comp,n_iter=10,random_state=26)\n",
    "tsvd_train,tsvd_test = {},{}\n",
    "for sv in dic_svd:\n",
    "    tsvd_train[sv]=dic_svd[sv].fit_transform(X_train_text)\n",
    "    tsvd_test[sv]=dic_svd[sv].transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in dic_svd:\n",
    "    for i in range(n):\n",
    "        X_train['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_train[n][:, i]\n",
    "        X_test['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_test[sv][:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_0=np.array(X_train.drop(\"Text\",axis=1))\n",
    "X_test_0=np.array(X_test.drop(\"Text\",axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "def model_ann():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim=1030,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dense(9,activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_classifier={\"XGB_medium\":XGBClassifier(n_estimators=200,max_depth=5, objective=\"multi:softprob\",subsample=0.7,seed=26),\n",
    "    \"XGB_small\":XGBClassifier(max_depth=2,objective=\"multi:softprob\",subsample=0.5,seed=26),\n",
    "                   \"XGB_tall\":XGBClassifier(n_estimators=300,max_depth=7,subsample=0.9,objective=\"multi:softprob\",seed=26)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gen_nowdw(X,X_test,y,classifier,file,neural_net=False):\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=26, shuffle=True)\n",
    "    if neural_net==False:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            fold += 1\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            y_test = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        print(\"One Fold predict for NN\")\n",
    "        clf=classifier\n",
    "        clf.fit(X,y)\n",
    "        y_test=clf.predict_proba(X_test)\n",
    "    classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"scores/stack_test/nowdw_{}_new.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train\") #peut etre que to array est exclusivement pour les xgb\n",
    "    \n",
    "    if os.path.isfile(\"scores/stack_train/nowdw_{}_new.csv\".format(file)):\n",
    "        print(\"not necessary, already done\")\n",
    "    else:\n",
    "        if neural_net==False:\n",
    "            y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict_proba\")\n",
    "        else:\n",
    "            y_pred=clf.predict_proba(X)\n",
    "        subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "        subm1['ID'] = ID_train\n",
    "        subm1.to_csv(\"scores/stack_train/nowdw_{}_new.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (2653, 1030) (668, 1030)\n",
      "Fold 2 (2654, 1030) (667, 1030)\n",
      "Fold 3 (2657, 1030) (664, 1030)\n",
      "Fold 4 (2659, 1030) (662, 1030)\n",
      "Fold 5 (2661, 1030) (660, 1030)\n",
      "cross_val sur train\n",
      "Fold 1 (2653, 1030) (668, 1030)\n",
      "Fold 2 (2654, 1030) (667, 1030)\n",
      "Fold 3 (2657, 1030) (664, 1030)\n",
      "Fold 4 (2659, 1030) (662, 1030)\n",
      "Fold 5 (2661, 1030) (660, 1030)\n",
      "cross_val sur train\n",
      "Fold 1 (2653, 1030) (668, 1030)\n",
      "Fold 2 (2654, 1030) (667, 1030)\n",
      "Fold 3 (2657, 1030) (664, 1030)\n",
      "Fold 4 (2659, 1030) (662, 1030)\n",
      "Fold 5 (2661, 1030) (660, 1030)\n",
      "cross_val sur train\n"
     ]
    }
   ],
   "source": [
    "for clf in dic_classifier:\n",
    "    model_gen_nowdw(X_train_0,X_test_0,y,dic_classifier[clf],file=clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPEATABLE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321, 1030)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model_nn(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim=1030,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dense(9,activation=\"softmax\"))\n",
    "    epochs=200\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    es=EarlyStopping(monitor=\"loss\",mode=\"min\",patience=10)\n",
    "    checkpoint=ModelCheckpoint(filepath=\"checkann\",save_best_only=True,monitor=\"loss\",mode=\"min\")\n",
    "    model.fit(X, y,epochs=100,batch_size=50,callbacks=[es,checkpoint],validation_split=0.2)\n",
    "    model.load_weights(\"checkann\")\n",
    "    y_test=model.predict_proba(X_test_0)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat=to_categorical(y,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416/668 [=================>............] - ETA: 0sloss: 2.41\n",
      "448/667 [===================>..........] - ETA: 0sloss: 2.15\n",
      "416/664 [=================>............] - ETA: 0sloss: 2.00\n",
      "416/662 [=================>............] - ETA: 0sloss: 1.96\n",
      "384/660 [================>.............] - ETA: 0sloss: 2.59\n",
      "2.22 (+/- 0.25)\n"
     ]
    }
   ],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=26)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_train_0, y):\n",
    "  # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=1030, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300,activation=\"relu\"))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    es=EarlyStopping(monitor=\"loss\",mode=\"min\",patience=10)\n",
    "    checkpoint=ModelCheckpoint(filepath=\"checknn\",save_best_only=True,monitor=\"loss\",mode=\"min\")\n",
    "    model.fit(X_train_0[train], y_cat[train],epochs=100,batch_size=50,validation_split=0.2,callbacks=[es,checkpoint],verbose=0)\n",
    "    model.load_weights(\"checknn\")\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_train_0[test], y_cat[test])\n",
    "    print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "    cvscores.append(scores[0] )\n",
    "print(\"%.2f (+/- %.2f)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "subm = pd.DataFrame(y_sol, columns=classes)\n",
    "subm['ID'] = ID_test\n",
    "subm.to_csv(\"scores/stack_test/neural_net_overfitting.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2656 samples, validate on 665 samples\n",
      "Epoch 1/100\n",
      "2656/2656 [==============================] - 6s - loss: 1.4908 - acc: 0.4974 - val_loss: 1.5952 - val_acc: 0.3985\n",
      "Epoch 2/100\n",
      "2656/2656 [==============================] - 1s - loss: 1.0469 - acc: 0.6096 - val_loss: 1.5901 - val_acc: 0.3744\n",
      "Epoch 3/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.9519 - acc: 0.6434 - val_loss: 1.4809 - val_acc: 0.3985\n",
      "Epoch 4/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.8743 - acc: 0.6642 - val_loss: 1.5278 - val_acc: 0.4827\n",
      "Epoch 5/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.8390 - acc: 0.6709 - val_loss: 1.6329 - val_acc: 0.3955\n",
      "Epoch 6/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7931 - acc: 0.6875 - val_loss: 1.6695 - val_acc: 0.4707\n",
      "Epoch 7/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7612 - acc: 0.6928 - val_loss: 1.7883 - val_acc: 0.4000\n",
      "Epoch 8/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7397 - acc: 0.7007 - val_loss: 1.6685 - val_acc: 0.4466\n",
      "Epoch 9/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7076 - acc: 0.7071 - val_loss: 1.8724 - val_acc: 0.4195\n",
      "Epoch 10/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6899 - acc: 0.7252 - val_loss: 1.7997 - val_acc: 0.4331\n",
      "Epoch 11/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6800 - acc: 0.7252 - val_loss: 1.9099 - val_acc: 0.4436\n",
      "Epoch 12/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6448 - acc: 0.7334 - val_loss: 2.1079 - val_acc: 0.4150\n",
      "Epoch 13/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6374 - acc: 0.7316 - val_loss: 2.0397 - val_acc: 0.3759\n",
      "Epoch 14/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6132 - acc: 0.7538 - val_loss: 1.8889 - val_acc: 0.4932\n",
      "Epoch 15/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5922 - acc: 0.7530 - val_loss: 2.1314 - val_acc: 0.4617\n",
      "Epoch 16/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5662 - acc: 0.7654 - val_loss: 2.1552 - val_acc: 0.4977\n",
      "Epoch 17/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5596 - acc: 0.7643 - val_loss: 2.2104 - val_acc: 0.4872\n",
      "Epoch 18/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5366 - acc: 0.7737 - val_loss: 2.2836 - val_acc: 0.3925\n",
      "Epoch 19/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5286 - acc: 0.7880 - val_loss: 2.2638 - val_acc: 0.4451\n",
      "Epoch 20/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5163 - acc: 0.7775 - val_loss: 2.2919 - val_acc: 0.4842\n",
      "Epoch 21/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4909 - acc: 0.7775 - val_loss: 2.5890 - val_acc: 0.4752\n",
      "Epoch 22/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4977 - acc: 0.7771 - val_loss: 2.4448 - val_acc: 0.4692\n",
      "Epoch 23/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4829 - acc: 0.7907 - val_loss: 2.5735 - val_acc: 0.4195\n",
      "Epoch 24/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4590 - acc: 0.8084 - val_loss: 2.5830 - val_acc: 0.4000\n",
      "Epoch 25/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4552 - acc: 0.8035 - val_loss: 2.6705 - val_acc: 0.4767\n",
      "Epoch 26/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4377 - acc: 0.8102 - val_loss: 2.9108 - val_acc: 0.3805\n",
      "Epoch 27/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4252 - acc: 0.8144 - val_loss: 2.9907 - val_acc: 0.3880\n",
      "Epoch 28/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4323 - acc: 0.8099 - val_loss: 2.7600 - val_acc: 0.4511\n",
      "Epoch 29/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4429 - acc: 0.8061 - val_loss: 2.8676 - val_acc: 0.4256\n",
      "Epoch 30/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4400 - acc: 0.8031 - val_loss: 2.8323 - val_acc: 0.3940\n",
      "Epoch 31/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4110 - acc: 0.8166 - val_loss: 2.9583 - val_acc: 0.4060\n",
      "Epoch 32/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4195 - acc: 0.8133 - val_loss: 2.9857 - val_acc: 0.4000\n",
      "Epoch 33/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4147 - acc: 0.8133 - val_loss: 3.1089 - val_acc: 0.3774\n",
      "Epoch 34/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3944 - acc: 0.8208 - val_loss: 3.3276 - val_acc: 0.3835\n",
      "Epoch 35/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4054 - acc: 0.8189 - val_loss: 3.2544 - val_acc: 0.3850\n",
      "Epoch 36/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4040 - acc: 0.8272 - val_loss: 3.1026 - val_acc: 0.4120\n",
      "Epoch 37/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3825 - acc: 0.8249 - val_loss: 3.2695 - val_acc: 0.39550.82\n",
      "Epoch 38/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3804 - acc: 0.8340 - val_loss: 3.4313 - val_acc: 0.4075\n",
      "Epoch 39/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3882 - acc: 0.8242 - val_loss: 3.3794 - val_acc: 0.4150\n",
      "Epoch 40/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3682 - acc: 0.8347 - val_loss: 3.3875 - val_acc: 0.4060\n",
      "Epoch 41/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4047 - acc: 0.8223 - val_loss: 3.5058 - val_acc: 0.4286\n",
      "Epoch 42/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3855 - acc: 0.8268 - val_loss: 3.5440 - val_acc: 0.3729\n",
      "Epoch 43/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3910 - acc: 0.8163 - val_loss: 3.4858 - val_acc: 0.4075\n",
      "Epoch 44/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3785 - acc: 0.8294 - val_loss: 3.4746 - val_acc: 0.4120\n",
      "Epoch 45/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3793 - acc: 0.8253 - val_loss: 3.7127 - val_acc: 0.4256\n",
      "Epoch 46/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3737 - acc: 0.8294 - val_loss: 3.5945 - val_acc: 0.4075\n",
      "Epoch 47/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3738 - acc: 0.8347 - val_loss: 3.5424 - val_acc: 0.4045\n",
      "Epoch 48/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3730 - acc: 0.8309 - val_loss: 3.6990 - val_acc: 0.3579\n",
      "Epoch 49/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3566 - acc: 0.8313 - val_loss: 3.7571 - val_acc: 0.3759\n",
      "Epoch 50/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3619 - acc: 0.8309 - val_loss: 3.9724 - val_acc: 0.3729\n",
      "Epoch 51/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3575 - acc: 0.8272 - val_loss: 4.0607 - val_acc: 0.3805\n",
      "Epoch 52/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3713 - acc: 0.8309 - val_loss: 3.7800 - val_acc: 0.4105\n",
      "Epoch 53/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3610 - acc: 0.8302 - val_loss: 3.9895 - val_acc: 0.3940\n",
      "Epoch 54/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3692 - acc: 0.8313 - val_loss: 4.0018 - val_acc: 0.4000\n",
      "Epoch 55/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3578 - acc: 0.8407 - val_loss: 4.1644 - val_acc: 0.3744\n",
      "Epoch 56/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3581 - acc: 0.8426 - val_loss: 3.9643 - val_acc: 0.3609\n",
      "Epoch 57/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3555 - acc: 0.8313 - val_loss: 3.7877 - val_acc: 0.3850\n",
      "Epoch 58/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3739 - acc: 0.8321 - val_loss: 3.9637 - val_acc: 0.4000\n",
      "Epoch 59/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3514 - acc: 0.8343 - val_loss: 3.8865 - val_acc: 0.3865\n",
      "Epoch 60/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3538 - acc: 0.8347 - val_loss: 3.9797 - val_acc: 0.4045\n",
      "Epoch 61/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3565 - acc: 0.8389 - val_loss: 3.8469 - val_acc: 0.3940\n",
      "Epoch 62/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3577 - acc: 0.8400 - val_loss: 3.7010 - val_acc: 0.4105\n",
      "Epoch 63/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3475 - acc: 0.8419 - val_loss: 3.9185 - val_acc: 0.4286\n",
      "Epoch 64/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3598 - acc: 0.8313 - val_loss: 3.9574 - val_acc: 0.4000\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 0s - loss: 0.3440 - acc: 0.8404 - val_loss: 4.0004 - val_acc: 0.4045\n",
      "Epoch 66/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3307 - acc: 0.8486 - val_loss: 4.1251 - val_acc: 0.4165\n",
      "Epoch 67/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3335 - acc: 0.8419 - val_loss: 4.2194 - val_acc: 0.3880\n",
      "Epoch 68/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3331 - acc: 0.8441 - val_loss: 4.4825 - val_acc: 0.3699\n",
      "Epoch 69/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3357 - acc: 0.8370 - val_loss: 4.2547 - val_acc: 0.3850\n",
      "Epoch 70/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3353 - acc: 0.8392 - val_loss: 4.3336 - val_acc: 0.3744\n",
      "Epoch 71/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3347 - acc: 0.8441 - val_loss: 4.3449 - val_acc: 0.3624\n",
      "Epoch 72/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3378 - acc: 0.8502 - val_loss: 4.3508 - val_acc: 0.3895\n",
      "Epoch 73/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3330 - acc: 0.8449 - val_loss: 4.2571 - val_acc: 0.3910\n",
      "Epoch 74/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3258 - acc: 0.8471 - val_loss: 4.3241 - val_acc: 0.3759\n",
      "Epoch 75/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3300 - acc: 0.8494 - val_loss: 4.2084 - val_acc: 0.3970\n",
      "Epoch 76/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3382 - acc: 0.8419 - val_loss: 4.1118 - val_acc: 0.3940\n",
      "Epoch 77/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3257 - acc: 0.8389 - val_loss: 4.2642 - val_acc: 0.3744\n",
      "Epoch 78/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3300 - acc: 0.8385 - val_loss: 4.2640 - val_acc: 0.4030\n",
      "Epoch 79/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3263 - acc: 0.8453 - val_loss: 4.4582 - val_acc: 0.3910\n",
      "Epoch 80/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3196 - acc: 0.8430 - val_loss: 4.5125 - val_acc: 0.4045\n",
      "Epoch 81/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3103 - acc: 0.8520 - val_loss: 4.2801 - val_acc: 0.4226\n",
      "Epoch 82/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3163 - acc: 0.8475 - val_loss: 4.2505 - val_acc: 0.4015\n",
      "Epoch 83/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3113 - acc: 0.8524 - val_loss: 4.0124 - val_acc: 0.4226\n",
      "Epoch 84/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3237 - acc: 0.8468 - val_loss: 4.2761 - val_acc: 0.4105\n",
      "Epoch 85/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3175 - acc: 0.8539 - val_loss: 4.2128 - val_acc: 0.3895\n",
      "Epoch 86/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3171 - acc: 0.8547 - val_loss: 4.2799 - val_acc: 0.3714\n",
      "Epoch 87/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3134 - acc: 0.8535 - val_loss: 4.4257 - val_acc: 0.3639\n",
      "Epoch 88/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3249 - acc: 0.8434 - val_loss: 3.9006 - val_acc: 0.4737\n",
      "Epoch 89/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3102 - acc: 0.8550 - val_loss: 4.4000 - val_acc: 0.4632\n",
      "Epoch 90/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3231 - acc: 0.8509 - val_loss: 4.4746 - val_acc: 0.3789\n",
      "Epoch 91/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3190 - acc: 0.8486 - val_loss: 4.6662 - val_acc: 0.3985\n",
      "Epoch 92/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3126 - acc: 0.8554 - val_loss: 4.5786 - val_acc: 0.3835\n",
      "Epoch 93/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3175 - acc: 0.8456 - val_loss: 4.6091 - val_acc: 0.3985\n",
      "Epoch 94/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3155 - acc: 0.8445 - val_loss: 4.7084 - val_acc: 0.3820\n",
      "Epoch 95/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3093 - acc: 0.8596 - val_loss: 4.5107 - val_acc: 0.3910\n",
      "Epoch 96/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3110 - acc: 0.8558 - val_loss: 4.6511 - val_acc: 0.4647\n",
      "Epoch 97/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3114 - acc: 0.8535 - val_loss: 4.6185 - val_acc: 0.3955\n",
      "Epoch 98/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3171 - acc: 0.8464 - val_loss: 4.8084 - val_acc: 0.3774\n",
      "Epoch 99/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3217 - acc: 0.8475 - val_loss: 4.3597 - val_acc: 0.3865\n",
      "Epoch 100/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3381 - acc: 0.8434 - val_loss: 4.0521 - val_acc: 0.4752\n",
      "5408/5668 [===========================>..] - ETA: 0sTrain on 2656 samples, validate on 665 samples\n",
      "Epoch 1/100\n",
      "2656/2656 [==============================] - 6s - loss: 1.4762 - acc: 0.4706 - val_loss: 1.5941 - val_acc: 0.3925\n",
      "Epoch 2/100\n",
      "2656/2656 [==============================] - 1s - loss: 1.0583 - acc: 0.6122 - val_loss: 1.5298 - val_acc: 0.3699\n",
      "Epoch 3/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.9521 - acc: 0.6363 - val_loss: 1.5109 - val_acc: 0.4105\n",
      "Epoch 4/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.8788 - acc: 0.6691 - val_loss: 1.5789 - val_acc: 0.4000\n",
      "Epoch 5/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.8431 - acc: 0.6657 - val_loss: 1.5559 - val_acc: 0.4241\n",
      "Epoch 6/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7891 - acc: 0.6849 - val_loss: 1.6583 - val_acc: 0.3925\n",
      "Epoch 7/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7539 - acc: 0.6947 - val_loss: 1.6616 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7332 - acc: 0.7044 - val_loss: 1.7580 - val_acc: 0.4962\n",
      "Epoch 9/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6929 - acc: 0.7191 - val_loss: 1.7875 - val_acc: 0.4421\n",
      "Epoch 10/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6631 - acc: 0.7316 - val_loss: 1.9147 - val_acc: 0.4376\n",
      "Epoch 11/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6411 - acc: 0.7406 - val_loss: 2.0789 - val_acc: 0.4226\n",
      "Epoch 12/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6263 - acc: 0.7455 - val_loss: 1.8270 - val_acc: 0.4812\n",
      "Epoch 13/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6038 - acc: 0.7500 - val_loss: 2.1443 - val_acc: 0.4586\n",
      "Epoch 14/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5948 - acc: 0.7583 - val_loss: 1.9090 - val_acc: 0.4752\n",
      "Epoch 15/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5795 - acc: 0.7673 - val_loss: 2.0773 - val_acc: 0.4602\n",
      "Epoch 16/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5637 - acc: 0.7579 - val_loss: 2.1834 - val_acc: 0.4571\n",
      "Epoch 17/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5507 - acc: 0.7669 - val_loss: 2.2589 - val_acc: 0.4511\n",
      "Epoch 18/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5299 - acc: 0.7764 - val_loss: 2.1675 - val_acc: 0.4992\n",
      "Epoch 19/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5144 - acc: 0.7797 - val_loss: 2.4007 - val_acc: 0.4707\n",
      "Epoch 20/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4962 - acc: 0.7865 - val_loss: 2.6013 - val_acc: 0.4376\n",
      "Epoch 21/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4931 - acc: 0.7884 - val_loss: 2.5908 - val_acc: 0.3880\n",
      "Epoch 22/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4929 - acc: 0.7918 - val_loss: 2.5890 - val_acc: 0.3564\n",
      "Epoch 23/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4876 - acc: 0.7918 - val_loss: 2.5274 - val_acc: 0.4962\n",
      "Epoch 24/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4570 - acc: 0.8008 - val_loss: 2.7193 - val_acc: 0.4812\n",
      "Epoch 25/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4612 - acc: 0.7989 - val_loss: 2.6820 - val_acc: 0.3955\n",
      "Epoch 26/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4473 - acc: 0.8110 - val_loss: 2.7748 - val_acc: 0.4632\n",
      "Epoch 27/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4466 - acc: 0.8076 - val_loss: 2.8527 - val_acc: 0.4767\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 1s - loss: 0.4423 - acc: 0.8016 - val_loss: 3.0395 - val_acc: 0.4406\n",
      "Epoch 29/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4362 - acc: 0.8136 - val_loss: 2.8719 - val_acc: 0.4692\n",
      "Epoch 30/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4153 - acc: 0.8193 - val_loss: 3.0527 - val_acc: 0.4827\n",
      "Epoch 31/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4176 - acc: 0.8219 - val_loss: 3.1800 - val_acc: 0.4090\n",
      "Epoch 32/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4066 - acc: 0.8170 - val_loss: 3.2152 - val_acc: 0.4030\n",
      "Epoch 33/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4044 - acc: 0.8170 - val_loss: 3.0635 - val_acc: 0.4602\n",
      "Epoch 34/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4026 - acc: 0.8219 - val_loss: 3.1820 - val_acc: 0.4451\n",
      "Epoch 35/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3864 - acc: 0.8223 - val_loss: 3.7763 - val_acc: 0.4346\n",
      "Epoch 36/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4097 - acc: 0.8125 - val_loss: 3.1948 - val_acc: 0.4617\n",
      "Epoch 37/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3870 - acc: 0.8287 - val_loss: 3.2977 - val_acc: 0.4707\n",
      "Epoch 38/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3942 - acc: 0.8087 - val_loss: 3.4503 - val_acc: 0.4406\n",
      "Epoch 39/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3972 - acc: 0.8212 - val_loss: 3.5525 - val_acc: 0.4571\n",
      "Epoch 40/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3866 - acc: 0.8208 - val_loss: 3.6767 - val_acc: 0.4466\n",
      "Epoch 41/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3777 - acc: 0.8313 - val_loss: 3.5123 - val_acc: 0.3789\n",
      "Epoch 42/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3791 - acc: 0.8302 - val_loss: 3.5173 - val_acc: 0.4406\n",
      "Epoch 43/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3719 - acc: 0.8294 - val_loss: 3.7126 - val_acc: 0.3970\n",
      "Epoch 44/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3761 - acc: 0.8287 - val_loss: 3.6603 - val_acc: 0.3774\n",
      "Epoch 45/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3805 - acc: 0.8219 - val_loss: 3.7522 - val_acc: 0.4316\n",
      "Epoch 46/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3776 - acc: 0.8302 - val_loss: 3.5956 - val_acc: 0.3684\n",
      "Epoch 47/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3759 - acc: 0.8340 - val_loss: 3.7326 - val_acc: 0.4241\n",
      "Epoch 48/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3593 - acc: 0.8392 - val_loss: 3.4915 - val_acc: 0.3774\n",
      "Epoch 49/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3596 - acc: 0.8396 - val_loss: 3.8370 - val_acc: 0.4692\n",
      "Epoch 50/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3709 - acc: 0.8249 - val_loss: 3.7434 - val_acc: 0.4376\n",
      "Epoch 51/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3586 - acc: 0.8321 - val_loss: 4.0657 - val_acc: 0.4195\n",
      "Epoch 52/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3572 - acc: 0.8309 - val_loss: 3.7382 - val_acc: 0.4361\n",
      "Epoch 53/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3518 - acc: 0.8381 - val_loss: 4.0593 - val_acc: 0.4195\n",
      "Epoch 54/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3432 - acc: 0.8453 - val_loss: 3.8195 - val_acc: 0.4421\n",
      "Epoch 55/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3480 - acc: 0.8404 - val_loss: 4.0283 - val_acc: 0.3729\n",
      "Epoch 56/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3375 - acc: 0.8445 - val_loss: 4.2682 - val_acc: 0.4045\n",
      "Epoch 57/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3428 - acc: 0.8373 - val_loss: 4.2803 - val_acc: 0.3744\n",
      "Epoch 58/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3421 - acc: 0.8411 - val_loss: 4.2443 - val_acc: 0.3850\n",
      "Epoch 59/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3539 - acc: 0.8332 - val_loss: 4.3532 - val_acc: 0.3398\n",
      "Epoch 60/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3383 - acc: 0.8434 - val_loss: 4.2773 - val_acc: 0.3459\n",
      "Epoch 61/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3431 - acc: 0.8453 - val_loss: 4.3602 - val_acc: 0.4496\n",
      "Epoch 62/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3337 - acc: 0.8366 - val_loss: 4.2911 - val_acc: 0.3774\n",
      "Epoch 63/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3442 - acc: 0.8358 - val_loss: 4.0681 - val_acc: 0.3774\n",
      "Epoch 64/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3525 - acc: 0.8396 - val_loss: 4.2998 - val_acc: 0.3865\n",
      "Epoch 65/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3620 - acc: 0.8340 - val_loss: 4.3013 - val_acc: 0.3774\n",
      "Epoch 66/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3491 - acc: 0.8400 - val_loss: 4.1743 - val_acc: 0.3895\n",
      "Epoch 67/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3596 - acc: 0.8362 - val_loss: 4.1617 - val_acc: 0.3714\n",
      "Epoch 68/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3368 - acc: 0.8426 - val_loss: 4.2449 - val_acc: 0.3729\n",
      "Epoch 69/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3358 - acc: 0.8434 - val_loss: 4.0819 - val_acc: 0.3669\n",
      "Epoch 70/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3625 - acc: 0.8340 - val_loss: 4.1422 - val_acc: 0.3489\n",
      "Epoch 71/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3440 - acc: 0.8471 - val_loss: 4.3150 - val_acc: 0.3789\n",
      "Epoch 72/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3369 - acc: 0.8464 - val_loss: 4.4258 - val_acc: 0.3594\n",
      "Epoch 73/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3284 - acc: 0.8434 - val_loss: 4.3937 - val_acc: 0.3729\n",
      "Epoch 74/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3268 - acc: 0.8422 - val_loss: 4.2927 - val_acc: 0.3669\n",
      "Epoch 75/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3291 - acc: 0.8468 - val_loss: 4.4837 - val_acc: 0.3654\n",
      "Epoch 76/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3433 - acc: 0.8426 - val_loss: 4.2253 - val_acc: 0.3564\n",
      "Epoch 77/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3330 - acc: 0.8490 - val_loss: 4.6628 - val_acc: 0.3489\n",
      "Epoch 78/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3391 - acc: 0.8479 - val_loss: 4.5466 - val_acc: 0.3444\n",
      "Epoch 79/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3218 - acc: 0.8517 - val_loss: 4.1557 - val_acc: 0.3970\n",
      "Epoch 80/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3127 - acc: 0.8562 - val_loss: 4.5869 - val_acc: 0.3624\n",
      "Epoch 81/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3152 - acc: 0.8509 - val_loss: 4.4639 - val_acc: 0.3519\n",
      "Epoch 82/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3196 - acc: 0.8471 - val_loss: 4.6685 - val_acc: 0.3459\n",
      "Epoch 83/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3135 - acc: 0.8584 - val_loss: 4.6369 - val_acc: 0.3519\n",
      "Epoch 84/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3057 - acc: 0.8502 - val_loss: 4.6149 - val_acc: 0.4376\n",
      "Epoch 85/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3263 - acc: 0.8502 - val_loss: 4.6708 - val_acc: 0.3414\n",
      "Epoch 86/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3263 - acc: 0.8513 - val_loss: 4.6457 - val_acc: 0.3323\n",
      "Epoch 87/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3126 - acc: 0.8547 - val_loss: 4.5717 - val_acc: 0.3444\n",
      "Epoch 88/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3113 - acc: 0.8502 - val_loss: 4.6375 - val_acc: 0.3955\n",
      "Epoch 89/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3259 - acc: 0.8445 - val_loss: 4.1660 - val_acc: 0.3820\n",
      "Epoch 90/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3341 - acc: 0.8471 - val_loss: 4.3636 - val_acc: 0.3504\n",
      "Epoch 91/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3222 - acc: 0.8509 - val_loss: 4.5763 - val_acc: 0.3368\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 1s - loss: 0.3116 - acc: 0.8524 - val_loss: 4.7008 - val_acc: 0.4030\n",
      "Epoch 93/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3095 - acc: 0.8543 - val_loss: 4.9429 - val_acc: 0.3353\n",
      "Epoch 94/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.2983 - acc: 0.8528 - val_loss: 4.8547 - val_acc: 0.3308\n",
      "Epoch 95/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3052 - acc: 0.8528 - val_loss: 4.4439 - val_acc: 0.3684\n",
      "Epoch 96/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3190 - acc: 0.8490 - val_loss: 4.6787 - val_acc: 0.4045\n",
      "Epoch 97/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3165 - acc: 0.8498 - val_loss: 4.5144 - val_acc: 0.3579\n",
      "Epoch 98/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3146 - acc: 0.8494 - val_loss: 5.0220 - val_acc: 0.3383\n",
      "Epoch 99/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3061 - acc: 0.8581 - val_loss: 5.2007 - val_acc: 0.3248\n",
      "Epoch 100/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3158 - acc: 0.8569 - val_loss: 5.0305 - val_acc: 0.3323\n",
      "5376/5668 [===========================>..] - ETA: 0sTrain on 2656 samples, validate on 665 samples\n",
      "Epoch 1/100\n",
      "2656/2656 [==============================] - 5s - loss: 1.4662 - acc: 0.4895 - val_loss: 1.5308 - val_acc: 0.4511\n",
      "Epoch 2/100\n",
      "2656/2656 [==============================] - 0s - loss: 1.0470 - acc: 0.6062 - val_loss: 1.5105 - val_acc: 0.4556\n",
      "Epoch 3/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.9478 - acc: 0.6318 - val_loss: 1.4820 - val_acc: 0.4586\n",
      "Epoch 4/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.8855 - acc: 0.6536 - val_loss: 1.5755 - val_acc: 0.4526\n",
      "Epoch 5/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.8386 - acc: 0.6702 - val_loss: 1.5407 - val_acc: 0.4496\n",
      "Epoch 6/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7873 - acc: 0.6830 - val_loss: 1.6550 - val_acc: 0.4436\n",
      "Epoch 7/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7689 - acc: 0.6879 - val_loss: 1.6787 - val_acc: 0.4466\n",
      "Epoch 8/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7395 - acc: 0.7022 - val_loss: 1.6798 - val_acc: 0.4571\n",
      "Epoch 9/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.7080 - acc: 0.7135 - val_loss: 1.6663 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.6860 - acc: 0.7274 - val_loss: 1.8373 - val_acc: 0.4842\n",
      "Epoch 11/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.6679 - acc: 0.7206 - val_loss: 2.0375 - val_acc: 0.4346\n",
      "Epoch 12/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6495 - acc: 0.7297 - val_loss: 1.8963 - val_acc: 0.4647\n",
      "Epoch 13/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6215 - acc: 0.7402 - val_loss: 2.1644 - val_acc: 0.4421\n",
      "Epoch 14/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.6022 - acc: 0.7451 - val_loss: 2.2336 - val_acc: 0.3910\n",
      "Epoch 15/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5834 - acc: 0.7590 - val_loss: 2.2457 - val_acc: 0.4647\n",
      "Epoch 16/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5806 - acc: 0.7549 - val_loss: 2.1660 - val_acc: 0.4481\n",
      "Epoch 17/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5542 - acc: 0.7703 - val_loss: 2.2819 - val_acc: 0.4226\n",
      "Epoch 18/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5271 - acc: 0.7805 - val_loss: 2.4403 - val_acc: 0.3774\n",
      "Epoch 19/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5175 - acc: 0.7869 - val_loss: 2.3283 - val_acc: 0.3955\n",
      "Epoch 20/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.5150 - acc: 0.7824 - val_loss: 2.5782 - val_acc: 0.4105\n",
      "Epoch 21/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4819 - acc: 0.7963 - val_loss: 2.6034 - val_acc: 0.3865\n",
      "Epoch 22/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4897 - acc: 0.7865 - val_loss: 2.6067 - val_acc: 0.3835\n",
      "Epoch 23/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4829 - acc: 0.7971 - val_loss: 2.7313 - val_acc: 0.3895\n",
      "Epoch 24/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4749 - acc: 0.7986 - val_loss: 2.7491 - val_acc: 0.4481\n",
      "Epoch 25/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4840 - acc: 0.7899 - val_loss: 2.7245 - val_acc: 0.3789\n",
      "Epoch 26/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4352 - acc: 0.8080 - val_loss: 3.1660 - val_acc: 0.3564\n",
      "Epoch 27/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4641 - acc: 0.8001 - val_loss: 2.7861 - val_acc: 0.4662\n",
      "Epoch 28/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4311 - acc: 0.8106 - val_loss: 3.0125 - val_acc: 0.4496\n",
      "Epoch 29/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4158 - acc: 0.8133 - val_loss: 3.0667 - val_acc: 0.4421\n",
      "Epoch 30/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4117 - acc: 0.8193 - val_loss: 3.2297 - val_acc: 0.3759\n",
      "Epoch 31/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4124 - acc: 0.8148 - val_loss: 3.0271 - val_acc: 0.3789\n",
      "Epoch 32/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4046 - acc: 0.8257 - val_loss: 3.3281 - val_acc: 0.3865\n",
      "Epoch 33/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4167 - acc: 0.8087 - val_loss: 3.3176 - val_acc: 0.4391\n",
      "Epoch 34/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4162 - acc: 0.8189 - val_loss: 3.4378 - val_acc: 0.3865\n",
      "Epoch 35/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4082 - acc: 0.8133 - val_loss: 3.1258 - val_acc: 0.4541\n",
      "Epoch 36/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4072 - acc: 0.8189 - val_loss: 3.1608 - val_acc: 0.3759\n",
      "Epoch 37/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4000 - acc: 0.8151 - val_loss: 3.4173 - val_acc: 0.3699\n",
      "Epoch 38/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.4040 - acc: 0.8140 - val_loss: 3.4239 - val_acc: 0.3789\n",
      "Epoch 39/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.4104 - acc: 0.8163 - val_loss: 3.4258 - val_acc: 0.3564\n",
      "Epoch 40/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3773 - acc: 0.8302 - val_loss: 3.6410 - val_acc: 0.3684\n",
      "Epoch 41/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3889 - acc: 0.8257 - val_loss: 3.7264 - val_acc: 0.4045\n",
      "Epoch 42/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3831 - acc: 0.8208 - val_loss: 3.8182 - val_acc: 0.3669\n",
      "Epoch 43/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3912 - acc: 0.8257 - val_loss: 3.6889 - val_acc: 0.3654\n",
      "Epoch 44/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3749 - acc: 0.8242 - val_loss: 3.5679 - val_acc: 0.3489\n",
      "Epoch 45/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3633 - acc: 0.8340 - val_loss: 3.8913 - val_acc: 0.3835\n",
      "Epoch 46/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3691 - acc: 0.8317 - val_loss: 3.8038 - val_acc: 0.3579\n",
      "Epoch 47/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3908 - acc: 0.8276 - val_loss: 3.9361 - val_acc: 0.3504\n",
      "Epoch 48/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3771 - acc: 0.8268 - val_loss: 4.1748 - val_acc: 0.3218\n",
      "Epoch 49/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3682 - acc: 0.8306 - val_loss: 4.0041 - val_acc: 0.3353\n",
      "Epoch 50/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3667 - acc: 0.8306 - val_loss: 4.0879 - val_acc: 0.3459\n",
      "Epoch 51/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3552 - acc: 0.8415 - val_loss: 4.2934 - val_acc: 0.3489\n",
      "Epoch 52/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3637 - acc: 0.8332 - val_loss: 4.2869 - val_acc: 0.3414\n",
      "Epoch 53/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3480 - acc: 0.8430 - val_loss: 4.5073 - val_acc: 0.3579\n",
      "Epoch 54/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3475 - acc: 0.8404 - val_loss: 4.4868 - val_acc: 0.3383\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 1s - loss: 0.3501 - acc: 0.8422 - val_loss: 4.2103 - val_acc: 0.3504\n",
      "Epoch 56/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3398 - acc: 0.8419 - val_loss: 4.4988 - val_acc: 0.3549\n",
      "Epoch 57/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3472 - acc: 0.8381 - val_loss: 4.4164 - val_acc: 0.3910\n",
      "Epoch 58/100\n",
      "2656/2656 [==============================] - 0s - loss: 0.3468 - acc: 0.8385 - val_loss: 4.5293 - val_acc: 0.3624\n",
      "Epoch 59/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3431 - acc: 0.8392 - val_loss: 4.7596 - val_acc: 0.3459\n",
      "Epoch 60/100\n",
      "2656/2656 [==============================] - 1s - loss: 0.3368 - acc: 0.8468 - val_loss: 4.8281 - val_acc: 0.3444\n",
      "Epoch 61/100\n",
      " 650/2656 [======>.......................] - ETA: 0s - loss: 0.3199 - acc: 0.8477"
     ]
    }
   ],
   "source": [
    "#repeating\n",
    "repeats=10\n",
    "scores={}\n",
    "for i in range(repeats):\n",
    "    scores[\"rep_\"+str(i)]=fit_model_nn(X_train_0,to_categorical(y,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
