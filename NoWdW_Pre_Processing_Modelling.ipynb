{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "train = pd.read_csv(cwd + '/bases/training_variants')\n",
    "test = pd.read_csv(cwd + '/bases/test_variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts = pd.read_csv(cwd + '/bases/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")\n",
    "test_texts = pd.read_csv(cwd + '/bases/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_texts, how='left', on='ID')\n",
    "test = pd.merge(test, test_texts, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### process the train and test set together\n",
    "data_all = pd.concat((train, test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set('!\"#$%&\\'()*+:;<=>?@[\\\\]^_`{|}~0123456789') \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc,lemmatiz=False):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free_0 = [re.sub(\",|\\.|/\",\" \",ch) for ch in stop_free]\n",
    "    if lemmatiz:\n",
    "        punc_free_lem=\"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free_lem.split())\n",
    "        return normalized\n",
    "    else:\n",
    "        punc_free = \"\".join(ch for ch in punc_free_0 if ch not in exclude)\n",
    "        return punc_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No lemmatization for the moment, be careful not to lemmatize then w2vec\n",
    "data_all.Text = [clean(doc) for doc in data_all.Text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data_all.iloc[:len(train)]\n",
    "test = data_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=train[\"Class\"]-1\n",
    "X_train=train.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "X_test=test.drop([\"Class\",\"ID\",\"Gene\",\"Variation\"],axis=1)\n",
    "txt_no_dup=train[\"Text\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=10, max_features=15000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(txt_no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_comp=[10,20,30,40,50,60,80,100,120,150,170,200]\n",
    "dic_svd={}\n",
    "for comp in list_comp:\n",
    "    dic_svd[comp]=TruncatedSVD(n_components=comp,n_iter=10,random_state=26)\n",
    "tsvd_train,tsvd_test = {},{}\n",
    "for sv in dic_svd:\n",
    "    tsvd_train[sv]=dic_svd[sv].fit_transform(X_train_text)\n",
    "    tsvd_test[sv]=dic_svd[sv].transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in dic_svd:\n",
    "    for i in range(n):\n",
    "        X_train['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_train[n][:, i]\n",
    "        X_test['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_test[sv][:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_0=np.array(X_train.drop(\"Text\",axis=1))\n",
    "X_test_0=np.array(X_test.drop(\"Text\",axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ctg=to_categorical(y,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3321/3321 [==============================] - 1s - loss: 1.4365 - acc: 0.4818     \n",
      "Epoch 2/40\n",
      "3321/3321 [==============================] - 1s - loss: 1.0489 - acc: 0.6092     \n",
      "Epoch 3/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.9589 - acc: 0.6366     \n",
      "Epoch 4/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.8983 - acc: 0.6462     \n",
      "Epoch 5/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.8520 - acc: 0.6718     \n",
      "Epoch 6/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.8116 - acc: 0.6871     \n",
      "Epoch 7/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.7804 - acc: 0.6865     \n",
      "Epoch 8/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.7544 - acc: 0.7055     \n",
      "Epoch 9/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.7170 - acc: 0.7182     \n",
      "Epoch 10/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.7028 - acc: 0.7185     \n",
      "Epoch 11/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.7033 - acc: 0.7061     \n",
      "Epoch 12/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.6596 - acc: 0.7275     \n",
      "Epoch 13/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.6255 - acc: 0.7456     \n",
      "Epoch 14/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.6056 - acc: 0.7474     - ETA: 1s - loss: 0\n",
      "Epoch 15/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5743 - acc: 0.7570     \n",
      "Epoch 16/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5497 - acc: 0.7718     \n",
      "Epoch 17/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5505 - acc: 0.7633     \n",
      "Epoch 18/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5250 - acc: 0.7850     \n",
      "Epoch 19/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5115 - acc: 0.7862     \n",
      "Epoch 20/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.5071 - acc: 0.7823     \n",
      "Epoch 21/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4968 - acc: 0.7865     \n",
      "Epoch 22/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4658 - acc: 0.7958     \n",
      "Epoch 23/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4731 - acc: 0.8007     \n",
      "Epoch 24/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4500 - acc: 0.8043     \n",
      "Epoch 25/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4420 - acc: 0.8043     \n",
      "Epoch 26/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4206 - acc: 0.8136     \n",
      "Epoch 27/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4209 - acc: 0.8178     \n",
      "Epoch 28/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4133 - acc: 0.8220     \n",
      "Epoch 29/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4063 - acc: 0.8190     \n",
      "Epoch 30/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4043 - acc: 0.8202     \n",
      "Epoch 31/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.4003 - acc: 0.8238     \n",
      "Epoch 32/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3971 - acc: 0.8275     \n",
      "Epoch 33/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3957 - acc: 0.8254     \n",
      "Epoch 34/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3800 - acc: 0.8332     \n",
      "Epoch 35/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3873 - acc: 0.8263     \n",
      "Epoch 36/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3945 - acc: 0.8269     \n",
      "Epoch 37/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3771 - acc: 0.8232     \n",
      "Epoch 38/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3736 - acc: 0.8320     \n",
      "Epoch 39/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3721 - acc: 0.8350     \n",
      "Epoch 40/40\n",
      "3321/3321 [==============================] - 1s - loss: 0.3461 - acc: 0.8452     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18ad18e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(300,input_dim=1030,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300,activation=\"relu\"))\n",
    "model.add(Dense(9,activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_0,y_ctg,epochs=40,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_classifier={\"XGB_medium\":XGBClassifier(n_estimators=200,max_depth=5, objective=\"multi:softprob\",subsample=0.7,seed=26),\n",
    "    \"XGB_small\":XGBClassifier(max_depth=2,objective=\"multi:softprob\",subsample=0.5,seed=26),\n",
    "                   \"XGB_tall\":XGBClassifier(n_estimators=300,max_depth=7,subsample=0.9,objective=\"multi:softprob\",seed=26),\n",
    "               \"Neural Net\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gen_nowdw(X,X_test,y,classifier,file,neural_net=False):\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=26, shuffle=True)\n",
    "    if neural_net==False:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            fold += 1\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            p_test = clf.predict_proba(X_test)\n",
    "            y_test += p_test/5\n",
    "    else:\n",
    "        print(\"One Fold predict for NN\")\n",
    "        clf=classifier\n",
    "        y_test=clf.predict(X_test)\n",
    "    classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"scores/stack_test/nowdw_{}.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train\") #peut etre que to array est exclusivement pour les xgb\n",
    "    \n",
    "    if os.path.isfile(\"scores/stack_train/nowdw_{}.csv\".format(file)):\n",
    "        print(\"not necessary, already done\")\n",
    "    else:\n",
    "        y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict_proba\") \n",
    "        subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "        subm1['ID'] = ID_train\n",
    "        subm1.to_csv(\"scores/stack_train/nowdw_{}.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (2653, 1030) (668, 1030)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-490506f73cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic_classifier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_gen_nowdw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_classifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-da482413880f>\u001b[0m in \u001b[0;36mmodel_gen_nowdw\u001b[0;34m(X, X_test, y, classifier, file, neural_net)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0my_test\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/LF/anaconda/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/LF/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/LF/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/LF/anaconda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for clf in dic_classifier:\n",
    "    model_gen_nowdw(X_train_0,X_test_0,y,dic_classifier[clf],file=clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
