{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"bases/working_train.csv\",encoding=\"utf8\")\n",
    "test=pd.read_csv(\"bases/working_test.csv\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LEAK PART FOR EVALUATING FEATURES :\n",
    "###### OR USE THIS IF U WANT TO USE THE LEAKS!!!!! (not necessary!) ##############################s\n",
    "pickle_in = open(\"Leaks/Leaks_all\", \"rb\") #### OR use Leaks for the full leaks (no sub)\n",
    "Leaks = pickle.load(pickle_in)\n",
    "\n",
    "test_leak = Leaks\n",
    "test_leak.index = test_leak.ID\n",
    "Class_leaks = test_leak.Class\n",
    "test =test[test.ID.isin(test_leak.index)] # drop Class, to be used for validation later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_features(X_train,y_train,X_test,clf=None):\n",
    "    if clf is None:\n",
    "        clf = LogisticRegression()\n",
    "    clf.fit(X_train,y_train)\n",
    "    probas=clf.predict_proba(X_test)\n",
    "    y = Class_leaks - 1\n",
    "    print('Log loss: {}'.format(log_loss(y, probas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=np.array(train[\"Class\"])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.6028411177680808\n"
     ]
    }
   ],
   "source": [
    "#this function can be used to check if one variable could be useful. \n",
    "evaluate_features(X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will add features from word2vec retrained then get the mean for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's important to remove duplicated spaces for word2vec learning !\n",
    "train[\"Text\"]=[\" \".join(doc.split()) for doc in train[\"Text\"].values]\n",
    "test[\"Text\"]=[\" \".join(doc.split()) for doc in test[\"Text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localisation not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        train[\"Text\"].values,test[\"Text\"].values),\"localisation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded_train = mean_embedding_vectorizer.fit_transform(train['Text'])\n",
    "mean_embedded_test = mean_embedding_vectorizer.fit_transform(test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_tr=pd.DataFrame(mean_embedded_train)\n",
    "df_embed_te=pd.DataFrame(mean_embedded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding_tr=df_embed_tr.reset_index()\n",
    "df_embedding_te=df_embed_te.reset_index()\n",
    "df_embedding_tr=df_embedding_tr.rename(columns={\"index\":\"ID\"})\n",
    "df_embedding_te=df_embedding_te.rename(columns={\"index\":\"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v=pd.merge(train,df_embedding_tr,on=\"ID\")\n",
    "test_w2v=pd.merge(test,df_embedding_te,on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v=test_w2v.drop(\"Class\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_w2v.to_csv(\"bases/working_train_w2v.csv\",index=False,encoding=\"utf8\")\n",
    "test_w2v.to_csv(\"bases/working_test_w2v.csv\",index=False,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THEN FASTTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
