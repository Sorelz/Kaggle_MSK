{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('..//bases/training_variants')\n",
    "test = pd.read_csv('..//bases/test_variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = pd.read_csv('..//bases/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")\n",
    "test_texts = pd.read_csv('..//bases/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test=pd.read_csv('..//bases/new_test_variants.csv')\n",
    "new_test_texts = pd.read_csv('..//bases/new_test_text.csv', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_texts, how='left', on='ID')\n",
    "test = pd.merge(test, test_texts, how='left', on='ID')\n",
    "new_test_final=pd.merge(new_test,new_test_texts,how=\"left\",on=\"ID\")\n",
    "leaks=pd.read_csv('..//bases/s1_add_train.csv')\n",
    "leaks_1=pd.DataFrame([leaks[\"ID\"],leaks.drop(\"ID\",axis=1).idxmax(axis=1).map(lambda x: x.lstrip('class'))])\n",
    "leaks_2=leaks_1.T\n",
    "leaks_2.columns=[\"ID\",\"Class\"]\n",
    "leaks_3=pd.merge(leaks_2,test[test.ID.isin(leaks_2.ID)])\n",
    "leaks_final=pd.merge(leaks_3,test_texts[test_texts.ID.isin(leaks_3.ID)])\n",
    "train_final=pd.concat([train,leaks_final]) #adding first stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train,test,leaks,leaks_1,leaks_2,leaks_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Gene Letter to their abbreviation in order to find them in the text\n",
    "One_to_Three_AA = {'C': 'Cys', 'D': 'Asp', 'S': 'Ser', 'Q': 'Gln', 'K': 'Lys',\n",
    "         'I': 'Ile', 'P': 'Pro', 'T': 'Thr', 'F': 'Phe', 'N': 'Asn', \n",
    "         'G': 'Gly', 'H': 'His', 'L': 'Leu', 'R': 'Arg', 'W': 'Trp', \n",
    "         'A': 'Ala', 'V': 'Val', 'E': 'Glu', 'Y': 'Tyr', 'M': 'Met'}\n",
    "pattern = re.compile('|'.join(One_to_Three_AA.keys()))\n",
    "##### Get variation types by using regex\n",
    "def variation_regex(data, pattern): # if you want to not ignore cases, add extra argument to function\n",
    "    Boolean = [not bool(re.search(pattern, i, re.IGNORECASE)) for i in data.Variation]\n",
    "    data_no_regex = data[Boolean]  # 182 Fusions => 495 over \n",
    "    not_Boolean = [not i for i in Boolean]  \n",
    "    data_regex = data[not_Boolean]\n",
    "    \n",
    "    return (data_regex, data_no_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### process the train and test set together\n",
    "data_all = pd.concat((train_final, new_test_final), axis=0, ignore_index=True)\n",
    "data_all_backup = data_all[:] ##### We keep backup because we want dummy variables of Gene & Text \n",
    "# TODO maybe also use Variation function of Gene from a database, and other suggestions. Also can use Count_sub as feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4675, 5)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub(data):\n",
    "\n",
    "    ##### The normal case is around 2080 out of the 2644\n",
    "    \n",
    "    \n",
    "    Boolean = [data.Variation[i] in data.Text[i] or #normal case\n",
    "               data.Variation[i][:-1] in data.Text[i] or #case 1.\n",
    "               pattern.sub(lambda x: One_to_Three_AA[x.group()], data.Variation[i][:-1]) # case2\n",
    "               in data.Text[i]  for i in data.index] ## because new indexing we use \n",
    "    \n",
    "    #TODO could also match insensitive as a next step for more info.\n",
    "    #Shorter Boolean below = the normal version\n",
    "    \n",
    "    #Boolean = [trainSub.Variation[i] in trainSub.Text[i] #normal case\n",
    "    #           for i in trainSub.ID] ## because new indexing we use ID\n",
    "    #           \n",
    "            \n",
    "    sub_in_text = data[Boolean]\n",
    "    not_Boolean = [not i for i in Boolean]  \n",
    "\n",
    "    sub_not_in_text = data[not_Boolean]\n",
    "#    sub_in_text['Count'] = [sub_in_text.Text[i].count(sub_in_text.Variation[i][:-1])\n",
    "#                    +sub_in_text.Text[i].count(pattern.sub(lambda x: One_to_Three_AA[x.group()], sub_in_text.Variation[i][:-1]))\n",
    "#                    for i in sub_in_text.index]\n",
    "    \n",
    "    return sub_in_text, sub_not_in_text\n",
    "##### For subs that are not find in text: use regex to account for a different number\n",
    "##### TODO: things you can further try - with AA name replacement, searching for the number only etc.\n",
    "def find_sub_noText(data):\n",
    "    Booleans = []\n",
    "    for i in data.index:\n",
    "        split_variation = re.split('(\\d+)', data.Variation[i]) # split based on a number\n",
    "        first_Amino = re.escape(split_variation[0]) #re.escpae uses variable as regex\n",
    "        last_Amino = re.escape(split_variation[-1])\n",
    "        #first_number = re.escape(split_variation[1][0])\n",
    "        #new_regex = r\"[^a-zA-Z0-9]\" + first_Amino + first_number\n",
    "        new_regex  = first_Amino + r\"\\d+\" + last_Amino\n",
    "        Boolean = bool(re.search(new_regex, data.Text[i]))\n",
    "        Booleans.append(Boolean)\n",
    "    \n",
    "    sub_number_in_text = data[Booleans]\n",
    "    not_Boolean = [not i for i in Booleans]  \n",
    "\n",
    "    sub_again_no_text = data[not_Boolean]\n",
    "    return sub_again_no_text, sub_number_in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Converts list of sentences into one string of sentences for each document => to use for tfidf etc.\n",
    "def sentences_to_string(sentences_list):\n",
    "    sentence_strings = []\n",
    "    for sentences in sentences_list:\n",
    "        sentence_string =  ' '.join(str(sentence) for sentence in sentences)\n",
    "        sentence_strings.append(sentence_string)\n",
    "    \n",
    "    return sentence_strings ### This doesn't take such a long time to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitutions (subs) processing of data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### First find those that have the format of being a substitution in data\n",
    "data_all['Substitutions_var'] = data_all.Variation.apply(lambda x: bool(re.search('^[A-Z]\\\\d+[A-Z*]$', x))*1) #multiplying by 1 converts True to 1, False to 0 => Maybe modify this later?\n",
    "data_all['Stop_codon_var'] = data_all.Variation.apply(lambda x: bool(re.search('[*]', x))*1) #multiplying by 1 converts True to 1, False to 0\n",
    "data_sub = data_all[data_all['Substitutions_var']==1] ### Now we know the index of where a substitution occurs - the data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_in_text, sub_not_in_text = find_sub(data_sub)\n",
    "sub_in_text_backup = sub_in_text[:] ## index gets changed by text_processing if we don't make a copy\n",
    "##### INVESTIGATION: Why do some subs don't appear in Text?: Try to automize this and find out\n",
    "### Substitutions can appear as SAME_PREFIX - Other number - SAME_SUFFIX\n",
    "\n",
    "sub_again_no_Text, sub_noText = find_sub_noText(sub_not_in_text) # 108 such cases out of 411 = nice improvement\n",
    "sub_noText_backup = sub_noText[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on variations who have the 2 letters right but not same numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "############## Non-subs preprocessing of data set #######################\n",
    "\n",
    "#def find_mutation_type(row, pattern):  ##### TODO: make clearer by using a function instead of lambda\n",
    "#    return bool(re.search('^fusion', row, re.IGNORECASE)) *1. Also for subs\n",
    "\n",
    "####### Fusions : 'Fusions' ############\n",
    "data_all['Fusion_var'] = data_all.Variation.apply(lambda x: bool(re.search('^fusion', x, re.IGNORECASE))*1) #multiplying by 1 converts True to 1, False to 0\n",
    "new_fusion, new_data_all = variation_regex(data_all, '^fusion') \n",
    "\n",
    "###### Fusions: 'Gene-Gene fusion' ########\n",
    "data_all['gene_fusion_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('fusion', x, re.IGNORECASE))*1) \n",
    "_ , new_data_all = variation_regex(new_data_all, 'fusion') \n",
    "###### Notice that NaN introduced for places where splicing occured => replace after NaN with 0's when complete\n",
    "\n",
    "####### Deletions: 'Deletions' ############\n",
    "data_all['Deletion_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('^del', x, re.IGNORECASE))*1) \n",
    "new_del, new_data_all = variation_regex(new_data_all, '^del') \n",
    "\n",
    "####### Deletions & Insertions wheteher together or seperately (doesn't make a big difference IMO)\n",
    "data_all['del_or_ins_var'] = new_data_all.Variation.apply(lambda x: bool(re.search('del|ins', x, re.IGNORECASE))*1) \n",
    "# we could also later divide it into del, ins if we want to\n",
    "\n",
    "###### Amplifications #########\n",
    "data_all['Amplification_var'] = data_all.Variation.apply(lambda x: bool(re.search('ampl', x, re.IGNORECASE))*1) \n",
    "\n",
    "###### Truncations ########### Don't forget there are 'Truncating mutations' = 95 and '_trunc' = 4\n",
    "data_all['Truncation_var'] = data_all.Variation.apply(lambda x: bool(re.search('trunc', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Exons #########\n",
    "data_all['exon_var'] = data_all.Variation.apply(lambda x: bool(re.search('exon', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Frameshift mutations ########\n",
    "data_all['frameshift_var'] = data_all.Variation.apply(lambda x: bool(re.search('fs', x, re.IGNORECASE))*1) \n",
    "\n",
    "####### Duplications ##############\n",
    "data_all['dup_var'] = data_all.Variation.apply(lambda x: bool(re.search('dup', x, re.IGNORECASE))*1) \n",
    "\n",
    "data_all.fillna(0, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : #Sentence Tokenizer for non-subs that are not in text, AND\n",
    "#subs that are not in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used\n",
    "################ The dummy variables for Gene and Text ##################\n",
    "## TODO: also use dummy for Text? There are 135 shared Genes and 142 shared Text between train and Leaks!  len(set(train.Text) & set(Leaks.Text))\n",
    "#data_all_dummy = data_all_backup[['Gene', 'Text']] # drop those columns we don't need as dummy.\n",
    "#X_dummy = pd.get_dummies(data_all_dummy) # converts categorical variables into dummy variable. From len set => 269 genes + 2090 texts\n",
    "#X_dummy_train = X_dummy[:train.shape[0]]\n",
    "#X_dummy_test = X_dummy[train.shape[0]:]\n",
    "#dummy_names = X_dummy.columns.values #### To remember names if you want to check again what Gene or Text used\n",
    "#X_dummy = X_dummy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use the variation types \n",
    "#variation_types = data_all.drop(['ID', 'Gene', 'Class', 'Text', 'Variation'], axis =1)\n",
    "#X_variation_train = variation_types[:train.shape[0]]\n",
    "#X_variation_test = variation_types[train.shape[0]:]\n",
    "#variation_names = variation_types.columns.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some more features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature for the length of the text\n",
    "data_all[\"Text_words\"] = data_all[\"Text\"].map(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = data_all.iloc[:len(train_final)]\n",
    "new_test = data_all.iloc[len(train_final):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dummy_columns( d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "def fix_test_columns( d, columns ):  \n",
    "\n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    extra_cols = set( d.columns ) - set( columns )\n",
    "    if extra_cols:\n",
    "        print (\"extra columns:\", extra_cols)\n",
    "\n",
    "    d = d[ columns ]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra columns: {'RAD54B', 'SUCLA2', 'HABP2', 'ROCK1', 'KCNJ13', 'SLC25A15', 'PRKRA', 'NEK8', 'AGXT', 'XRCC1', 'NDUFS3', 'CHST3', 'PTCH2', 'SLC17A5', 'RGS9', 'FLNB', 'IKBKAP', 'PLA2G6', 'RNF6', 'BFSP2', 'SIX3', 'CST3', 'ALOX12B', 'DNM1L', 'GYG1', 'STK19', 'STK33', 'DCC', 'APOL1', 'RPS19', 'AKAP9', 'ADAMTS13', 'SLC25A13', 'MOCS2', 'STAT5B', 'SCN9A', 'EPHB2', 'KCNE2', 'TGM5', 'RP1', 'GNE', 'RAB27A', 'PDE8B', 'DYNC2H1', 'MPDU1', 'SLC19A2', 'WISP3', 'KISS1R', 'LARGE1', 'SLC22A4', 'MYOT', 'SCO2', 'LITAF', 'ASS1', 'MCC', 'SPAST', 'AURKC', 'SLC7A9', 'GPHN', 'OTOF', 'YARS', 'B4GALT7', 'KLF11', 'LRP5', 'TRPC6', 'CYP7B1', 'LCT', 'DNAH5', 'BBS5', 'BCS1L', 'TRPM1', 'RPS26', 'SCO1', 'CRB1', 'AP3B1', 'KCNQ4', 'SEPT9', 'CLDN16', 'MOCS1', 'TINF2', 'GCM2', 'CSF1R', 'SLC4A4', 'TSHR', 'EIF2B5', 'GJB3', 'CRNKL1', 'PHOX2B', 'BMPR1B', 'SLC22A5', 'CILP', 'ABCC6', 'KERA', 'LRP4', 'KRIT1', 'TP63', 'COX15', 'ALG10', 'SLC27A4', 'ATP2C1', 'PNPO', 'SLC6A5', 'TTK', 'LRP6', 'SLC33A1', 'GLE1', 'RECQL4', 'SLC25A12', 'SYT6', 'ABCB11', 'SF3B2', 'GRM6', 'EPHA2', 'ZFPM2', 'POLH', 'DNAI1', 'SLC7A7', 'RBBP8', 'ITM2B', 'ADGRG1', 'BAG3', 'EPHA5', 'CRLF1', 'TNFRSF11A', 'NDUFS6', 'KCNMB1', 'SCN4A', 'WNT4', 'GALK1', 'PER2', 'DPM1', 'EDAR'}\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=25, n_iter=12, random_state=26)\n",
    "\n",
    "one_hot_gene = pd.get_dummies(new_train['Gene'])\n",
    "one_hot_gene_test=pd.get_dummies(new_test[\"Gene\"])\n",
    "one_hot_gene_test=fix_test_columns(one_hot_gene_test,one_hot_gene.columns)\n",
    "truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)\n",
    "truncated_one_hot_gene_for_test=svd.transform(one_hot_gene_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_train=pd.DataFrame(truncated_one_hot_gene,columns=[\"tsvd_gene\"+ str(x) for x in range(0,25)])\n",
    "genes_test=pd.DataFrame(truncated_one_hot_gene_for_test,columns=[\"tsvd_gene\"+ str(x) for x in range(0,25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test[\"index\"]=range(0,len(new_test))\n",
    "new_train_1=pd.merge(new_train.reset_index(),genes_train.reset_index()).drop(\"index\",axis=1)\n",
    "new_test_1=pd.merge(new_test,genes_test.reset_index()).drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1.to_csv(\"checkpoints_databases/nw_working_train_0.csv\",index=False,encoding=\"utf8\")\n",
    "new_test_1.to_csv(\"checkpoints_databases/nw_working_test_0.csv\",index=False,encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
