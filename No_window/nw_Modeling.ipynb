{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import os \n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use cross_val_score on XGBoost to select 100,200 or 300 for each preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_w2v = {} \n",
    "work_test_w2v = {}\n",
    "pre_process=[\"w2v_100.csv\",\"w2v_200.csv\",\"w2v_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_w2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_train_\"+f)\n",
    "    work_test_w2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_d2v = {} \n",
    "work_test_d2v = {}\n",
    "pre_process=[\"d2v_100.csv\",\"d2v_200.csv\",\"d2v_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_d2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_train_\"+f)\n",
    "    work_test_d2v[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train_tfidf = {} \n",
    "work_test_tfidf = {}\n",
    "pre_process=[\"tfidf_tsvd_100.csv\",\"tfidf_tsvd_200.csv\",\"tfidf_tsvd_300.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train_tfidf[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_train_\"+f)\n",
    "    work_test_tfidf[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_v=pd.read_csv(\"../bases/new_training_variants.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(new_train_v.iloc[:,0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../bases/new_training_variants.csv').reset_index()\n",
    "train.columns=[[\"Tempo_ID\",\"Class\"]]\n",
    "test = pd.read_csv('../bases/new_test_variants.csv')\n",
    "ID_train=train.Tempo_ID\n",
    "ID_test=test.ID\n",
    "del train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5, random_state=26, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanw2v_100 -0.895570431786 std:w2v_100 0.0548596096564\n",
      "meanw2v_200 -0.90597903088 std:w2v_200 0.0509971236521\n",
      "meanw2v_300 -0.913247864735 std:w2v_300 0.040367728267\n"
     ]
    }
   ],
   "source": [
    "clf_xgb=XGBClassifier(max_depth=5, objective=\"multi:softprob\",seed=26)\n",
    "for name in work_train_w2v:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_w2v[name].drop(\"ID\",axis=1)),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meand2v_100 -1.02297684649 std:d2v_100 0.0567970281113\n",
      "meand2v_200 -1.02919691932 std:d2v_200 0.050459167505\n",
      "meand2v_300 -1.03101971684 std:d2v_300 0.0491266926944\n"
     ]
    }
   ],
   "source": [
    "for name in work_train_d2v:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_d2v[name].drop(\"ID\",axis=1)),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meantfidf_tsvd_100 -0.889870181278 std:tfidf_tsvd_100 0.0535310479521\n",
      "meantfidf_tsvd_200 -0.903077065542 std:tfidf_tsvd_200 0.045128434583\n",
      "meantfidf_tsvd_300 -0.905181327301 std:tfidf_tsvd_300 0.0512618176122\n"
     ]
    }
   ],
   "source": [
    "for name in work_train_tfidf:\n",
    "    h=cross_val_score(clf_xgb,np.array(work_train_tfidf[name].drop(\"ID\",axis=1)),y,cv=kf,n_jobs=-1,scoring=\"neg_log_loss\")\n",
    "    print(\"mean\"+name+\" \"+str(h.mean()),\n",
    "         \"std:\"+name+\" \"+str(h.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Substitutions_var</th>\n",
       "      <th>Stop_codon_var</th>\n",
       "      <th>Fusion_var</th>\n",
       "      <th>gene_fusion_var</th>\n",
       "      <th>Deletion_var</th>\n",
       "      <th>del_or_ins_var</th>\n",
       "      <th>Amplification_var</th>\n",
       "      <th>Truncation_var</th>\n",
       "      <th>exon_var</th>\n",
       "      <th>...</th>\n",
       "      <th>tsvd_100_90</th>\n",
       "      <th>tsvd_100_91</th>\n",
       "      <th>tsvd_100_92</th>\n",
       "      <th>tsvd_100_93</th>\n",
       "      <th>tsvd_100_94</th>\n",
       "      <th>tsvd_100_95</th>\n",
       "      <th>tsvd_100_96</th>\n",
       "      <th>tsvd_100_97</th>\n",
       "      <th>tsvd_100_98</th>\n",
       "      <th>tsvd_100_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>-0.064537</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>-0.051711</td>\n",
       "      <td>-0.008686</td>\n",
       "      <td>0.063702</td>\n",
       "      <td>-0.033369</td>\n",
       "      <td>0.047507</td>\n",
       "      <td>-0.011223</td>\n",
       "      <td>-0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009623</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>-0.024687</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>-0.004227</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>-0.019795</td>\n",
       "      <td>-0.023560</td>\n",
       "      <td>0.012469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015950</td>\n",
       "      <td>-0.011164</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.025594</td>\n",
       "      <td>-0.012595</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.041702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028990</td>\n",
       "      <td>-0.017455</td>\n",
       "      <td>-0.015833</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>-0.043635</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>-0.036137</td>\n",
       "      <td>-0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028298</td>\n",
       "      <td>-0.021922</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>-0.012296</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>-0.014589</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.012608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.017127</td>\n",
       "      <td>-0.010975</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>-0.018487</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.025474</td>\n",
       "      <td>0.018020</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>-0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024859</td>\n",
       "      <td>-0.023065</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>0.014682</td>\n",
       "      <td>0.035405</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>-0.002926</td>\n",
       "      <td>-0.015661</td>\n",
       "      <td>-0.015077</td>\n",
       "      <td>-0.038714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020009</td>\n",
       "      <td>0.029894</td>\n",
       "      <td>-0.044784</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>0.041337</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>-0.007241</td>\n",
       "      <td>0.024137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020198</td>\n",
       "      <td>-0.006154</td>\n",
       "      <td>0.036330</td>\n",
       "      <td>-0.057816</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>-0.056993</td>\n",
       "      <td>-0.018344</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>-0.027651</td>\n",
       "      <td>0.022929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010341</td>\n",
       "      <td>-0.012949</td>\n",
       "      <td>0.026076</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.019916</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>-0.024145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014051</td>\n",
       "      <td>-0.005544</td>\n",
       "      <td>-0.003668</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>-0.038488</td>\n",
       "      <td>-0.006138</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>-0.011831</td>\n",
       "      <td>-0.071792</td>\n",
       "      <td>-0.045738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034008</td>\n",
       "      <td>-0.008175</td>\n",
       "      <td>-0.030609</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.044187</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>-0.012347</td>\n",
       "      <td>-0.035557</td>\n",
       "      <td>-0.030858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>-0.057539</td>\n",
       "      <td>0.035946</td>\n",
       "      <td>-0.009523</td>\n",
       "      <td>0.059923</td>\n",
       "      <td>-0.057699</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>-0.015719</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>0.009297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.053938</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>-0.013379</td>\n",
       "      <td>-0.027928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>0.015428</td>\n",
       "      <td>-0.007819</td>\n",
       "      <td>0.015587</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>-0.031232</td>\n",
       "      <td>0.008454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022531</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>-0.007217</td>\n",
       "      <td>-0.011820</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>0.003631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020397</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.007373</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.023042</td>\n",
       "      <td>-0.003768</td>\n",
       "      <td>0.025693</td>\n",
       "      <td>-0.031111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029122</td>\n",
       "      <td>-0.008562</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.027642</td>\n",
       "      <td>-0.063516</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>-0.021103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017137</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>-0.030830</td>\n",
       "      <td>0.098508</td>\n",
       "      <td>0.020280</td>\n",
       "      <td>-0.002138</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>0.058039</td>\n",
       "      <td>-0.051571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>-0.021840</td>\n",
       "      <td>0.011064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>-0.029220</td>\n",
       "      <td>-0.017481</td>\n",
       "      <td>-0.006440</td>\n",
       "      <td>-0.002205</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>-0.032245</td>\n",
       "      <td>-0.024125</td>\n",
       "      <td>0.027851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.026147</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>0.051707</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.011751</td>\n",
       "      <td>-0.004379</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.022172</td>\n",
       "      <td>-0.001265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033148</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>-0.013770</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>-0.006718</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>-0.008119</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>0.030334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>-0.030329</td>\n",
       "      <td>-0.005698</td>\n",
       "      <td>0.042732</td>\n",
       "      <td>0.018296</td>\n",
       "      <td>-0.019918</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>0.020951</td>\n",
       "      <td>0.010637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>-0.026544</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>0.010058</td>\n",
       "      <td>-0.011230</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-0.001941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039869</td>\n",
       "      <td>-0.006410</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>-0.067035</td>\n",
       "      <td>-0.036653</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>-0.022597</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>-0.012888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011529</td>\n",
       "      <td>0.030505</td>\n",
       "      <td>-0.005657</td>\n",
       "      <td>-0.032345</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>-0.054474</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>-0.017426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014243</td>\n",
       "      <td>-0.039619</td>\n",
       "      <td>-0.012452</td>\n",
       "      <td>-0.045782</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>-0.018471</td>\n",
       "      <td>-0.009306</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>-0.011061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018088</td>\n",
       "      <td>-0.036646</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>0.027191</td>\n",
       "      <td>0.042585</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>-0.019602</td>\n",
       "      <td>-0.019053</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>-0.060715</td>\n",
       "      <td>-0.007001</td>\n",
       "      <td>-0.041101</td>\n",
       "      <td>-0.024852</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>-0.007110</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>-0.024656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>957</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.005420</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>-0.007658</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>-0.018029</td>\n",
       "      <td>-0.011289</td>\n",
       "      <td>-0.008423</td>\n",
       "      <td>-0.008038</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038488</td>\n",
       "      <td>-0.063254</td>\n",
       "      <td>-0.005914</td>\n",
       "      <td>0.056374</td>\n",
       "      <td>-0.022311</td>\n",
       "      <td>-0.027941</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>-0.007126</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>-0.030255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>959</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051600</td>\n",
       "      <td>0.030485</td>\n",
       "      <td>-0.010512</td>\n",
       "      <td>-0.081855</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.036209</td>\n",
       "      <td>0.063302</td>\n",
       "      <td>-0.037599</td>\n",
       "      <td>-0.017483</td>\n",
       "      <td>-0.007580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>960</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031629</td>\n",
       "      <td>-0.008132</td>\n",
       "      <td>-0.024797</td>\n",
       "      <td>-0.012760</td>\n",
       "      <td>0.040386</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.021343</td>\n",
       "      <td>-0.013362</td>\n",
       "      <td>-0.012785</td>\n",
       "      <td>-0.020401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>961</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.056880</td>\n",
       "      <td>-0.004535</td>\n",
       "      <td>-0.007152</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.044572</td>\n",
       "      <td>-0.026911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017904</td>\n",
       "      <td>-0.014393</td>\n",
       "      <td>0.025920</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>-0.102925</td>\n",
       "      <td>-0.089625</td>\n",
       "      <td>0.034681</td>\n",
       "      <td>-0.022845</td>\n",
       "      <td>0.042648</td>\n",
       "      <td>0.062812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>963</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>-0.006161</td>\n",
       "      <td>-0.025204</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>-0.004567</td>\n",
       "      <td>-0.020932</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.014628</td>\n",
       "      <td>0.003386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.016413</td>\n",
       "      <td>-0.002413</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.034937</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>-0.033486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>-0.010687</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.060032</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>0.004998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.034458</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>-0.013426</td>\n",
       "      <td>0.018680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.097810</td>\n",
       "      <td>-0.079139</td>\n",
       "      <td>-0.010697</td>\n",
       "      <td>0.010222</td>\n",
       "      <td>0.028738</td>\n",
       "      <td>-0.012810</td>\n",
       "      <td>0.101421</td>\n",
       "      <td>0.124255</td>\n",
       "      <td>-0.010983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>968</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049937</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.054385</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>-0.012460</td>\n",
       "      <td>-0.006200</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.013093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>969</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006430</td>\n",
       "      <td>0.018362</td>\n",
       "      <td>-0.009630</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>0.031948</td>\n",
       "      <td>-0.008503</td>\n",
       "      <td>-0.024613</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.014422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>970</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030979</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>-0.008560</td>\n",
       "      <td>0.036986</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>-0.022348</td>\n",
       "      <td>-0.011978</td>\n",
       "      <td>0.029581</td>\n",
       "      <td>0.035954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>971</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>-0.015223</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>-0.003645</td>\n",
       "      <td>0.027136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.023908</td>\n",
       "      <td>0.023082</td>\n",
       "      <td>0.010657</td>\n",
       "      <td>0.018187</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>-0.014933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>973</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003913</td>\n",
       "      <td>-0.006735</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-0.013777</td>\n",
       "      <td>-0.027479</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.013028</td>\n",
       "      <td>-0.021680</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.009736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>974</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011129</td>\n",
       "      <td>0.032191</td>\n",
       "      <td>-0.012299</td>\n",
       "      <td>0.051360</td>\n",
       "      <td>-0.010182</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.037617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>975</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>-0.003207</td>\n",
       "      <td>-0.024585</td>\n",
       "      <td>-0.007336</td>\n",
       "      <td>-0.017340</td>\n",
       "      <td>-0.002462</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.010592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>976</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>-0.010052</td>\n",
       "      <td>-0.016493</td>\n",
       "      <td>-0.015317</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>-0.023624</td>\n",
       "      <td>0.047490</td>\n",
       "      <td>0.020412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025212</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>-0.015792</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>-0.011176</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.035254</td>\n",
       "      <td>-0.023301</td>\n",
       "      <td>0.036834</td>\n",
       "      <td>0.037147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019619</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-0.007188</td>\n",
       "      <td>-0.024172</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>-0.039699</td>\n",
       "      <td>-0.003479</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>979</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.017750</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>0.015545</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.016710</td>\n",
       "      <td>0.019228</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>-0.040241</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>-0.017201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>981</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021679</td>\n",
       "      <td>0.032041</td>\n",
       "      <td>0.039816</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>0.028949</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.049019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>-0.022476</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.023852</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>-0.012084</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.036288</td>\n",
       "      <td>0.041053</td>\n",
       "      <td>-0.020226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>983</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009557</td>\n",
       "      <td>0.035745</td>\n",
       "      <td>-0.005842</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>-0.023706</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>-0.065897</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.028368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>0.070466</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>-0.037786</td>\n",
       "      <td>0.036171</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.032657</td>\n",
       "      <td>0.073908</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>-0.033111</td>\n",
       "      <td>-0.026511</td>\n",
       "      <td>-0.007198</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>-0.009751</td>\n",
       "      <td>-0.023082</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>-0.015459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>986</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024481</td>\n",
       "      <td>-0.028785</td>\n",
       "      <td>-0.004831</td>\n",
       "      <td>-0.042057</td>\n",
       "      <td>-0.059556</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>0.021789</td>\n",
       "      <td>0.079927</td>\n",
       "      <td>0.014912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows  138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Substitutions_var  Stop_codon_var  Fusion_var  gene_fusion_var  \\\n",
       "0      1                  1               0           0              0.0   \n",
       "1      2                  0               0           0              0.0   \n",
       "2      3                  1               0           0              0.0   \n",
       "3      4                  1               0           0              0.0   \n",
       "4      5                  0               0           0              0.0   \n",
       "5      6                  1               0           0              0.0   \n",
       "6      7                  1               0           0              0.0   \n",
       "7      8                  1               0           0              0.0   \n",
       "8      9                  1               0           0              0.0   \n",
       "9     10                  1               0           0              0.0   \n",
       "10    11                  1               0           0              0.0   \n",
       "11    12                  1               0           0              0.0   \n",
       "12    13                  1               0           0              0.0   \n",
       "13    14                  1               0           0              0.0   \n",
       "14    15                  1               0           0              0.0   \n",
       "15    16                  1               0           0              0.0   \n",
       "16    17                  1               0           0              0.0   \n",
       "17    18                  1               0           0              0.0   \n",
       "18    19                  1               0           0              0.0   \n",
       "19    20                  1               0           0              0.0   \n",
       "20    21                  1               0           0              0.0   \n",
       "21    22                  1               0           0              0.0   \n",
       "22    23                  1               0           0              0.0   \n",
       "23    24                  0               0           0              1.0   \n",
       "24    25                  1               0           0              0.0   \n",
       "25    26                  1               0           0              0.0   \n",
       "26    27                  1               0           0              0.0   \n",
       "27    28                  1               0           0              0.0   \n",
       "28    29                  1               0           0              0.0   \n",
       "29    30                  1               0           0              0.0   \n",
       "..   ...                ...             ...         ...              ...   \n",
       "956  957                  1               0           0              0.0   \n",
       "957  958                  1               0           0              0.0   \n",
       "958  959                  1               0           0              0.0   \n",
       "959  960                  1               0           0              0.0   \n",
       "960  961                  1               0           0              0.0   \n",
       "961  962                  0               0           0              0.0   \n",
       "962  963                  0               0           0              0.0   \n",
       "963  964                  1               0           0              0.0   \n",
       "964  965                  1               0           0              0.0   \n",
       "965  966                  1               0           0              0.0   \n",
       "966  967                  1               0           0              0.0   \n",
       "967  968                  1               0           0              0.0   \n",
       "968  969                  1               0           0              0.0   \n",
       "969  970                  1               0           0              0.0   \n",
       "970  971                  1               0           0              0.0   \n",
       "971  972                  1               0           0              0.0   \n",
       "972  973                  1               0           0              0.0   \n",
       "973  974                  1               0           0              0.0   \n",
       "974  975                  1               0           0              0.0   \n",
       "975  976                  1               0           0              0.0   \n",
       "976  977                  1               0           0              0.0   \n",
       "977  978                  1               0           0              0.0   \n",
       "978  979                  1               0           0              0.0   \n",
       "979  980                  1               0           0              0.0   \n",
       "980  981                  1               0           0              0.0   \n",
       "981  982                  1               0           0              0.0   \n",
       "982  983                  1               0           0              0.0   \n",
       "983  984                  1               0           0              0.0   \n",
       "984  985                  1               0           0              0.0   \n",
       "985  986                  1               0           0              0.0   \n",
       "\n",
       "     Deletion_var  del_or_ins_var  Amplification_var  Truncation_var  \\\n",
       "0             0.0             0.0                  0               0   \n",
       "1             0.0             0.0                  0               1   \n",
       "2             0.0             0.0                  0               0   \n",
       "3             0.0             0.0                  0               0   \n",
       "4             0.0             1.0                  0               0   \n",
       "5             0.0             0.0                  0               0   \n",
       "6             0.0             0.0                  0               0   \n",
       "7             0.0             0.0                  0               0   \n",
       "8             0.0             0.0                  0               0   \n",
       "9             0.0             0.0                  0               0   \n",
       "10            0.0             0.0                  0               0   \n",
       "11            0.0             0.0                  0               0   \n",
       "12            0.0             0.0                  0               0   \n",
       "13            0.0             0.0                  0               0   \n",
       "14            0.0             0.0                  0               0   \n",
       "15            0.0             0.0                  0               0   \n",
       "16            0.0             0.0                  0               0   \n",
       "17            0.0             0.0                  0               0   \n",
       "18            0.0             0.0                  0               0   \n",
       "19            0.0             0.0                  0               0   \n",
       "20            0.0             0.0                  0               0   \n",
       "21            0.0             0.0                  0               0   \n",
       "22            0.0             0.0                  0               0   \n",
       "23            0.0             0.0                  0               0   \n",
       "24            0.0             0.0                  0               0   \n",
       "25            0.0             0.0                  0               0   \n",
       "26            0.0             0.0                  0               0   \n",
       "27            0.0             0.0                  0               0   \n",
       "28            0.0             0.0                  0               0   \n",
       "29            0.0             0.0                  0               0   \n",
       "..            ...             ...                ...             ...   \n",
       "956           0.0             0.0                  0               0   \n",
       "957           0.0             0.0                  0               0   \n",
       "958           0.0             0.0                  0               0   \n",
       "959           0.0             0.0                  0               0   \n",
       "960           0.0             0.0                  0               0   \n",
       "961           0.0             1.0                  0               0   \n",
       "962           1.0             0.0                  0               0   \n",
       "963           0.0             0.0                  0               0   \n",
       "964           0.0             0.0                  0               0   \n",
       "965           0.0             0.0                  0               0   \n",
       "966           0.0             0.0                  0               0   \n",
       "967           0.0             0.0                  0               0   \n",
       "968           0.0             0.0                  0               0   \n",
       "969           0.0             0.0                  0               0   \n",
       "970           0.0             0.0                  0               0   \n",
       "971           0.0             0.0                  0               0   \n",
       "972           0.0             0.0                  0               0   \n",
       "973           0.0             0.0                  0               0   \n",
       "974           0.0             0.0                  0               0   \n",
       "975           0.0             0.0                  0               0   \n",
       "976           0.0             0.0                  0               0   \n",
       "977           0.0             0.0                  0               0   \n",
       "978           0.0             0.0                  0               0   \n",
       "979           0.0             0.0                  0               0   \n",
       "980           0.0             0.0                  0               0   \n",
       "981           0.0             0.0                  0               0   \n",
       "982           0.0             0.0                  0               0   \n",
       "983           0.0             0.0                  0               0   \n",
       "984           0.0             0.0                  0               0   \n",
       "985           0.0             0.0                  0               0   \n",
       "\n",
       "     exon_var     ...       tsvd_100_90  tsvd_100_91  tsvd_100_92  \\\n",
       "0           0     ...          0.051136    -0.064537     0.013280   \n",
       "1           0     ...         -0.009623    -0.046242    -0.024687   \n",
       "2           0     ...         -0.015950    -0.011164    -0.017143   \n",
       "3           0     ...         -0.028990    -0.017455    -0.015833   \n",
       "4           0     ...         -0.028298    -0.021922     0.033257   \n",
       "5           0     ...          0.003633     0.017127    -0.010975   \n",
       "6           0     ...         -0.024859    -0.023065     0.010567   \n",
       "7           0     ...         -0.020009     0.029894    -0.044784   \n",
       "8           0     ...         -0.020198    -0.006154     0.036330   \n",
       "9           0     ...          0.010341    -0.012949     0.026076   \n",
       "10          0     ...         -0.014051    -0.005544    -0.003668   \n",
       "11          0     ...         -0.034008    -0.008175    -0.030609   \n",
       "12          0     ...          0.030524    -0.057539     0.035946   \n",
       "13          0     ...          0.018035    -0.002750     0.001941   \n",
       "14          0     ...          0.009624     0.010758    -0.007104   \n",
       "15          0     ...         -0.022531     0.007846     0.015159   \n",
       "16          0     ...         -0.020397    -0.001123    -0.007373   \n",
       "17          0     ...         -0.029122    -0.008562     0.010967   \n",
       "18          0     ...         -0.017137     0.008297     0.007081   \n",
       "19          0     ...          0.011378     0.024027    -0.002159   \n",
       "20          0     ...         -0.024888    -0.029220    -0.017481   \n",
       "21          0     ...          0.010959     0.026147    -0.000762   \n",
       "22          0     ...         -0.033148    -0.012930    -0.013770   \n",
       "23          0     ...          0.004811    -0.020472    -0.030329   \n",
       "24          0     ...          0.001537     0.006068    -0.026544   \n",
       "25          0     ...         -0.039869    -0.006410     0.043426   \n",
       "26          0     ...         -0.011529     0.030505    -0.005657   \n",
       "27          0     ...          0.014243    -0.039619    -0.012452   \n",
       "28          0     ...         -0.018088    -0.036646    -0.012280   \n",
       "29          0     ...          0.047761    -0.060715    -0.007001   \n",
       "..        ...     ...               ...          ...          ...   \n",
       "956         0     ...         -0.000013    -0.005420     0.007683   \n",
       "957         0     ...         -0.038488    -0.063254    -0.005914   \n",
       "958         0     ...         -0.051600     0.030485    -0.010512   \n",
       "959         0     ...         -0.031629    -0.008132    -0.024797   \n",
       "960         0     ...          0.004150     0.056880    -0.004535   \n",
       "961         0     ...         -0.017904    -0.014393     0.025920   \n",
       "962         0     ...          0.020342    -0.006161    -0.025204   \n",
       "963         0     ...          0.001520    -0.016413    -0.002413   \n",
       "964         0     ...          0.024264     0.002476     0.014648   \n",
       "965         0     ...          0.007944     0.012745     0.035874   \n",
       "966         0     ...          0.009174     0.097810    -0.079139   \n",
       "967         0     ...         -0.049937    -0.005890    -0.020829   \n",
       "968         0     ...         -0.006430     0.018362    -0.009630   \n",
       "969         0     ...         -0.030979     0.002331    -0.008560   \n",
       "970         0     ...          0.003935    -0.015223     0.004218   \n",
       "971         0     ...          0.023165    -0.016989     0.002991   \n",
       "972         0     ...         -0.003913    -0.006735     0.007143   \n",
       "973         0     ...         -0.011129     0.032191    -0.012299   \n",
       "974         0     ...         -0.000164     0.027180    -0.003207   \n",
       "975         0     ...          0.004157     0.012737    -0.010052   \n",
       "976         0     ...         -0.025212     0.002856    -0.015792   \n",
       "977         0     ...          0.019619     0.007786    -0.005868   \n",
       "978         0     ...          0.003541     0.017750     0.018489   \n",
       "979         0     ...         -0.002532    -0.016710     0.019228   \n",
       "980         0     ...          0.021679     0.032041     0.039816   \n",
       "981         0     ...          0.021618    -0.022476     0.021843   \n",
       "982         0     ...          0.009557     0.035745    -0.005842   \n",
       "983         0     ...          0.029711     0.070466     0.056033   \n",
       "984         0     ...         -0.003017    -0.033111    -0.026511   \n",
       "985         0     ...         -0.024481    -0.028785    -0.004831   \n",
       "\n",
       "     tsvd_100_93  tsvd_100_94  tsvd_100_95  tsvd_100_96  tsvd_100_97  \\\n",
       "0      -0.051711    -0.008686     0.063702    -0.033369     0.047507   \n",
       "1       0.002689     0.001146    -0.004227     0.013684    -0.019795   \n",
       "2       0.013692     0.025594    -0.012595     0.012041     0.009233   \n",
       "3       0.007888     0.013270    -0.043635     0.029360     0.002756   \n",
       "4      -0.012296     0.008621    -0.002116    -0.014589    -0.009098   \n",
       "5       0.000863    -0.018487    -0.034054    -0.025474     0.018020   \n",
       "6       0.014682     0.035405     0.000287    -0.002926    -0.015661   \n",
       "7      -0.002686     0.002535     0.013052     0.041337     0.006878   \n",
       "8      -0.057816     0.043661    -0.056993    -0.018344     0.029321   \n",
       "9       0.015974     0.019916    -0.008094    -0.030111    -0.035647   \n",
       "10      0.016076    -0.038488    -0.006138     0.005727    -0.011831   \n",
       "11      0.004124     0.041401     0.044187     0.006327    -0.012347   \n",
       "12     -0.009523     0.059923    -0.057699     0.057335    -0.015719   \n",
       "13      0.002201     0.012474     0.001954     0.053938     0.009063   \n",
       "14     -0.001898     0.015428    -0.007819     0.015587     0.017695   \n",
       "15      0.023245    -0.007217    -0.011820     0.003556     0.013171   \n",
       "16     -0.009267     0.007430     0.010738     0.023042    -0.003768   \n",
       "17      0.025018     0.017443     0.015389     0.027642    -0.063516   \n",
       "18     -0.030830     0.098508     0.020280    -0.002138     0.016567   \n",
       "19      0.011973     0.010937    -0.001382     0.025168     0.015134   \n",
       "20     -0.006440    -0.002205     0.007648     0.017762    -0.032245   \n",
       "21      0.051707    -0.007708    -0.011751    -0.004379    -0.000501   \n",
       "22      0.048266    -0.003942    -0.006718     0.002932    -0.008119   \n",
       "23     -0.005698     0.042732     0.018296    -0.019918     0.016466   \n",
       "24      0.013855     0.010058    -0.011230     0.002488     0.032603   \n",
       "25     -0.032859    -0.067035    -0.036653    -0.001485    -0.022597   \n",
       "26     -0.032345     0.003874     0.027838    -0.054474     0.021200   \n",
       "27     -0.045782     0.017577     0.009155    -0.018471    -0.009306   \n",
       "28      0.027191     0.042585     0.009550    -0.019602    -0.019053   \n",
       "29     -0.041101    -0.024852     0.004319    -0.007110     0.011242   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "956    -0.007658     0.012154    -0.018029    -0.011289    -0.008423   \n",
       "957     0.056374    -0.022311    -0.027941     0.019331    -0.007126   \n",
       "958    -0.081855     0.017823     0.036209     0.063302    -0.037599   \n",
       "959    -0.012760     0.040386    -0.010829    -0.021343    -0.013362   \n",
       "960    -0.007152     0.000986     0.024921    -0.000999    -0.006827   \n",
       "961     0.010755    -0.102925    -0.089625     0.034681    -0.022845   \n",
       "962     0.000475    -0.004567    -0.020932     0.004415    -0.003056   \n",
       "963     0.000641     0.006603     0.034714     0.001343     0.034937   \n",
       "964     0.007525    -0.010687     0.013754     0.000242    -0.060032   \n",
       "965     0.000767     0.002555     0.014930     0.034458    -0.000821   \n",
       "966    -0.010697     0.010222     0.028738    -0.012810     0.101421   \n",
       "967     0.054385     0.025034    -0.012460    -0.006200     0.018094   \n",
       "968    -0.025726     0.031948    -0.008503    -0.024613     0.015400   \n",
       "969     0.036986     0.005763     0.012373    -0.022348    -0.011978   \n",
       "970     0.000647     0.001878    -0.001927     0.004111     0.005555   \n",
       "971     0.004300     0.023908     0.023082     0.010657     0.018187   \n",
       "972    -0.013777    -0.027479     0.008085    -0.013028    -0.021680   \n",
       "973     0.051360    -0.010182     0.006550     0.002169     0.020846   \n",
       "974    -0.024585    -0.007336    -0.017340    -0.002462     0.002455   \n",
       "975    -0.016493    -0.015317     0.013800     0.009617    -0.023624   \n",
       "976     0.038130    -0.011176     0.042560     0.035254    -0.023301   \n",
       "977    -0.007188    -0.024172     0.015933     0.009245    -0.039699   \n",
       "978     0.003212    -0.004919     0.026681     0.015545     0.015496   \n",
       "979     0.011997    -0.040241    -0.007928     0.010460     0.016918   \n",
       "980     0.014840    -0.009871     0.028949     0.010911    -0.001771   \n",
       "981     0.023852     0.005158    -0.012084     0.010867     0.036288   \n",
       "982     0.013066     0.001824    -0.023706     0.011661    -0.065897   \n",
       "983    -0.037786     0.036171     0.023641     0.032657     0.073908   \n",
       "984    -0.007198     0.020134    -0.009751    -0.023082     0.003664   \n",
       "985    -0.042057    -0.059556     0.026780     0.011944     0.021789   \n",
       "\n",
       "     tsvd_100_98  tsvd_100_99  \n",
       "0      -0.011223    -0.025000  \n",
       "1      -0.023560     0.012469  \n",
       "2       0.001668    -0.041702  \n",
       "3      -0.036137    -0.002726  \n",
       "4       0.006339     0.012608  \n",
       "5       0.011501    -0.002968  \n",
       "6      -0.015077    -0.038714  \n",
       "7      -0.007241     0.024137  \n",
       "8      -0.027651     0.022929  \n",
       "9       0.016012    -0.024145  \n",
       "10     -0.071792    -0.045738  \n",
       "11     -0.035557    -0.030858  \n",
       "12      0.063174     0.009297  \n",
       "13     -0.013379    -0.027928  \n",
       "14     -0.031232     0.008454  \n",
       "15     -0.013801     0.003631  \n",
       "16      0.025693    -0.031111  \n",
       "17     -0.010742    -0.021103  \n",
       "18      0.058039    -0.051571  \n",
       "19     -0.021840     0.011064  \n",
       "20     -0.024125     0.027851  \n",
       "21     -0.022172    -0.001265  \n",
       "22      0.029783     0.030334  \n",
       "23      0.020951     0.010637  \n",
       "24     -0.012556    -0.001941  \n",
       "25      0.014971    -0.012888  \n",
       "26      0.062335    -0.017426  \n",
       "27      0.003947    -0.011061  \n",
       "28     -0.004750     0.001856  \n",
       "29      0.006551    -0.024656  \n",
       "..           ...          ...  \n",
       "956    -0.008038     0.001655  \n",
       "957     0.006075    -0.030255  \n",
       "958    -0.017483    -0.007580  \n",
       "959    -0.012785    -0.020401  \n",
       "960     0.044572    -0.026911  \n",
       "961     0.042648     0.062812  \n",
       "962    -0.014628     0.003386  \n",
       "963     0.000475    -0.033486  \n",
       "964    -0.007774     0.004998  \n",
       "965    -0.013426     0.018680  \n",
       "966     0.124255    -0.010983  \n",
       "967     0.011509     0.013093  \n",
       "968     0.027094     0.014422  \n",
       "969     0.029581     0.035954  \n",
       "970    -0.003645     0.027136  \n",
       "971     0.010691    -0.014933  \n",
       "972    -0.000108     0.009736  \n",
       "973     0.023785     0.037617  \n",
       "974     0.016052     0.010592  \n",
       "975     0.047490     0.020412  \n",
       "976     0.036834     0.037147  \n",
       "977    -0.003479     0.018315  \n",
       "978    -0.009649     0.012350  \n",
       "979     0.024508    -0.017201  \n",
       "980     0.003372     0.049019  \n",
       "981     0.041053    -0.020226  \n",
       "982     0.000385     0.028368  \n",
       "983     0.036812     0.010418  \n",
       "984     0.008327    -0.015459  \n",
       "985     0.079927     0.014912  \n",
       "\n",
       "[986 rows x 138 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_test[\"tfidf_tsvd_100\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH PHASE ALL ALGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train= {} \n",
    "work_test = {}\n",
    "pre_process=[\"d2v_100.csv\",\"tfidf_tsvd_100.csv\",\"w2v_100.csv\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_train_\"+f)\n",
    "    work_test[re.sub(\"\\.csv\",\"\",f)] = pd.read_csv(path+\"nw_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 67.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v_100\n",
      "[mean: -1.10183, std: 0.04434, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.11608, std: 0.03952, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.03937, std: 0.05671, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.06731, std: 0.05026, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.10267, std: 0.04157, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.11616, std: 0.03778, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.04035, std: 0.05765, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.06385, std: 0.04691, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.01124, std: 0.06010, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.02515, std: 0.05799, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.00477, std: 0.07964, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01056, std: 0.07415, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.01882, std: 0.05451, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.03160, std: 0.04726, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.99883, std: 0.07398, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.00864, std: 0.06668, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.99059, std: 0.06817, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.99940, std: 0.06953, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.05722, std: 0.09074, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.02819, std: 0.08708, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.98799, std: 0.06035, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.99927, std: 0.06163, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.01819, std: 0.08175, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01683, std: 0.07830, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.09443, std: 0.04345, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.11642, std: 0.03855, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.03659, std: 0.06029, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.06728, std: 0.04848, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.09895, std: 0.04032, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.11983, std: 0.03718, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.03765, std: 0.05590, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.06607, std: 0.04853, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -1.00796, std: 0.06270, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.02711, std: 0.05615, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.00691, std: 0.08267, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01188, std: 0.07502, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -1.01173, std: 0.05402, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.03254, std: 0.05427, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.99996, std: 0.07450, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01476, std: 0.07668, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.99579, std: 0.07367, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.00193, std: 0.06885, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.06911, std: 0.09549, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.03693, std: 0.08953, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.98985, std: 0.06490, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -1.01186, std: 0.06765, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -1.02979, std: 0.08652, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.02639, std: 0.08793, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}]\n",
      "{'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "-0.98799150285\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 94.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -0.91554, std: 0.04153, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92413, std: 0.04066, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.88847, std: 0.05802, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.88659, std: 0.05416, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.91610, std: 0.04252, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92441, std: 0.03528, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.88585, std: 0.05543, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.88395, std: 0.05074, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.89044, std: 0.05955, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.88160, std: 0.05815, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.95667, std: 0.07430, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.93311, std: 0.07024, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.88664, std: 0.05694, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.87274, std: 0.05181, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.93369, std: 0.06888, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.91751, std: 0.06505, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.92980, std: 0.05762, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.91204, std: 0.06112, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.03594, std: 0.06742, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01363, std: 0.07201, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.90925, std: 0.06001, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.90258, std: 0.05873, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.98317, std: 0.06987, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.98173, std: 0.07001, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.91606, std: 0.04439, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92785, std: 0.03948, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.89280, std: 0.05725, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.88982, std: 0.05375, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.91480, std: 0.04319, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92697, std: 0.03796, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.88708, std: 0.05665, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.88487, std: 0.05092, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.89263, std: 0.05421, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.88812, std: 0.05432, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.96541, std: 0.07203, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.94167, std: 0.06842, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.88725, std: 0.05275, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.88247, std: 0.05681, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.94094, std: 0.06844, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.92920, std: 0.07135, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.93229, std: 0.05640, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92617, std: 0.05876, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.04139, std: 0.06910, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.03283, std: 0.07101, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.90583, std: 0.05494, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.91345, std: 0.05653, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.98477, std: 0.06433, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.99532, std: 0.06827, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}]\n",
      "{'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}\n",
      "-0.87273601942\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 90.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 125.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_100\n",
      "[mean: -0.93032, std: 0.03761, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.94576, std: 0.03704, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.89707, std: 0.04870, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.90587, std: 0.04871, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.93094, std: 0.03437, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.94363, std: 0.03466, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.89250, std: 0.04405, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.89994, std: 0.04322, params: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.89708, std: 0.04755, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.89127, std: 0.05070, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.96616, std: 0.06068, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.94205, std: 0.06367, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.88839, std: 0.04591, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.88599, std: 0.04980, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.93715, std: 0.05805, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.92563, std: 0.06260, params: {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.93712, std: 0.06024, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92067, std: 0.05903, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.04438, std: 0.06818, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.01836, std: 0.07304, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.90871, std: 0.05477, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.90166, std: 0.05386, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.98839, std: 0.06442, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.97320, std: 0.06557, params: {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.93425, std: 0.03885, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.94238, std: 0.03433, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.90717, std: 0.05147, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.90630, std: 0.04512, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.93264, std: 0.03677, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.94063, std: 0.03439, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.89789, std: 0.04993, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.90061, std: 0.04397, params: {'colsample_bytree': 1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.90102, std: 0.05185, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.89497, std: 0.05498, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -0.97424, std: 0.06263, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.94709, std: 0.07176, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.89196, std: 0.05116, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.88566, std: 0.04898, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.94580, std: 0.06587, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.92880, std: 0.06370, params: {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}, mean: -0.94369, std: 0.05694, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.92677, std: 0.06189, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}, mean: -1.05130, std: 0.06746, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}, mean: -1.03046, std: 0.07684, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1}, mean: -0.91398, std: 0.05503, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}, mean: -0.90267, std: 0.05546, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}, mean: -0.99602, std: 0.06629, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}, mean: -0.97601, std: 0.06824, params: {'colsample_bytree': 1, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 1}]\n",
      "{'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}\n",
      "-0.885656806362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_xgboost=XGBClassifier(max_depth=3, objective=\"multi:softprob\",seed=26)\n",
    "param_test= {\n",
    "    \"max_depth\" : [3,5,7],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"n_estimators\" : [100,200],\n",
    "    \"subsample\":[0.8,1],\n",
    "    \"colsample_bytree\":[0.8,1]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_xgboost, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
    "#tfidf : {'colsample_bytree': 0.8, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}\n",
    "#w2v : {'colsample_bytree': 1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v_100\n",
      "[mean: -1.71471, std: 0.00934, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.71198, std: 0.00720, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.65478, std: 0.02224, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.65026, std: 0.01736, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.63703, std: 0.01133, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.63596, std: 0.02267, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.62014, std: 0.01198, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.61963, std: 0.02007, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.40123, std: 0.02087, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.40411, std: 0.02018, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.32583, std: 0.02570, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.32519, std: 0.02470, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.30712, std: 0.02190, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.30981, std: 0.02733, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.29615, std: 0.02897, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.29135, std: 0.02776, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.26930, std: 0.02649, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.27241, std: 0.02925, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.21383, std: 0.03676, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.21352, std: 0.04300, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.19508, std: 0.03326, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.20114, std: 0.04501, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.18226, std: 0.04133, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.17783, std: 0.04681, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.70753, std: 0.00980, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.70615, std: 0.01023, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.64858, std: 0.01022, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.64097, std: 0.01233, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.62638, std: 0.00972, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.62377, std: 0.01010, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.61271, std: 0.00929, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.61088, std: 0.01873, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.39452, std: 0.01900, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.39687, std: 0.01762, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.32444, std: 0.02110, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.31843, std: 0.01972, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.29985, std: 0.02321, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.29344, std: 0.02681, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.28347, std: 0.02294, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.27893, std: 0.02790, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.27224, std: 0.02598, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.26941, std: 0.02896, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.21501, std: 0.03306, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.21042, std: 0.04014, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.18758, std: 0.03266, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.19110, std: 0.04547, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.17187, std: 0.03659, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.16844, std: 0.04515, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}]\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
      "-1.16844338777\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -1.59855, std: 0.01637, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.59189, std: 0.01554, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.53482, std: 0.01789, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.52955, std: 0.01667, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.51620, std: 0.01840, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.50908, std: 0.01384, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.49688, std: 0.01829, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.48604, std: 0.01470, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.22447, std: 0.02130, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.22176, std: 0.02416, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.13658, std: 0.02114, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.13600, std: 0.02406, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.11283, std: 0.02366, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.11236, std: 0.01930, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.08738, std: 0.02261, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.07987, std: 0.02242, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.04445, std: 0.02697, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.04293, std: 0.03023, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -0.96458, std: 0.03078, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -0.96350, std: 0.03400, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -0.94805, std: 0.03119, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -0.94547, std: 0.03351, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -0.92979, std: 0.03410, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -0.92298, std: 0.03888, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.59209, std: 0.01376, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.58368, std: 0.01465, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.52896, std: 0.01631, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.52444, std: 0.01013, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.50601, std: 0.01674, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.49747, std: 0.01234, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.48948, std: 0.02115, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.48164, std: 0.01317, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.22195, std: 0.02297, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.21750, std: 0.02203, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.13366, std: 0.02041, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.12991, std: 0.01841, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.10331, std: 0.02228, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.09950, std: 0.02090, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.08580, std: 0.02496, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.07888, std: 0.02002, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.04189, std: 0.02728, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.03874, std: 0.02775, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -0.96332, std: 0.02802, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -0.95832, std: 0.03068, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -0.93899, std: 0.03168, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -0.93365, std: 0.03138, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -0.92825, std: 0.03650, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -0.92395, std: 0.03686, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}]\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
      "-0.922979447054\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_100\n",
      "[mean: -1.61438, std: 0.01175, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.61260, std: 0.00565, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.55027, std: 0.01063, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.54867, std: 0.00930, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.52788, std: 0.01277, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.52228, std: 0.01232, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.50684, std: 0.01323, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.50245, std: 0.01157, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.24414, std: 0.02279, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.24503, std: 0.01693, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.15575, std: 0.01824, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.15301, std: 0.02030, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.12829, std: 0.02228, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.12514, std: 0.02135, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.10256, std: 0.01990, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.10012, std: 0.01506, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.06675, std: 0.02948, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.06831, std: 0.02488, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -0.98571, std: 0.02479, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -0.97802, std: 0.02891, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -0.96199, std: 0.02994, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -0.96234, std: 0.03328, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -0.94585, std: 0.03149, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -0.94393, std: 0.03289, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.60875, std: 0.01287, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.60784, std: 0.00964, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.54048, std: 0.01229, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.53635, std: 0.00672, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.52099, std: 0.01124, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.51206, std: 0.00620, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.50120, std: 0.01406, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.48673, std: 0.00536, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 8, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.24235, std: 0.02170, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.24344, std: 0.01770, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 6, 'subsample': 0.8}, mean: -1.14489, std: 0.01893, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.7}, mean: -1.14808, std: 0.01440, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 12, 'subsample': 0.8}, mean: -1.12317, std: 0.02086, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.7}, mean: -1.11574, std: 0.01726, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 16, 'subsample': 0.8}, mean: -1.09914, std: 0.02020, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.7}, mean: -1.08621, std: 0.01432, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 24, 'num_leaves': 22, 'subsample': 0.8}, mean: -1.06709, std: 0.02627, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.7}, mean: -1.06908, std: 0.02798, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 6, 'subsample': 0.8}, mean: -0.97864, std: 0.02923, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.7}, mean: -0.97943, std: 0.03076, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 12, 'subsample': 0.8}, mean: -0.95999, std: 0.03461, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.7}, mean: -0.95186, std: 0.03177, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 16, 'subsample': 0.8}, mean: -0.94524, std: 0.03637, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.7}, mean: -0.93148, std: 0.02966, params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}]\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
      "-0.931482969854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_lgbm=LGBMClassifier(seed=26)\n",
    "param_test= {\n",
    "    'n_estimators': [8,24,48],\n",
    "    'num_leaves': [6,12,16,22],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'colsample_bytree' : [0.7,0.8],\n",
    "    'subsample' : [0.7,0.8]\n",
    "    }\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_lgbm, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
    "#tfidf : {'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}\n",
    "#w2v : {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'n_estimators': 48, 'num_leaves': 22, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   43.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v_100\n",
      "[mean: -1.86803, std: 0.03398, params: {'C': 0.001, 'penalty': 'l1'}, mean: -1.85301, std: 0.03435, params: {'C': 0.001, 'penalty': 'l2'}, mean: -1.85695, std: 0.03592, params: {'C': 0.01, 'penalty': 'l1'}, mean: -1.76132, std: 0.02708, params: {'C': 0.01, 'penalty': 'l2'}, mean: -1.48878, std: 0.03079, params: {'C': 0.1, 'penalty': 'l1'}, mean: -1.70851, std: 0.02299, params: {'C': 0.1, 'penalty': 'l2'}, mean: -1.28133, std: 0.02638, params: {'C': 1, 'penalty': 'l1'}, mean: -1.62934, std: 0.04810, params: {'C': 1, 'penalty': 'l2'}, mean: -1.26483, std: 0.03273, params: {'C': 10, 'penalty': 'l1'}, mean: -1.62548, std: 0.04035, params: {'C': 10, 'penalty': 'l2'}, mean: -1.31990, std: 0.04065, params: {'C': 100, 'penalty': 'l1'}, mean: -1.64046, std: 0.03516, params: {'C': 100, 'penalty': 'l2'}, mean: -1.41898, std: 0.06563, params: {'C': 1000, 'penalty': 'l1'}, mean: -1.63924, std: 0.03499, params: {'C': 1000, 'penalty': 'l2'}]\n",
      "{'C': 10, 'penalty': 'l1'}\n",
      "-1.2648347351\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -1.86803, std: 0.03398, params: {'C': 0.001, 'penalty': 'l1'}, mean: -1.84770, std: 0.03393, params: {'C': 0.001, 'penalty': 'l2'}, mean: -1.85690, std: 0.03590, params: {'C': 0.01, 'penalty': 'l1'}, mean: -1.72502, std: 0.03332, params: {'C': 0.01, 'penalty': 'l2'}, mean: -1.37320, std: 0.03032, params: {'C': 0.1, 'penalty': 'l1'}, mean: -1.54961, std: 0.03515, params: {'C': 0.1, 'penalty': 'l2'}, mean: -1.04443, std: 0.03156, params: {'C': 1, 'penalty': 'l1'}, mean: -1.50064, std: 0.04759, params: {'C': 1, 'penalty': 'l2'}, mean: -0.97721, std: 0.03534, params: {'C': 10, 'penalty': 'l1'}, mean: -1.46547, std: 0.04555, params: {'C': 10, 'penalty': 'l2'}, mean: -1.11089, std: 0.06555, params: {'C': 100, 'penalty': 'l1'}, mean: -1.47910, std: 0.05720, params: {'C': 100, 'penalty': 'l2'}, mean: -1.25983, std: 0.09510, params: {'C': 1000, 'penalty': 'l1'}, mean: -1.47220, std: 0.03731, params: {'C': 1000, 'penalty': 'l2'}]\n",
      "{'C': 10, 'penalty': 'l1'}\n",
      "-0.977210584672\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_100\n",
      "[mean: -1.86803, std: 0.03397, params: {'C': 0.001, 'penalty': 'l1'}, mean: -1.73130, std: 0.02830, params: {'C': 0.001, 'penalty': 'l2'}, mean: -1.79844, std: 0.03595, params: {'C': 0.01, 'penalty': 'l1'}, mean: -1.49137, std: 0.02284, params: {'C': 0.01, 'penalty': 'l2'}, mean: -1.32659, std: 0.03685, params: {'C': 0.1, 'penalty': 'l1'}, mean: -1.34454, std: 0.01237, params: {'C': 0.1, 'penalty': 'l2'}, mean: -1.07196, std: 0.04045, params: {'C': 1, 'penalty': 'l1'}, mean: -1.28040, std: 0.04270, params: {'C': 1, 'penalty': 'l2'}, mean: -1.06280, std: 0.05567, params: {'C': 10, 'penalty': 'l1'}, mean: -1.28403, std: 0.04111, params: {'C': 10, 'penalty': 'l2'}, mean: -1.16657, std: 0.09201, params: {'C': 100, 'penalty': 'l1'}, mean: -1.29447, std: 0.02789, params: {'C': 100, 'penalty': 'l2'}, mean: -1.23551, std: 0.11745, params: {'C': 1000, 'penalty': 'l1'}, mean: -1.27845, std: 0.04311, params: {'C': 1000, 'penalty': 'l2'}]\n",
      "{'C': 10, 'penalty': 'l1'}\n",
      "-1.06279894516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_log=LogisticRegression()\n",
    "param_test= {\n",
    "    \"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"penalty\" : [\"l1\",\"l2\"]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_log, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'C': 10, 'penalty': 'l1'}\n",
    "#tfidf : {'C': 10, 'penalty': 'l1'}\n",
    "#w2v : {'C': 10, 'penalty': 'l1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   46.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v_100\n",
      "[mean: -1.94169, std: 0.00465, params: {'learning_rate': 0.3, 'n_estimators': 50}, mean: -1.99920, std: 0.01060, params: {'learning_rate': 0.3, 'n_estimators': 100}, mean: -1.96065, std: 0.01302, params: {'learning_rate': 0.5, 'n_estimators': 50}, mean: -2.01419, std: 0.01107, params: {'learning_rate': 0.5, 'n_estimators': 100}, mean: -2.00529, std: 0.02161, params: {'learning_rate': 0.9, 'n_estimators': 50}, mean: -2.04604, std: 0.01959, params: {'learning_rate': 0.9, 'n_estimators': 100}, mean: -2.00737, std: 0.01516, params: {'learning_rate': 1, 'n_estimators': 50}, mean: -2.05062, std: 0.01401, params: {'learning_rate': 1, 'n_estimators': 100}]\n",
      "{'learning_rate': 0.3, 'n_estimators': 50}\n",
      "-1.94168942506\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -1.91750, std: 0.01409, params: {'learning_rate': 0.3, 'n_estimators': 50}, mean: -1.94188, std: 0.01189, params: {'learning_rate': 0.3, 'n_estimators': 100}, mean: -1.95182, std: 0.01644, params: {'learning_rate': 0.5, 'n_estimators': 50}, mean: -1.97111, std: 0.01814, params: {'learning_rate': 0.5, 'n_estimators': 100}, mean: -2.00289, std: 0.02692, params: {'learning_rate': 0.9, 'n_estimators': 50}, mean: -2.06171, std: 0.07002, params: {'learning_rate': 0.9, 'n_estimators': 100}, mean: -2.03082, std: 0.03772, params: {'learning_rate': 1, 'n_estimators': 50}, mean: -2.07098, std: 0.04586, params: {'learning_rate': 1, 'n_estimators': 100}]\n",
      "{'learning_rate': 0.3, 'n_estimators': 50}\n",
      "-1.91749502231\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_100\n",
      "[mean: -1.91682, std: 0.00978, params: {'learning_rate': 0.3, 'n_estimators': 50}, mean: -1.94374, std: 0.00987, params: {'learning_rate': 0.3, 'n_estimators': 100}, mean: -1.95939, std: 0.03143, params: {'learning_rate': 0.5, 'n_estimators': 50}, mean: -1.99379, std: 0.04053, params: {'learning_rate': 0.5, 'n_estimators': 100}, mean: -2.02985, std: 0.02044, params: {'learning_rate': 0.9, 'n_estimators': 50}, mean: -2.03370, std: 0.01915, params: {'learning_rate': 0.9, 'n_estimators': 100}, mean: -2.04482, std: 0.02814, params: {'learning_rate': 1, 'n_estimators': 50}, mean: -2.09304, std: 0.04078, params: {'learning_rate': 1, 'n_estimators': 100}]\n",
      "{'learning_rate': 0.3, 'n_estimators': 50}\n",
      "-1.91681966898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_ada=AdaBoostClassifier(n_estimators=100, learning_rate=1.0, algorithm=\"SAMME.R\", random_state=26)\n",
    "param_test={\n",
    "    \"n_estimators\":[50,100],\n",
    "    \"learning_rate\":[0.3,0.5,0.9,1]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_ada, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'learning_rate': 0.3, 'n_estimators': 50}\n",
    "#tfidf : {'learning_rate': 0.3, 'n_estimators': 50}\n",
    "#w2v : {'learning_rate': 0.3, 'n_estimators': 50}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v_100\n",
      "[mean: -1.22844, std: 0.03563, params: {'max_depth': 15, 'n_estimators': 200}, mean: -1.22791, std: 0.03435, params: {'max_depth': 15, 'n_estimators': 300}, mean: -1.22571, std: 0.03375, params: {'max_depth': 15, 'n_estimators': 400}, mean: -1.16550, std: 0.04155, params: {'max_depth': 20, 'n_estimators': 200}, mean: -1.16466, std: 0.04370, params: {'max_depth': 20, 'n_estimators': 300}, mean: -1.16096, std: 0.04186, params: {'max_depth': 20, 'n_estimators': 400}, mean: -1.15077, std: 0.04684, params: {'max_depth': 25, 'n_estimators': 200}, mean: -1.14424, std: 0.04787, params: {'max_depth': 25, 'n_estimators': 300}, mean: -1.14550, std: 0.04492, params: {'max_depth': 25, 'n_estimators': 400}, mean: -1.16388, std: 0.06146, params: {'max_depth': 30, 'n_estimators': 200}, mean: -1.15763, std: 0.04410, params: {'max_depth': 30, 'n_estimators': 300}, mean: -1.15487, std: 0.04284, params: {'max_depth': 30, 'n_estimators': 400}]\n",
      "{'max_depth': 25, 'n_estimators': 300}\n",
      "-1.14424083177\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_tsvd_100\n",
      "[mean: -0.94831, std: 0.02960, params: {'max_depth': 15, 'n_estimators': 200}, mean: -0.94279, std: 0.03010, params: {'max_depth': 15, 'n_estimators': 300}, mean: -0.95152, std: 0.02612, params: {'max_depth': 15, 'n_estimators': 400}, mean: -0.98358, std: 0.05267, params: {'max_depth': 20, 'n_estimators': 200}, mean: -0.97815, std: 0.02881, params: {'max_depth': 20, 'n_estimators': 300}, mean: -0.97319, std: 0.02574, params: {'max_depth': 20, 'n_estimators': 400}, mean: -1.24485, std: 0.07011, params: {'max_depth': 25, 'n_estimators': 200}, mean: -1.13895, std: 0.14529, params: {'max_depth': 25, 'n_estimators': 300}, mean: -1.09501, std: 0.06563, params: {'max_depth': 25, 'n_estimators': 400}, mean: -1.47265, std: 0.11060, params: {'max_depth': 30, 'n_estimators': 200}, mean: -1.39942, std: 0.18520, params: {'max_depth': 30, 'n_estimators': 300}, mean: -1.27881, std: 0.05559, params: {'max_depth': 30, 'n_estimators': 400}]\n",
      "{'max_depth': 15, 'n_estimators': 300}\n",
      "-0.94278938571\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_100\n",
      "[mean: -1.00518, std: 0.04244, params: {'max_depth': 15, 'n_estimators': 200}, mean: -0.98611, std: 0.03042, params: {'max_depth': 15, 'n_estimators': 300}, mean: -1.00611, std: 0.04150, params: {'max_depth': 15, 'n_estimators': 400}, mean: -1.18631, std: 0.08091, params: {'max_depth': 20, 'n_estimators': 200}, mean: -1.13412, std: 0.07174, params: {'max_depth': 20, 'n_estimators': 300}, mean: -1.09728, std: 0.05019, params: {'max_depth': 20, 'n_estimators': 400}, mean: -1.38482, std: 0.07572, params: {'max_depth': 25, 'n_estimators': 200}, mean: -1.27289, std: 0.04654, params: {'max_depth': 25, 'n_estimators': 300}, mean: -1.21932, std: 0.05829, params: {'max_depth': 25, 'n_estimators': 400}, mean: -1.42201, std: 0.07977, params: {'max_depth': 30, 'n_estimators': 200}, mean: -1.35885, std: 0.09196, params: {'max_depth': 30, 'n_estimators': 300}, mean: -1.29912, std: 0.03608, params: {'max_depth': 30, 'n_estimators': 400}]\n",
      "{'max_depth': 15, 'n_estimators': 300}\n",
      "-0.986113396697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_dt=RandomForestClassifier()\n",
    "param_test={\n",
    "    \"max_depth\":[15,20,25,30],\n",
    "    \"n_estimators\":[200,300,400]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_dt, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=kf,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)\n",
    "#d2v : {'max_depth': 25, 'n_estimators': 300}\n",
    "#tfidf : {'max_depth': 15, 'n_estimators': 300}\n",
    "#w2v : {'max_depth': 15, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING PHASE ALL ALGOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gen(X,X_test,y,classifier,file,five_fold_predict=True):\n",
    "    #if not os.path.exists(\"scores/\"+file):\n",
    "    #   os.makedirs(\"scores/\"+file)\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, random_state=26, shuffle=True)\n",
    "    if five_fold_predict:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "            fold += 1\n",
    "\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            p_test = clf.predict_proba(X_test)\n",
    "            y_test += p_test/5\n",
    "\n",
    "    classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"nw_scores/nw_stack_test/nw_{}.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train \") #peut etre que to array est exclusivement pour les xgb\n",
    "    \n",
    "    #if os.path.isfile(\"nw_scores/nw_stack_train/nw_{}.csv\".format(file)):\n",
    "       # print(\"not necessary, already done\")\n",
    "    \n",
    "    y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict_proba\")\n",
    "    subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "    subm1['ID'] = ID_train\n",
    "    subm1.to_csv(\"nw_scores/nw_stack_train/nw_{}.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost here\n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 138) (743, 138)\n",
      "Fold 2 (2949, 138) (740, 138)\n",
      "Fold 3 (2952, 138) (737, 138)\n",
      "Fold 4 (2954, 138) (735, 138)\n",
      "Fold 5 (2955, 138) (734, 138)\n",
      "cross_val sur train \n",
      "lgbm here\n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 138) (743, 138)\n",
      "Fold 2 (2949, 138) (740, 138)\n",
      "Fold 3 (2952, 138) (737, 138)\n",
      "Fold 4 (2954, 138) (735, 138)\n",
      "Fold 5 (2955, 138) (734, 138)\n",
      "cross_val sur train \n",
      "logreg here\n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 138) (743, 138)\n",
      "Fold 2 (2949, 138) (740, 138)\n",
      "Fold 3 (2952, 138) (737, 138)\n",
      "Fold 4 (2954, 138) (735, 138)\n",
      "Fold 5 (2955, 138) (734, 138)\n",
      "cross_val sur train \n",
      "adaboost here\n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 138) (743, 138)\n",
      "Fold 2 (2949, 138) (740, 138)\n",
      "Fold 3 (2952, 138) (737, 138)\n",
      "Fold 4 (2954, 138) (735, 138)\n",
      "Fold 5 (2955, 138) (734, 138)\n",
      "cross_val sur train \n",
      "random forest here\n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 137) (743, 137)\n",
      "Fold 2 (2949, 137) (740, 137)\n",
      "Fold 3 (2952, 137) (737, 137)\n",
      "Fold 4 (2954, 137) (735, 137)\n",
      "Fold 5 (2955, 137) (734, 137)\n",
      "cross_val sur train \n",
      "Fold 1 (2946, 138) (743, 138)\n",
      "Fold 2 (2949, 138) (740, 138)\n",
      "Fold 3 (2952, 138) (737, 138)\n",
      "Fold 4 (2954, 138) (735, 138)\n",
      "Fold 5 (2955, 138) (734, 138)\n",
      "cross_val sur train \n"
     ]
    }
   ],
   "source": [
    "dic_xgb={\"xgb_d2v\":XGBClassifier(colsample_bytree=0.8,max_depth=7,min_child_weight=3,n_estimators=100,subsample=0.8,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_tfidf\":XGBClassifier(colsample_bytree=0.8,max_depth=5,min_child_weight=3,n_estimators=100,subsample=1,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_w2v\":XGBClassifier(colsample_bytree=1,max_depth=5,min_child_weight=3,n_estimators=100,subsample=1,objective=\"multi:softprob\",seed=26)}\n",
    "dic_lgbm={\"lgbm_d2v\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.8,n_estimators=48,num_leaves=22,subsample=0.8,seed=26),\n",
    "        \"lgbm_tfidf\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.7,n_estimators=48,num_leaves=22,subsample=0.8,seed=26),\n",
    "        \"lgbm_w2v\":LGBMClassifier(boosting_type=\"gbdt\",colsample_bytree=0.8,n_estimators=48,num_leaves=22,subsample=0.8,seed=26)}\n",
    "dic_lr={\"lr_d2v\":LogisticRegression(C=10,penalty=\"l1\"),\n",
    "        \"lr_tfidf\":LogisticRegression(C=10,penalty=\"l1\"),\n",
    "        \"lr_w2v\":LogisticRegression(C=10,penalty=\"l1\")}\n",
    "dic_ada={\"ada_d2v\":AdaBoostClassifier(n_estimators=50, learning_rate=0.3, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_tfidf\":AdaBoostClassifier(n_estimators=50, learning_rate=0.3, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_w2v\":AdaBoostClassifier(n_estimators=50, learning_rate=0.3, algorithm=\"SAMME.R\", random_state=26)}\n",
    "dic_rf={\"rf_d2v\":RandomForestClassifier(n_estimators=300,max_depth=25,random_state=26),\n",
    "        \"rf_tfidf\":RandomForestClassifier(n_estimators=300,max_depth=15,random_state=26),\n",
    "        \"rf_w2v\":RandomForestClassifier(n_estimators=300,max_depth=15,random_state=26)}\n",
    "\n",
    "print(\"xgboost here\")\n",
    "for clf,name in zip(dic_xgb.keys(),work_train.keys()):\n",
    "    model_gen(X=np.array(work_train[name].drop(\"ID\",axis=1)),X_test=np.array(work_test[name].drop(\"ID\",axis=1)),y=y,classifier=dic_xgb[clf],file=clf)\n",
    "print(\"lgbm here\")\n",
    "for clf,name in zip(dic_lgbm.keys(),work_train.keys()):\n",
    "    model_gen(X=np.array(work_train[name].drop(\"ID\",axis=1)),X_test=np.array(work_test[name].drop(\"ID\",axis=1)),y=y,classifier=dic_lgbm[clf],file=clf)\n",
    "print(\"logreg here\")\n",
    "for clf,name in zip(dic_lr.keys(),work_train.keys()):\n",
    "    model_gen(X=np.array(work_train[name].drop(\"ID\",axis=1)),X_test=np.array(work_test[name].drop(\"ID\",axis=1)),y=y,classifier=dic_lr[clf],file=clf)\n",
    "print(\"adaboost here\")\n",
    "for clf,name in zip(dic_ada.keys(),work_train.keys()):\n",
    "    model_gen(X=np.array(work_train[name].drop(\"ID\",axis=1)),X_test=np.array(work_test[name].drop(\"ID\",axis=1)),y=y,classifier=dic_ada[clf],file=clf)\n",
    "print(\"random forest here\")\n",
    "for clf,name in zip(dic_rf.keys(),work_train.keys()):\n",
    "    model_gen(X=np.array(work_train[name].drop(\"ID\",axis=1)),X_test=np.array(work_test[name].drop(\"ID\",axis=1)),y=y,classifier=dic_rf[clf],file=clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
