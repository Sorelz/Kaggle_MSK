{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franck\\Documents\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import os \n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_train = {} \n",
    "work_test = {}\n",
    "pre_process=[\"d2v.npz\",\"tfidf.npz\",\"w2v.npz\"]\n",
    "path=\"checkpoints_databases/\"\n",
    "for f in pre_process:\n",
    "    work_train[re.sub(\"\\.npz\",\"\",f)] = ssp.load_npz(path+\"nw_working_train_\"+f)\n",
    "    work_test[re.sub(\"\\.npz\",\"\",f)] = ssp.load_npz(path+\"nw_working_test_\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2v\n",
      "tfidf\n",
      "w2v\n"
     ]
    }
   ],
   "source": [
    "for a in work_train:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.read_csv(\"..//bases/training_variants\").Class.values - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('..//bases/training_variants')\n",
    "test = pd.read_csv('..//bases/test_variants')\n",
    "ID_train=train.ID\n",
    "ID_test=test.ID\n",
    "del train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH PHASE ALL ALGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb=XGBClassifier(max_depth=3, objective=\"multi:softprob\",seed=26)\n",
    "param_test= {\n",
    "    \"max_depth\" : [3,5,7],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"n_estimators\" : [100,200],\n",
    "    \"subsample\":[0.8,1],\n",
    "    \"colsample_bytree\":[0.8,1]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_xgb, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=3,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lgbm=LGBMClassifier(seed=26)\n",
    "param_test= {\n",
    "    'n_estimators': [8,24,48],\n",
    "    'num_leaves': [6,12,16,22],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'colsample_bytree' : [0.7 0.8],\n",
    "    'subsample' : [0.7,0.8]\n",
    "    }\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_lgbm, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=3,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log=LogisticRegression()\n",
    "param_test= {\n",
    "    \"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"penalty\" : [\"l1\",\"l2\"]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_log, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=3,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada=AdaBoostClassifier(n_estimators=100, learning_rate=1.0, algorithm=\"SAMME.R\", random_state=26)\n",
    "param_test={\n",
    "    \"n_estimators\":[50,100],\n",
    "    \"learning_rate\":[0.3,0.5,0.9,1]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_ada, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=3,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt=RandomForestClassifier()\n",
    "param_test={\n",
    "    \"max_depth\":[5,7,10,15],\n",
    "    \"n_estimators\":[100,200,300]\n",
    "}\n",
    "for name in work_train:\n",
    "    gsearch=GridSearchCV(estimator=clf_dt, param_grid = param_test,scoring=\"neg_log_loss\",n_jobs=-1,iid=False, cv=3,verbose=True)\n",
    "    gsearch.fit(work_train[name],y)\n",
    "    print(name)\n",
    "    print(gsearch.grid_scores_)\n",
    "    print(gsearch.best_params_)\n",
    "    print(gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING PHASE ALL ALGOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gen(X,X_test,y,classifier,file,five_fold_predict=True):\n",
    "    #if not os.path.exists(\"scores/\"+file):\n",
    "    #   os.makedirs(\"scores/\"+file)\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, random_state=26, shuffle=True)\n",
    "    if five_fold_predict:\n",
    "        fold = 0\n",
    "        y_test=0\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "            fold += 1\n",
    "\n",
    "            X_train, X_valid    = X[train_index],   X[test_index]\n",
    "            y_train, y_valid    = y[train_index],   y[test_index]\n",
    "\n",
    "            print(\"Fold\", fold, X_train.shape, X_valid.shape)\n",
    "\n",
    "            clf=classifier\n",
    "            clf.fit(X_train,y_train)\n",
    "            p_test = clf.predict_proba(X_test)\n",
    "            y_test += p_test/5\n",
    "\n",
    "    classes = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\n",
    "    subm = pd.DataFrame(y_test, columns=classes)\n",
    "    subm['ID'] = ID_test\n",
    "    \n",
    "    subm.to_csv(\"nw_scores/nw_stack_test/nw_{}.csv\".format(file),index=False)\n",
    "    \n",
    "    print(\"cross_val sur train \") #peut etre que to array est exclusivement pour les xgb\n",
    "    \n",
    "    if os.path.isfile(\"nw_scores/nw_stack_train/nw_{}.csv\".format(file)):\n",
    "        print(\"not necessary, already done\")\n",
    "    else:\n",
    "        y_pred=cross_val_predict(estimator=clf,X=X,y=y,cv=kf,method=\"predict_proba\")\n",
    "        subm1 = pd.DataFrame(y_pred, columns=classes)\n",
    "        subm1['ID'] = ID_train\n",
    "        subm1.to_csv(\"nw_scores/nw_stack_train/nw_{}.csv\".format(file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d2v': <3321x629 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1178572 stored elements in Compressed Sparse Column format>,\n",
       " 'tfidf': <3321x10279 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 5332973 stored elements in Compressed Sparse Column format>,\n",
       " 'w2v': <3321x379 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 348322 stored elements in Compressed Sparse Column format>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (2653, 629) (668, 629)\n",
      "Fold 2 (2654, 629) (667, 629)\n",
      "Fold 3 (2657, 629) (664, 629)\n",
      "Fold 4 (2659, 629) (662, 629)\n",
      "Fold 5 (2661, 629) (660, 629)\n",
      "cross_val sur train \n"
     ]
    }
   ],
   "source": [
    "dic_xgb={\"xgb_d2v\":XGBClassifier(max_depth=3,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_tfidf\":XGBClassifier(max_depth=3,objective=\"multi:softprob\",seed=26),\n",
    "        \"xgb_w2v\":XGBClassifier(max_depth=3,objective=\"multi:softprob\",seed=26)}\n",
    "dic_lgbm={\"lgbm_d2v\":LGBMClassifier(seed=26),\n",
    "        \"lgbm_tfidf\":LGBMClassifier(seed=26),\n",
    "        \"lgbm_w2v\":LGBMClassifier(seed=26)}\n",
    "dic_lr={\"lr_d2v\":LogisticRegression(),\n",
    "        \"lr_tfidf\":LogisticRegression(),\n",
    "        \"lr_w2v\":LogisticRegression()}\n",
    "dic_ada={\"ada_d2v\":AdaBoostClassifier(n_estimators=100, learning_rate=1.0, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_tfidf\":AdaBoostClassifier(n_estimators=100, learning_rate=1.0, algorithm=\"SAMME.R\", random_state=26),\n",
    "        \"ada_w2v\":AdaBoostClassifier(n_estimators=100, learning_rate=1.0, algorithm=\"SAMME.R\", random_state=26)}\n",
    "dic_rf={\"rf_d2v\":RandomForestClassifier(n_estimators=200,max_depth=10,random_state=26),\n",
    "        \"rf_tfidf\":RandomForestClassifier(n_estimators=200,max_depth=10,random_state=26),\n",
    "        \"rf_w2v\":RandomForestClassifier(n_estimators=200,max_depth=10,random_state=26)}\n",
    "\n",
    "for clf in dic_xgb:\n",
    "    for name in work_train:\n",
    "        model_gen(X=work_train[name],X_test=work_test[name],y=y,classifier=dic_xgb[clf],file=clf)\n",
    "for clf in dic_lgbm:\n",
    "    for name in work_train:\n",
    "        model_gen(X=work_train[name],X_test=work_test[name],y=y,classifier=dic_lgbm[clf],file=clf)\n",
    "for clf in dic_lr:\n",
    "    for name in work_train:\n",
    "        model_gen(X=work_train[name],X_test=work_test[name],y=y,classifier=dic_lr[clf],file=clf)\n",
    "for clf in dic_ada:\n",
    "    for name in work_train:\n",
    "        model_gen(X=work_train[name],X_test=work_test[name],y=y,classifier=dic_ada[clf],file=clf)\n",
    "for clf in dic_rf:\n",
    "    for name in work_train:\n",
    "        model_gen(X=work_train[name],X_test=work_test[name],y=y,classifier=dic_rf[clf],file=clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
