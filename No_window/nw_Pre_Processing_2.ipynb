{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim import utils\n",
    "import os\n",
    "import nltk\n",
    "import scipy.sparse as ssp\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"checkpoints_databases/nw_working_train.csv\",encoding=\"utf8\")\n",
    "test=pd.read_csv(\"checkpoints_databases/nw_working_test.csv\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl=train.drop([\"Variation\",\"Text\",\"Class\"],axis=1)\n",
    "test_cl=test.drop([\"Text\",\"Class\",\"Variation\"],axis=1)\n",
    "train_cl.to_csv(\"nw_meta_features/meta_train_l1l2.csv\")\n",
    "test_cl.to_csv(\"nw_meta_features/meta_test_l1l2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will add features from word2vec retrained then get the mean for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location,size):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=size, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's important to remove duplicated spaces for word2vec learning !\n",
    "train[\"Text\"]=[\" \".join(doc.split()) for doc in train[\"Text\"].values]\n",
    "test[\"Text\"]=[\" \".join(doc.split()) for doc in test[\"Text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2v_features100\n",
      "Found w2v_features200\n",
      "Found w2v_features300\n"
     ]
    }
   ],
   "source": [
    "number_w2v=[100,200,300]\n",
    "w2v={}\n",
    "for size in number_w2v:\n",
    "    w2v[\"w2v_\"+str(size)] = get_word2vec(\n",
    "        MySentences(\n",
    "            train[\"Text\"].values),\"w2v_features\"+str(size),size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer={}\n",
    "mean_embedded_train={}\n",
    "mean_embedded_test={}\n",
    "for name in w2v:\n",
    "    mean_embedding_vectorizer[name] = MeanEmbeddingVectorizer(w2v[name])\n",
    "    mean_embedded_train[name] = mean_embedding_vectorizer[name].fit_transform(train['Text'])\n",
    "    mean_embedded_test[name] = mean_embedding_vectorizer[name].fit_transform(test['Text'])\n",
    "df_embed_tr={}\n",
    "df_embed_tr={}\n",
    "for name in w2v:\n",
    "    df_embed_tr[name]=pd.DataFrame(mean_embedded_train[name])\n",
    "    df_embed_te[name]=pd.DataFrame(mean_embedded_test[name])\n",
    "for name in w2v:\n",
    "    df_embedding_tr[name]=df_embed_tr.reset_index()\n",
    "    df_embedding_te[name]=df_embed_te.reset_index()\n",
    "    df_embedding_tr[name]=df_embedding_tr.rename(columns={\"index\":\"ID\"})\n",
    "    df_embedding_te[name]=df_embedding_te.rename(columns={\"index\":\"ID\"})\n",
    "train_w2v={}\n",
    "test_w2v={}\n",
    "for name in w2v:\n",
    "    train_w2v[name]=pd.merge(train_cl,df_embedding_tr,on=\"ID\")\n",
    "    test_w2v[name]=pd.merge(test_cl,df_embedding_te,on=\"ID\")\n",
    "np_w2v_train={}\n",
    "nw_w2v_test={}\n",
    "ssp_w2v_train={}\n",
    "ssp_w2v_test={}\n",
    "for name in w2v:\n",
    "    np_w2v_train[name]=np.array(train_w2v[name].drop(\"ID\",axis=1))\n",
    "    np_w2v_test[name]=np.array(test_w2v[name].drop(\"ID\",axis=1))\n",
    "    ssp_w2v_train[name]=ssp.csc_matrix(np_w2v_train[name])\n",
    "    ssp_w2v_test[name]=ssp.csc_matrix(np_w2v_test[name])\n",
    "for name in w2v:\n",
    "    ssp.save_npz(\"checkpoints_databases/nw_working_train_w2v_\"+name+\".npz\",ssp_w2v_train[name])\n",
    "    ssp.save_npz(\"checkpoints_databases/nw_working_test_w2v_\"+name+\".npz\",ssp_w2v_test[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now TFIDF+300tsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        min_df=10, max_features=10000, strip_accents=None, lowercase = False,\n",
    "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1,3), use_idf=True,\n",
    "        smooth_idf=True, sublinear_tf=True\n",
    "        ).fit(train[\"Text\"])\n",
    "\n",
    "X_train_text = tfidf.transform(train[\"Text\"])\n",
    "X_test_text = tfidf.transform(test[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_train= {}\n",
    "tsvd_test={}\n",
    "list_comp=[100,200,300]\n",
    "dic_svd={}\n",
    "for comp in list_comp:\n",
    "    dic_svd[str(comp)]=TruncatedSVD(n_components=comp,n_iter=25,random_state=26)\n",
    "for svd in dic_svd:\n",
    "    tsvd_train[svd]=dic_svd[svd].fit_transform(X_train_text)\n",
    "    tsvd_test[svd]=dic_svd[svd].transform(X_test_text)\n",
    "    X_train=pd.DataFrame()\n",
    "X_test=pd.DataFrame()\n",
    "for n in dic_svd:\n",
    "    for i in range(int(n)):\n",
    "        X_train['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_train[n][:, i]\n",
    "        X_test['tsvd_' +str(n)+\"_\"+str(i)] = tsvd_test[n][:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100=X_train.iloc[:,:100]\n",
    "X_train_200=X_train.iloc[:,100:300]\n",
    "X_train_300=X_train.iloc[:,300:600]\n",
    "X_test_100=X_test.iloc[:,:100]\n",
    "X_test_200=X_test.iloc[:,100:300]\n",
    "X_test_300=X_test.iloc[:,300:600]\n",
    "dic_train={}\n",
    "dic_test={}\n",
    "dic_train[\"tsvd_100\"]=ssp.hstack((train_cl.drop(\"ID\",axis=1), X_train_100),format=\"csc\")\n",
    "dic_test[\"tsvd_100\"]=ssp.hstack((test_cl.drop(\"ID\",axis=1),X_test_100),format=\"csc\")\n",
    "dic_train[\"tsvd_200\"]=ssp.hstack((train_cl.drop(\"ID\",axis=1), X_train_200),format=\"csc\")\n",
    "dic_test[\"tsvd_200\"]=ssp.hstack((test_cl.drop(\"ID\",axis=1),X_test_200),format=\"csc\")\n",
    "dic_train[\"tsvd_300\"]=ssp.hstack((train_cl.drop(\"ID\",axis=1), X_train_300),format=\"csc\")\n",
    "dic_test[\"tsvd_300\"]=ssp.hstack((test_cl.drop(\"ID\",axis=1),X_test_300),format=\"csc\")\n",
    "for name in dic_train:\n",
    "    ssp.save_npz(\"checkpoints_databases/nw_working_train_tfidf_\"+name+\".npz\",dic_train[name])\n",
    "    ssp.save_npz(\"checkpoints_databases/nw_working_test_tfidf_\"+name+\".npz\",dic_test[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "train_sentences = constructLabeledSentences(train['Text'])\n",
    "test_sentences = constructLabeledSentences(test['Text'])\n",
    "\n",
    "Text_dim=[100,200,300]\n",
    "d2v_train={}\n",
    "d2v_test={}\n",
    "for size in Text_dim:\n",
    "    d2v_train[\"d2v_\"+str(size)] = Doc2Vec(min_count=1, window=10, size=size, sample=1e-4, negative=5, workers=-1, iter=5,seed=26)\n",
    "    d2v_train[\"d2v_\"+str(size)].build_vocab(train_sentences)\n",
    "    d2v_train[\"d2v_\"+str(size)].train(train_sentences, total_examples=d2v_train[\"d2v_\"+str(size)].corpus_count,\n",
    "                                      epochs=d2v_train[\"d2v_\"+str(size)].iter)\n",
    "for size in Text_dim:\n",
    "    d2v_test[\"d2v_\"+str(size)] = Doc2Vec(min_count=1, window=10, size=size, sample=1e-4, negative=5, workers=-1, iter=5,seed=26)\n",
    "    d2v_test[\"d2v_\"+str(size)].build_vocab(test_sentences)\n",
    "    d2v_test[\"d2v_\"+str(size)].train(test_sentences, total_examples=d2v_test[\"d2v_\"+str(size)].corpus_count,\n",
    "                                 epochs=d2v_test[\"d2v_\"+str(size)].iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_train_arrays={}\n",
    "d2v_test_arrays={}\n",
    "d2v_train_arrays[\"d2v_100\"] = np.zeros((len(train), 100))\n",
    "d2v_test_arrays[\"d2v_100\"] = np.zeros((len(test), 100))\n",
    "d2v_train_arrays[\"d2v_200\"] = np.zeros((len(train), 200))\n",
    "d2v_test_arrays[\"d2v_200\"] = np.zeros((len(test), 200))\n",
    "d2v_train_arrays[\"d2v_300\"] = np.zeros((len(train), 300))\n",
    "d2v_test_arrays[\"d2v_300\"] = np.zeros((len(test), 300))\n",
    "\n",
    "for n_train,n_test in zip(range(len(train)),range(len(test))):\n",
    "    d2v_train_arrays[\"d2v_100\"][i] = d2v_train[\"d2v_100\"].docvecs['Text_'+str(n_train)]\n",
    "    d2v_test_arrays[\"d2v_100\"][i] = d2v_test[\"d2v_100\"].docvecs['Text_'+str(n_test)]\n",
    "    d2v_train_arrays[\"d2v_200\"][i] = d2v_train[\"d2v_200\"].docvecs['Text_'+str(n_train)]\n",
    "    d2v_test_arrays[\"d2v_200\"][i] = d2v_test[\"d2v_200\"].docvecs['Text_'+str(n_test)]\n",
    "    d2v_train_arrays[\"d2v_300\"][i] = d2v_train[\"d2v_300\"].docvecs['Text_'+str(n_train)]\n",
    "    d2v_test_arrays[\"d2v_300\"][i] = d2v_test[\"d2v_300\"].docvecs['Text_'+str(n_test)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=ssp.hstack((train_cl.drop(\"ID\",axis=1), d2v_train_arrays),format=\"csc\")\n",
    "X_test=ssp.hstack((test_cl.drop(\"ID\",axis=1), d2v_test_arrays),format=\"csc\")\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "ssp.save_npz(\"checkpoints_databases/nw_working_train_d2v.npz\",X_train)\n",
    "ssp.save_npz(\"checkpoints_databases/nw_working_test_d2v.npz\",X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
